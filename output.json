[
  {
    "title": "HAZARD Challenge: Embodied Decision Making in Dynamically Changing\n  Environments",
    "authors": "Qinhong Zhou, Sunli Chen, Yisong Wang, Haozhe Xu, Weihua Du, Hongxin Zhang, Yilun Du, Joshua B. Tenenbaum, Chuang Gan",
    "abstract": "  Recent advances in high-fidelity virtual environments serve as one of the\nmajor driving forces for building intelligent embodied agents to perceive,\nreason and interact with the physical world. Typically, these environments\nremain unchanged unless agents interact with them. However, in real-world\nscenarios, agents might also face dynamically changing environments\ncharacterized by unexpected events and need to rapidly take action accordingly.\nTo remedy this gap, we propose a new simulated embodied benchmark, called\nHAZARD, specifically designed to assess the decision-making abilities of\nembodied agents in dynamic situations. HAZARD consists of three unexpected\ndisaster scenarios, including fire, flood, and wind, and specifically supports\nthe utilization of large language models (LLMs) to assist common sense\nreasoning and decision-making. This benchmark enables us to evaluate autonomous\nagents' decision-making capabilities across various pipelines, including\nreinforcement learning (RL), rule-based, and search-based methods in\ndynamically changing environments. As a first step toward addressing this\nchallenge using large language models, we further develop an LLM-based agent\nand perform an in-depth analysis of its promise and challenge of solving these\nchallenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.\n",
    "link": "http://arxiv.org/abs/2401.12975v1"
  },
  {
    "title": "On the Efficacy of Text-Based Input Modalities for Action Anticipation",
    "authors": "Apoorva Beedu, Karan Samel, Irfan Essa",
    "abstract": "  Although the task of anticipating future actions is highly uncertain,\ninformation from additional modalities help to narrow down plausible action\nchoices. Each modality provides different environmental context for the model\nto learn from. While previous multi-modal methods leverage information from\nmodalities such as video and audio, we primarily explore how text inputs for\nactions and objects can also enable more accurate action anticipation.\nTherefore, we propose a Multi-modal Anticipative Transformer (MAT), an\nattention-based video transformer architecture that jointly learns from\nmulti-modal features and text captions. We train our model in two-stages, where\nthe model first learns to predict actions in the video clip by aligning with\ncaptions, and during the second stage, we fine-tune the model to predict future\nactions. Compared to existing methods, MAT has the advantage of learning\nadditional environmental context from two kinds of text inputs: action\ndescriptions during the pre-training stage, and the text inputs for detected\nobjects and actions during modality feature fusion. Through extensive\nexperiments, we evaluate the effectiveness of the pre-training stage, and show\nthat our model outperforms previous methods on all datasets. In addition, we\nexamine the impact of object and action information obtained via text and\nperform extensive ablations. We evaluate the performance on on three datasets:\nEpicKitchens-100, EpicKitchens-55 and EGTEA GAZE+; and show that text\ndescriptions do indeed aid in more effective action anticipation.\n",
    "link": "http://arxiv.org/abs/2401.12972v1"
  },
  {
    "title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of\n  Robotic Agents",
    "authors": "Michael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Karol Hausman, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Sean Kirmani, Isabel Leal, Edward Lee, Sergey Levine, Yao Lu, Isabel Leal, Sharath Maddineni, Kanishka Rao, Dorsa Sadigh, Pannag Sanketi, Pierre Sermanet, Quan Vuong, Stefan Welker, Fei Xia, Ted Xiao, Peng Xu, Steve Xu, Zhuo Xu",
    "abstract": "  Foundation models that incorporate language, vision, and more recently\nactions have revolutionized the ability to harness internet scale data to\nreason about useful tasks. However, one of the key challenges of training\nembodied foundation models is the lack of data grounded in the physical world.\nIn this paper, we propose AutoRT, a system that leverages existing foundation\nmodels to scale up the deployment of operational robots in completely unseen\nscenarios with minimal human supervision. AutoRT leverages vision-language\nmodels (VLMs) for scene understanding and grounding, and further uses large\nlanguage models (LLMs) for proposing diverse and novel instructions to be\nperformed by a fleet of robots. Guiding data collection by tapping into the\nknowledge of foundation models enables AutoRT to effectively reason about\nautonomy tradeoffs and safety while significantly scaling up data collection\nfor robot learning. We demonstrate AutoRT proposing instructions to over 20\nrobots across multiple buildings and collecting 77k real robot episodes via\nboth teleoperation and autonomous robot policies. We experimentally show that\nsuch \"in-the-wild\" data collected by AutoRT is significantly more diverse, and\nthat AutoRT's use of LLMs allows for instruction following data collection\nrobots that can align to human preferences.\n",
    "link": "http://arxiv.org/abs/2401.12963v1"
  },
  {
    "title": "Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding",
    "authors": "Mirac Suzgun, Adam Tauman Kalai",
    "abstract": "  We introduce meta-prompting, an effective scaffolding technique designed to\nenhance the functionality of language models (LMs). This approach transforms a\nsingle LM into a multi-faceted conductor, adept at managing and integrating\nmultiple independent LM queries. By employing high-level instructions,\nmeta-prompting guides the LM to break down complex tasks into smaller, more\nmanageable subtasks. These subtasks are then handled by distinct \"expert\"\ninstances of the same LM, each operating under specific, tailored instructions.\nCentral to this process is the LM itself, in its role as the conductor, which\nensures seamless communication and effective integration of the outputs from\nthese expert models. It additionally employs its inherent critical thinking and\nrobust verification processes to refine and authenticate the end result. This\ncollaborative prompting approach empowers a single LM to simultaneously act as\na comprehensive orchestrator and a panel of diverse experts, significantly\nenhancing its performance across a wide array of tasks. The zero-shot,\ntask-agnostic nature of meta-prompting greatly simplifies user interaction by\nobviating the need for detailed, task-specific instructions. Furthermore, our\nresearch demonstrates the seamless integration of external tools, such as a\nPython interpreter, into the meta-prompting framework, thereby broadening its\napplicability and utility. Through rigorous experimentation with GPT-4, we\nestablish the superiority of meta-prompting over conventional scaffolding\nmethods: When averaged across all tasks, including the Game of 24,\nCheckmate-in-One, and Python Programming Puzzles, meta-prompting, augmented\nwith a Python interpreter functionality, surpasses standard prompting by 17.1%,\nexpert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.\n",
    "link": "http://arxiv.org/abs/2401.12954v1"
  },
  {
    "title": "Transformer-Based Models Are Not Yet Perfect At Learning to Emulate\n  Structural Recursion",
    "authors": "Dylan Zhang, Curt Tigges, Zory Zhang, Stella Biderman, Maxim Raginsky, Talia Ringer",
    "abstract": "  This paper investigates the ability of transformer-based models to learn\nstructural recursion from examples. Recursion is a universal concept in both\nnatural and formal languages. Structural recursion is central to the\nprogramming language and formal mathematics tasks where symbolic tools\ncurrently excel beyond neural models, such as inferring semantic relations\nbetween datatypes and emulating program behavior. We introduce a general\nframework that nicely connects the abstract concepts of structural recursion in\nthe programming language domain to concrete sequence modeling problems and\nlearned models' behavior. The framework includes a representation that captures\nthe general \\textit{syntax} of structural recursion, coupled with two different\nframeworks for understanding their \\textit{semantics} -- one that is more\nnatural from a programming languages perspective and one that helps bridge that\nperspective with a mechanistic understanding of the underlying transformer\narchitecture.\n  With our framework as a powerful conceptual tool, we identify different\nissues under various set-ups. The models trained to emulate recursive\ncomputations cannot fully capture the recursion yet instead fit short-cut\nalgorithms and thus cannot solve certain edge cases that are under-represented\nin the training distribution. In addition, it is difficult for state-of-the-art\nlarge language models (LLMs) to mine recursive rules from in-context\ndemonstrations. Meanwhile, these LLMs fail in interesting ways when emulating\nreduction (step-wise computation) of the recursive function.\n",
    "link": "http://arxiv.org/abs/2401.12947v1"
  },
  {
    "title": "Truck Parking Usage Prediction with Decomposed Graph Neural Networks",
    "authors": "Rei Tamaru, Yang Cheng, Steven Parker, Ernie Perry, Bin Ran, Soyoung Ahn",
    "abstract": "  Truck parking on freight corridors faces various challenges, such as\ninsufficient parking spaces and compliance with Hour-of-Service (HOS)\nregulations. These constraints often result in unauthorized parking practices,\ncausing safety concerns. To enhance the safety of freight operations, providing\naccurate parking usage prediction proves to be a cost-effective solution.\nDespite the existing research demonstrating satisfactory accuracy for\npredicting individual truck parking site usage, few approaches have been\nproposed for predicting usage with spatial dependencies of multiple truck\nparking sites. We present the Regional Temporal Graph Neural Network (RegT-GCN)\nas a predictive framework for assessing parking usage across the entire state\nto provide better truck parking information and mitigate unauthorized parking.\nThe framework leverages the topological structures of truck parking site\ndistributions and historical parking data to predict occupancy rates across a\nstate. To achieve this, we introduce a Regional Decomposition approach, which\neffectively captures the geographical characteristics. We also introduce the\nspatial module working efficiently with the temporal module. Evaluation results\ndemonstrate that the proposed model surpasses other baseline models, improving\nthe performance by more than $20\\%$ compared with the original model. The\nproposed model allows truck parking sites' percipience of the topological\nstructures and provides higher performance.\n",
    "link": "http://arxiv.org/abs/2401.12920v1"
  },
  {
    "title": "Active Inference as a Model of Agency",
    "authors": "Lancelot Da Costa, Samuel Tenka, Dominic Zhao, Noor Sajid",
    "abstract": "  Is there a canonical way to think of agency beyond reward maximisation? In\nthis paper, we show that any type of behaviour complying with physically sound\nassumptions about how macroscopic biological agents interact with the world\ncanonically integrates exploration and exploitation in the sense of minimising\nrisk and ambiguity about states of the world. This description, known as active\ninference, refines the free energy principle, a popular descriptive framework\nfor action and perception originating in neuroscience. Active inference\nprovides a normative Bayesian framework to simulate and model agency that is\nwidely used in behavioural neuroscience, reinforcement learning (RL) and\nrobotics. The usefulness of active inference for RL is three-fold. \\emph{a})\nActive inference provides a principled solution to the exploration-exploitation\ndilemma that usefully simulates biological agency. \\emph{b}) It provides an\nexplainable recipe to simulate behaviour, whence behaviour follows as an\nexplainable mixture of exploration and exploitation under a generative world\nmodel, and all differences in behaviour are explicit in differences in world\nmodel. \\emph{c}) This framework is universal in the sense that it is\ntheoretically possible to rewrite any RL algorithm conforming to the\ndescriptive assumptions of active inference as an active inference algorithm.\nThus, active inference can be used as a tool to uncover and compare the\ncommitments and assumptions of more specific models of agency.\n",
    "link": "http://arxiv.org/abs/2401.12917v1"
  },
  {
    "title": "Red Teaming Visual Language Models",
    "authors": "Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu, Qi Liu",
    "abstract": "  VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language\nModels) to accept multimodal inputs. Since it has been verified that LLMs can\nbe induced to generate harmful or inaccurate content through specific test\ncases (termed as Red Teaming), how VLMs perform in similar scenarios,\nespecially with their combination of textual and visual inputs, remains a\nquestion. To explore this problem, we present a novel red teaming dataset\nRTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal\njail-breaking, face fairness, etc) under 4 primary aspects (faithfulness,\nprivacy, safety, fairness). Our RTVLM is the first red-teaming dataset to\nbenchmark current VLMs in terms of these 4 different aspects. Detailed analysis\nshows that 10 prominent open-sourced VLMs struggle with the red teaming in\ndifferent degrees and have up to 31% performance gap with GPT-4V. Additionally,\nwe simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning\n(SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM\ntest set, 13% in MM-Hal, and without noticeable decline in MM-Bench,\noverpassing other LLaVA-based models with regular alignment data. This reveals\nthat current open-sourced VLMs still lack red teaming alignment. Our code and\ndatasets will be open-source.\n",
    "link": "http://arxiv.org/abs/2401.12915v1"
  },
  {
    "title": "Emergent Communication Protocol Learning for Task Offloading in\n  Industrial Internet of Things",
    "authors": "Salwa Mostafa, Mateus P. Mota, Alvaro Valcarce, Mehdi Bennis",
    "abstract": "  In this paper, we leverage a multi-agent reinforcement learning (MARL)\nframework to jointly learn a computation offloading decision and multichannel\naccess policy with corresponding signaling. Specifically, the base station and\nindustrial Internet of Things mobile devices are reinforcement learning agents\nthat need to cooperate to execute their computation tasks within a deadline\nconstraint. We adopt an emergent communication protocol learning framework to\nsolve this problem. The numerical results illustrate the effectiveness of\nemergent communication in improving the channel access success rate and the\nnumber of successfully computed tasks compared to contention-based,\ncontention-free, and no-communication approaches. Moreover, the proposed task\noffloading policy outperforms remote and local computation baselines.\n",
    "link": "http://arxiv.org/abs/2401.12914v1"
  },
  {
    "title": "From Understanding to Utilization: A Survey on Explainability for Large\n  Language Models",
    "authors": "Haoyan Luo, Lucia Specia",
    "abstract": "  This survey paper delves into the burgeoning field of explainability for\nLarge Language Models (LLMs), a critical yet challenging aspect of natural\nlanguage processing. With LLMs playing a pivotal role in various applications,\ntheir \"black-box\" nature raises concerns about transparency and ethical use.\nThis paper emphasizes the necessity for enhanced explainability in LLMs,\naddressing both the general public's trust and the technical community's need\nfor a deeper understanding of these models. We concentrate on pre-trained\nTransformer-based LLMs, such as LLaMA, which present unique interpretability\nchallenges due to their scale and complexity. Our review categorizes existing\nexplainability methods and discusses their application in improving model\ntransparency and reliability. We also discuss representative evaluation\nmethods, highlighting their strengths and limitations. The goal of this survey\nis to bridge the gap between theoretical understanding and practical\napplication, offering insights for future research and development in the field\nof LLM explainability.\n",
    "link": "http://arxiv.org/abs/2401.12874v1"
  },
  {
    "title": "Improving Machine Translation with Human Feedback: An Exploration of\n  Quality Estimation as a Reward Model",
    "authors": "Zhiwei He, Xing Wang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang, Shuming Shi, Zhaopeng Tu",
    "abstract": "  Insufficient modeling of human preferences within the reward model is a major\nobstacle for leveraging human feedback to improve translation quality.\nFortunately, quality estimation (QE), which predicts the quality of a given\ntranslation without reference, has achieved impressive alignment with human\nevaluations in the last two years. In this work, we investigate the potential\nof employing the QE model as the reward model (the QE-based reward model) to\npredict human preferences for feedback training. We first identify the\noveroptimization problem during QE-based feedback training, manifested as an\nincrease in reward while translation quality declines. We examine the problem\nand argue that the vulnerability of the QE model might lead to high rewards for\nincorrect translations, resulting in overoptimization and error propagation. To\naddress the problem, we adopt a simple yet effective method that uses heuristic\nrules to detect the incorrect translations and assigns a penalty term to the\nQE-based rewards for the detected incorrect translations. Experimental results\nshow that the proposed QE-based feedback training achieves consistent and\nsignificant improvements across various settings, further verified through\nhuman preference studies. Our subsequent analysis demonstrates the high data\nefficiency of the proposed QE-based feedback training: the proposed approach\nusing a small amount of monolingual data can outperform systems using larger\nparallel corpora.\n",
    "link": "http://arxiv.org/abs/2401.12873v1"
  },
  {
    "title": "TroVE: Inducing Verifiable and Efficient Toolboxes for Solving\n  Programmatic Tasks",
    "authors": "Zhiruo Wang, Daniel Fried, Graham Neubig",
    "abstract": "  Language models (LMs) can solve tasks such as answering questions about\ntables or images by writing programs. However, using primitive functions often\nleads to verbose and error-prone programs, and higher-level functions require\nexpert design. To enable better solutions without human labor, we ask code LMs\nto curate reusable high-level functions, and use them to write solutions. We\npresent TROVE, a training-free method of inducing a verifiable and efficient\ntoolbox of functions, by generating via using, growing, and periodically\ntrimming the toolbox. On 11 datasets from math, table question answering, and\nimage reasoning tasks, TROVE consistently yields simpler solutions with higher\naccuracy than baselines using CODELLAMA and previous methods using GPT, while\nusing 79-98% smaller toolboxes. TROVE further enables 31% faster and 13% more\naccurate human verification than baselines. With the same pipeline, it creates\ndiverse functions for varied tasks and datasets, providing insights into their\nindividual characteristics.\n",
    "link": "http://arxiv.org/abs/2401.12869v1"
  },
  {
    "title": "Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported\n  Coordination of Mobile Crowdsourcing",
    "authors": "Ralf Bruns, Jeremias D\u00f6tterl, J\u00fcrgen Dunkel, Sascha Ossowski",
    "abstract": "  Mobile crowdsourcing refers to systems where the completion of tasks\nnecessarily requires physical movement of crowdworkers in an on-demand\nworkforce. Evidence suggests that in such systems, tasks often get assigned to\ncrowdworkers who struggle to complete those tasks successfully, resulting in\nhigh failure rates and low service quality. A promising solution to ensure\nhigher quality of service is to continuously adapt the assignment and respond\nto failure-causing events by transferring tasks to better-suited workers who\nuse different routes or vehicles. However, implementing task transfers in\nmobile crowdsourcing is difficult because workers are autonomous and may reject\ntransfer requests. Moreover, task outcomes are uncertain and need to be\npredicted. In this paper, we propose different mechanisms to achieve outcome\nprediction and task coordination in mobile crowdsourcing. First, we analyze\ndifferent data stream learning approaches for the prediction of task outcomes.\nSecond, based on the suggested prediction model, we propose and evaluate two\ndifferent approaches for task coordination with different degrees of autonomy:\nan opportunistic approach for crowdshipping with collaborative, but\nnon-autonomous workers, and a market-based model with autonomous workers for\ncrowdsensing.\n",
    "link": "http://arxiv.org/abs/2401.12866v1"
  },
  {
    "title": "KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning",
    "authors": "Debjyoti Mondal, Suraj Modi, Subhadarshi Panda, Rituraj Singh, Godawari Sudhakar Rao",
    "abstract": "  Large Language Models (LLMs) have demonstrated impressive performance in\nnatural language processing tasks by leveraging chain of thought (CoT) that\nenables step-by-step thinking. Extending LLMs with multimodal capabilities is\nthe recent interest, but incurs computational cost and requires substantial\nhardware resources. To address these challenges, we propose KAM-CoT a framework\nthat integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities\nfor a comprehensive understanding of multimodal tasks. KAM-CoT adopts a\ntwo-stage training process with KG grounding to generate effective rationales\nand answers. By incorporating external knowledge from KGs during reasoning, the\nmodel gains a deeper contextual understanding reducing hallucinations and\nenhancing the quality of answers. This knowledge-augmented CoT reasoning\nempowers the model to handle questions requiring external context, providing\nmore informed answers. Experimental findings show KAM-CoT outperforms the\nstate-of-the-art methods. On the ScienceQA dataset, we achieve an average\naccuracy of 93.87%, surpassing GPT-3.5 (75.17%) by 18% and GPT-4 (83.99%) by\n10%. Remarkably, KAM-CoT achieves these results with only 280M trainable\nparameters at a time, demonstrating its cost-efficiency and effectiveness.\n",
    "link": "http://arxiv.org/abs/2401.12863v1"
  },
  {
    "title": "FedRSU: Federated Learning for Scene Flow Estimation on Roadside Units",
    "authors": "Shaoheng Fang, Rui Ye, Wenhao Wang, Zuhong Liu, Yuxiao Wang, Yafei Wang, Siheng Chen, Yanfeng Wang",
    "abstract": "  Roadside unit (RSU) can significantly improve the safety and robustness of\nautonomous vehicles through Vehicle-to-Everything (V2X) communication.\nCurrently, the usage of a single RSU mainly focuses on real-time inference and\nV2X collaboration, while neglecting the potential value of the high-quality\ndata collected by RSU sensors. Integrating the vast amounts of data from\nnumerous RSUs can provide a rich source of data for model training. However,\nthe absence of ground truth annotations and the difficulty of transmitting\nenormous volumes of data are two inevitable barriers to fully exploiting this\nhidden value. In this paper, we introduce FedRSU, an innovative federated\nlearning framework for self-supervised scene flow estimation. In FedRSU, we\npresent a recurrent self-supervision training paradigm, where for each RSU, the\nscene flow prediction of points at every timestamp can be supervised by its\nsubsequent future multi-modality observation. Another key component of FedRSU\nis federated learning, where multiple devices collaboratively train an ML model\nwhile keeping the training data local and private. With the power of the\nrecurrent self-supervised learning paradigm, FL is able to leverage innumerable\nunderutilized data from RSU. To verify the FedRSU framework, we construct a\nlarge-scale multi-modality dataset RSU-SF. The dataset consists of 17 RSU\nclients, covering various scenarios, modalities, and sensor settings. Based on\nRSU-SF, we show that FedRSU can greatly improve model performance in ITS and\nprovide a comprehensive benchmark under diverse FL scenarios. To the best of\nour knowledge, we provide the first real-world LiDAR-camera multi-modal dataset\nand benchmark for the FL community.\n",
    "link": "http://arxiv.org/abs/2401.12862v1"
  },
  {
    "title": "Classification of grapevine varieties using UAV hyperspectral imaging",
    "authors": "Alfonso L\u00f3pez, Carlos Javier Ogayar, Francisco Ram\u00f3n Feito, Joaquim Jo\u00e3o Sousa",
    "abstract": "  The classification of different grapevine varieties is a relevant phenotyping\ntask in Precision Viticulture since it enables estimating the growth of\nvineyard rows dedicated to different varieties, among other applications\nconcerning the wine industry. This task can be performed with destructive\nmethods that require time-consuming tasks, including data collection and\nanalysis in the laboratory. However, Unmanned Aerial Vehicles (UAV) provide a\nmore efficient and less prohibitive approach to collecting hyperspectral data,\ndespite acquiring noisier data. Therefore, the first task is the processing of\nthese data to correct and downsample large amounts of data. In addition, the\nhyperspectral signatures of grape varieties are very similar. In this work, a\nConvolutional Neural Network (CNN) is proposed for classifying seventeen\nvarieties of red and white grape variants. Rather than classifying single\nsamples, these are processed together with their neighbourhood. Hence, the\nextraction of spatial and spectral features is addressed with 1) a spatial\nattention layer and 2) Inception blocks. The pipeline goes from processing to\ndataset elaboration, finishing with the training phase. The fitted model is\nevaluated in terms of response time, accuracy and data separability, and\ncompared with other state-of-the-art CNNs for classifying hyperspectral data.\nOur network was proven to be much more lightweight with a reduced number of\ninput bands, a lower number of trainable weights and therefore, reduced\ntraining time. Despite this, the evaluated metrics showed much better results\nfor our network (~99% overall accuracy), in comparison with previous works\nbarely achieving 81% OA.\n",
    "link": "http://arxiv.org/abs/2401.12851v1"
  },
  {
    "title": "Overlap-aware End-to-End Supervised Hierarchical Graph Clustering for\n  Speaker Diarization",
    "authors": "Prachi Singh, Sriram Ganapathy",
    "abstract": "  Speaker diarization, the task of segmenting an audio recording based on\nspeaker identity, constitutes an important speech pre-processing step for\nseveral downstream applications. The conventional approach to diarization\ninvolves multiple steps of embedding extraction and clustering, which are often\noptimized in an isolated fashion. While end-to-end diarization systems attempt\nto learn a single model for the task, they are often cumbersome to train and\nrequire large supervised datasets. In this paper, we propose an end-to-end\nsupervised hierarchical clustering algorithm based on graph neural networks\n(GNN), called End-to-end Supervised HierARchical Clustering (E-SHARC). The\nE-SHARC approach uses front-end mel-filterbank features as input and jointly\nlearns an embedding extractor and the GNN clustering module, performing\nrepresentation learning, metric learning, and clustering with end-to-end\noptimization. Further, with additional inputs from an external overlap\ndetector, the E-SHARC approach is capable of predicting the speakers in the\noverlapping speech regions. The experimental evaluation on several benchmark\ndatasets like AMI, VoxConverse and DISPLACE, illustrates that the proposed\nE-SHARC framework improves significantly over the state-of-art diarization\nsystems.\n",
    "link": "http://arxiv.org/abs/2401.12850v1"
  },
  {
    "title": "How well can large language models explain business processes?",
    "authors": "Dirk Fahland, Fabian Fournier, Lior Limonad, Inna Skarbovsky, Ava J. E. Swevels",
    "abstract": "  Large Language Models (LLMs) are likely to play a prominent role in future\nAI-augmented business process management systems (ABPMSs) catering\nfunctionalities across all system lifecycle stages. One such system's\nfunctionality is Situation-Aware eXplainability (SAX), which relates to\ngenerating causally sound and yet human-interpretable explanations that take\ninto account the process context in which the explained condition occurred. In\nthis paper, we present the SAX4BPM framework developed to generate SAX\nexplanations. The SAX4BPM suite consists of a set of services and a central\nknowledge repository. The functionality of these services is to elicit the\nvarious knowledge ingredients that underlie SAX explanations. A key innovative\ncomponent among these ingredients is the causal process execution view. In this\nwork, we integrate the framework with an LLM to leverage its power to\nsynthesize the various input ingredients for the sake of improved SAX\nexplanations. Since the use of LLMs for SAX is also accompanied by a certain\ndegree of doubt related to its capacity to adequately fulfill SAX along with\nits tendency for hallucination and lack of inherent capacity to reason, we\npursued a methodological evaluation of the quality of the generated\nexplanations. To this aim, we developed a designated scale and conducted a\nrigorous user study. Our findings show that the input presented to the LLMs\naided with the guard-railing of its performance, yielding SAX explanations\nhaving better-perceived fidelity. This improvement is moderated by the\nperception of trust and curiosity. More so, this improvement comes at the cost\nof the perceived interpretability of the explanation.\n",
    "link": "http://arxiv.org/abs/2401.12846v1"
  },
  {
    "title": "SGTR+: End-to-end Scene Graph Generation with Transformer",
    "authors": "Rongjie Li, Songyang Zhang, Xuming He",
    "abstract": "  Scene Graph Generation (SGG) remains a challenging visual understanding task\ndue to its compositional property. Most previous works adopt a bottom-up,\ntwo-stage or point-based, one-stage approach, which often suffers from high\ntime complexity or suboptimal designs. In this work, we propose a novel SGG\nmethod to address the aforementioned issues, formulating the task as a\nbipartite graph construction problem. To address the issues above, we create a\ntransformer-based end-to-end framework to generate the entity and entity-aware\npredicate proposal set, and infer directed edges to form relation triplets.\nMoreover, we design a graph assembling module to infer the connectivity of the\nbipartite scene graph based on our entity-aware structure, enabling us to\ngenerate the scene graph in an end-to-end manner. Based on bipartite graph\nassembling paradigm, we further propose a new technical design to address the\nefficacy of entity-aware modeling and optimization stability of graph\nassembling. Equipped with the enhanced entity-aware design, our method achieves\noptimal performance and time-complexity. Extensive experimental results show\nthat our design is able to achieve the state-of-the-art or comparable\nperformance on three challenging benchmarks, surpassing most of the existing\napproaches and enjoying higher efficiency in inference. Code is available:\nhttps://github.com/Scarecrow0/SGTR\n",
    "link": "http://arxiv.org/abs/2401.12835v1"
  },
  {
    "title": "Enhancing Next Destination Prediction: A Novel LSTM Approach Using\n  Real-World Airline Data",
    "authors": "Salih Salihoglu, Gulser Koksal, Orhan Abar",
    "abstract": "  In the modern transportation industry, accurate prediction of travelers' next\ndestinations brings multiple benefits to companies, such as customer\nsatisfaction and targeted marketing. This study focuses on developing a precise\nmodel that captures the sequential patterns and dependencies in travel data,\nenabling accurate predictions of individual travelers' future destinations. To\nachieve this, a novel model architecture with a sliding window approach based\non Long Short-Term Memory (LSTM) is proposed for destination prediction in the\ntransportation industry. The experimental results highlight satisfactory\nperformance and high scores achieved by the proposed model across different\ndata sizes and performance metrics. This research contributes to advancing\ndestination prediction methods, empowering companies to deliver personalized\nrecommendations and optimize customer experiences in the dynamic travel\nlandscape.\n",
    "link": "http://arxiv.org/abs/2401.12830v1"
  },
  {
    "title": "Deep Learning Based Simulators for the Phosphorus Removal Process\n  Control in Wastewater Treatment via Deep Reinforcement Learning Algorithms",
    "authors": "Esmaeel Mohammadi, Mikkel Stokholm-Bjerregaard, Aviaja Anna Hansen, Per Halkj\u00e6r Nielsen, Daniel Ortiz-Arroyo, Petar Durdevic",
    "abstract": "  Phosphorus removal is vital in wastewater treatment to reduce reliance on\nlimited resources. Deep reinforcement learning (DRL) is a machine learning\ntechnique that can optimize complex and nonlinear systems, including the\nprocesses in wastewater treatment plants, by learning control policies through\ntrial and error. However, applying DRL to chemical and biological processes is\nchallenging due to the need for accurate simulators. This study trained six\nmodels to identify the phosphorus removal process and used them to create a\nsimulator for the DRL environment. Although the models achieved high accuracy\n(&gt;97%), uncertainty and incorrect prediction behavior limited their performance\nas simulators over longer horizons. Compounding errors in the models'\npredictions were identified as one of the causes of this problem. This approach\nfor improving process control involves creating simulation environments for DRL\nalgorithms, using data from supervisory control and data acquisition (SCADA)\nsystems with a sufficient historical horizon without complex system modeling or\nparameter estimation.\n",
    "link": "http://arxiv.org/abs/2401.12822v1"
  },
  {
    "title": "Dynamic Layer Tying for Parameter-Efficient Transformers",
    "authors": "Tamir David Hay, Lior Wolf",
    "abstract": "  In the pursuit of reducing the number of trainable parameters in deep\ntransformer networks, we employ Reinforcement Learning to dynamically select\nlayers during training and tie them together. Every few iterations, the RL\nagent is asked whether to train each layer $i$ independently or to copy the\nweights of a previous layer $j&lt;i$. This facilitates weight sharing, reduces the\nnumber of trainable parameters, and also serves as an effective regularization\ntechnique. Experimental evaluations validate that our model modestly\noutperforms the baseline transformer model with regard to perplexity and\ndrastically reduces the number of trainable parameters. In particular, the\nmemory consumption during training is up to one order of magnitude less than\nthe conventional training method.\n",
    "link": "http://arxiv.org/abs/2401.12819v1"
  },
  {
    "title": "Binary structured physics-informed neural networks for solving equations\n  with rapidly changing solutions",
    "authors": "Yanzhi Liu, Ruifan Wu, Ying Jiang",
    "abstract": "  Physics-informed neural networks (PINNs), rooted in deep learning, have\nemerged as a promising approach for solving partial differential equations\n(PDEs). By embedding the physical information described by PDEs into\nfeedforward neural networks, PINNs are trained as surrogate models to\napproximate solutions without the need for label data. Nevertheless, even\nthough PINNs have shown remarkable performance, they can face difficulties,\nespecially when dealing with equations featuring rapidly changing solutions.\nThese difficulties encompass slow convergence, susceptibility to becoming\ntrapped in local minima, and reduced solution accuracy. To address these\nissues, we propose a binary structured physics-informed neural network (BsPINN)\nframework, which employs binary structured neural network (BsNN) as the neural\nnetwork component. By leveraging a binary structure that reduces inter-neuron\nconnections compared to fully connected neural networks, BsPINNs excel in\ncapturing the local features of solutions more effectively and efficiently.\nThese features are particularly crucial for learning the rapidly changing in\nthe nature of solutions. In a series of numerical experiments solving Burgers\nequation, Euler equation, Helmholtz equation, and high-dimension Poisson\nequation, BsPINNs exhibit superior convergence speed and heightened accuracy\ncompared to PINNs. From these experiments, we discover that BsPINNs resolve the\nissues caused by increased hidden layers in PINNs resulting in over-smoothing,\nand prevent the decline in accuracy due to non-smoothness of PDEs solutions.\n",
    "link": "http://arxiv.org/abs/2401.12806v1"
  },
  {
    "title": "A Review of Deep Learning Methods for Photoplethysmography Data",
    "authors": "Guangkun Nie, Jiabao Zhu, Gongzheng Tang, Deyun Zhang, Shijia Geng, Qinghao Zhao, Shenda Hong",
    "abstract": "  Photoplethysmography (PPG) is a highly promising device due to its advantages\nin portability, user-friendly operation, and non-invasive capabilities to\nmeasure a wide range of physiological information. Recent advancements in deep\nlearning have demonstrated remarkable outcomes by leveraging PPG signals for\ntasks related to personal health management and other multifaceted\napplications. In this review, we systematically reviewed papers that applied\ndeep learning models to process PPG data between January 1st of 2017 and July\n31st of 2023 from Google Scholar, PubMed and Dimensions. Each paper is analyzed\nfrom three key perspectives: tasks, models, and data. We finally extracted 193\npapers where different deep learning frameworks were used to process PPG\nsignals. Based on the tasks addressed in these papers, we categorized them into\ntwo major groups: medical-related, and non-medical-related. The medical-related\ntasks were further divided into seven subgroups, including blood pressure\nanalysis, cardiovascular monitoring and diagnosis, sleep health, mental health,\nrespiratory monitoring and analysis, blood glucose analysis, as well as others.\nThe non-medical-related tasks were divided into four subgroups, which encompass\nsignal processing, biometric identification, electrocardiogram reconstruction,\nand human activity recognition. In conclusion, significant progress has been\nmade in the field of using deep learning methods to process PPG data recently.\nThis allows for a more thorough exploration and utilization of the information\ncontained in PPG signals. However, challenges remain, such as limited quantity\nand quality of publicly available databases, a lack of effective validation in\nreal-world scenarios, and concerns about the interpretability, scalability, and\ncomplexity of deep learning models. Moreover, there are still emerging research\nareas that require further investigation.\n",
    "link": "http://arxiv.org/abs/2401.12783v1"
  },
  {
    "title": "Deep Learning-based Intraoperative MRI Reconstruction",
    "authors": "Jon Andr\u00e9 Ottesen, Tryggve Storas, Svein Are Sirirud Vatnehol, Grethe L\u00f8vland, Einar O. Vik-Mo, Till Schellhorn, Karoline Skogen, Christopher Larsson, Atle Bj\u00f8rnerud, Inge Rasmus Groote-Eindbaas, Matthan W. A. Caan",
    "abstract": "  Purpose: To evaluate the quality of deep learning reconstruction for\nprospectively accelerated intraoperative magnetic resonance imaging (iMRI)\nduring resective brain tumor surgery.\n  Materials and Methods: Accelerated iMRI was performed during brain surgery\nusing dual surface coils positioned around the area of resection. A deep\nlearning (DL) model was trained on the fastMRI neuro dataset to mimic the data\nfrom the iMRI protocol. Evaluation was performed on imaging material from 40\npatients imaged between 01.11.2021 - 01.06.2023 that underwent iMRI during\ntumor resection surgery. A comparative analysis was conducted between the\nconventional compressed sense (CS) method and the trained DL reconstruction\nmethod. Blinded evaluation of multiple image quality metrics was performed by\ntwo working neuro-radiologists and a working neurosurgeon on a 1 to 5 Likert\nscale (1=non diagnostic, 2=poor, 3=acceptable, 4=good, 5=excellent), and the\nfavored reconstruction variant.\n  Results: The DL reconstruction was strongly favored or favored over the CS\nreconstruction for 33/40, 39/40, and 8/40 of cases for reader 1, 2, and 3,\nrespectively. Two of three readers consistently assigned higher ratings for the\nDL reconstructions, and the DL reconstructions had a higher score than their\nrespective CS counterparts for 72%, 72%, and 14% of the cases for reader 1, 2,\nand 3, respectively. Still, the DL reconstructions exhibited shortcomings such\nas a striping artifact and reduced signal.\n  Conclusion: DL shows promise to allow for high-quality reconstructions of\nintraoperative MRI with equal to or improved perceived spatial resolution,\nsignal-to-noise ratio, diagnostic confidence, diagnostic conspicuity, and\nspatial resolution compared to compressed sense.\n",
    "link": "http://arxiv.org/abs/2401.12771v1"
  },
  {
    "title": "What the Weight?! A Unified Framework for Zero-Shot Knowledge\n  Composition",
    "authors": "Carolin Holtermann, Markus Frohmann, Navid Rekabsaz, Anne Lauscher",
    "abstract": "  The knowledge encapsulated in a model is the core factor determining its\nfinal performance on downstream tasks. Much research in NLP has focused on\nefficient methods for storing and adapting different types of knowledge, e.g.,\nin dedicated modularized structures, and on how to effectively combine these,\ne.g., by learning additional parameters. However, given the many possible\noptions, a thorough understanding of the mechanisms involved in these\ncompositions is missing, and hence it remains unclear which strategies to\nutilize. To address this research gap, we propose a novel framework for\nzero-shot module composition, which encompasses existing and some novel\nvariations for selecting, weighting, and combining parameter modules under a\nsingle unified notion. Focusing on the scenario of domain knowledge and adapter\nlayers, our framework provides a systematic unification of concepts, allowing\nus to conduct the first comprehensive benchmarking study of various zero-shot\nknowledge composition strategies. In particular, we test two module combination\nmethods and five selection and weighting strategies for their effectiveness and\nefficiency in an extensive experimental setup. Our results highlight the\nefficacy of ensembling but also hint at the power of simple though\noften-ignored weighting methods. Further in-depth analyses allow us to\nunderstand the role of weighting vs. top-k selection, and show that, to a\ncertain extent, the performance of adapter composition can even be predicted.\n",
    "link": "http://arxiv.org/abs/2401.12756v1"
  },
  {
    "title": "The Distributional Uncertainty of the SHAP score in Explainable Machine\n  Learning",
    "authors": "Santiago Cifuentes, Leopoldo Bertossi, Nina Pardal, Sergio Abriola, Maria Vanina Martinez, Miguel Romero",
    "abstract": "  Attribution scores reflect how important the feature values in an input\nentity are for the output of a machine learning model. One of the most popular\nattribution scores is the SHAP score, which is an instantiation of the general\nShapley value used in coalition game theory. The definition of this score\nrelies on a probability distribution on the entity population. Since the exact\ndistribution is generally unknown, it needs to be assigned subjectively or be\nestimated from data, which may lead to misleading feature scores. In this\npaper, we propose a principled framework for reasoning on SHAP scores under\nunknown entity population distributions. In our framework, we consider an\nuncertainty region that contains the potential distributions, and the SHAP\nscore of a feature becomes a function defined over this region. We study the\nbasic problems of finding maxima and minima of this function, which allows us\nto determine tight ranges for the SHAP scores of all features. In particular,\nwe pinpoint the complexity of these problems, and other related ones, showing\nthem to be NP-complete. Finally, we present experiments on a real-world\ndataset, showing that our framework may contribute to a more robust feature\nscoring.\n",
    "link": "http://arxiv.org/abs/2401.12731v1"
  },
  {
    "title": "Evaluation of large language models for assessing code maintainability",
    "authors": "Marc Dillmann, Julien Siebert, Adam Trendowicz",
    "abstract": "  Increased availability of open-source software repositories and recent\nadvances in code analysis using large language models (LLMs) has triggered a\nwave of new work to automate software engineering tasks that were previously\nvery difficult to automate. In this paper, we investigate a recent line of work\nthat hypothesises that comparing the probability of code generated by LLMs with\nthe probability the current code would have had can indicate potential quality\nproblems. We investigate the association between the cross-entropy of code\ngenerated by ten different models (based on GPT2 and Llama2) and the following\nquality aspects: readability, understandability, complexity, modularisation,\nand overall maintainability assessed by experts and available in an benchmark\ndataset. Our results show that, controlling for the number of logical lines of\ncodes (LLOC), cross-entropy computed by LLMs is indeed a predictor of\nmaintainability on a class level (the higher the cross-entropy the lower the\nmaintainability). However, this relation is reversed when one does not control\nfor LLOC (e.g., comparing small classes with longer ones). Furthermore, while\nthe complexity of LLMs affects the range of cross-entropy (smaller models tend\nto have a wider range of cross-entropy), this plays a significant role in\npredicting maintainability aspects. Our study limits itself on ten different\npretrained models (based on GPT2 and Llama2) and on maintainability aspects\ncollected by Schnappinger et al. When controlling for logical lines of code\n(LLOC), cross-entropy is a predictor of maintainability. However, while related\nwork has shown the potential usefulness of cross-entropy at the level of tokens\nor short sequences, at the class level this criterion alone may prove\ninsufficient to predict maintainability and further research is needed to make\nbest use of this information in practice.\n",
    "link": "http://arxiv.org/abs/2401.12714v1"
  },
  {
    "title": "Deep Neural Network Benchmarks for Selective Classification",
    "authors": "Andrea Pugnana, Lorenzo Perini, Jesse Davis, Salvatore Ruggieri",
    "abstract": "  With the increasing deployment of machine learning models in many\nsocially-sensitive tasks, there is a growing demand for reliable and\ntrustworthy predictions. One way to accomplish these requirements is to allow a\nmodel to abstain from making a prediction when there is a high risk of making\nan error. This requires adding a selection mechanism to the model, which\nselects those examples for which the model will provide a prediction. The\nselective classification framework aims to design a mechanism that balances the\nfraction of rejected predictions (i.e., the proportion of examples for which\nthe model does not make a prediction) versus the improvement in predictive\nperformance on the selected predictions. Multiple selective classification\nframeworks exist, most of which rely on deep neural network architectures.\nHowever, the empirical evaluation of the existing approaches is still limited\nto partial comparisons among methods and settings, providing practitioners with\nlittle insight into their relative merits. We fill this gap by benchmarking 18\nbaselines on a diverse set of 44 datasets that includes both image and tabular\ndata. Moreover, there is a mix of binary and multiclass tasks. We evaluate\nthese approaches using several criteria, including selective error rate,\nempirical coverage, distribution of rejected instance's classes, and\nperformance on out-of-distribution instances. The results indicate that there\nis not a single clear winner among the surveyed baselines, and the best method\ndepends on the users' objectives.\n",
    "link": "http://arxiv.org/abs/2401.12708v1"
  },
  {
    "title": "Securing Recommender System via Cooperative Training",
    "authors": "Qingyang Wang, Chenwang Wu, Defu Lian, Enhong Chen",
    "abstract": "  Recommender systems are often susceptible to well-crafted fake profiles,\nleading to biased recommendations. Among existing defense methods,\ndata-processing-based methods inevitably exclude normal samples, while\nmodel-based methods struggle to enjoy both generalization and robustness. To\nthis end, we suggest integrating data processing and the robust model to\npropose a general framework, Triple Cooperative Defense (TCD), which employs\nthree cooperative models that mutually enhance data and thereby improve\nrecommendation robustness. Furthermore, Considering that existing attacks\nstruggle to balance bi-level optimization and efficiency, we revisit poisoning\nattacks in recommender systems and introduce an efficient attack strategy,\nCo-training Attack (Co-Attack), which cooperatively optimizes the attack\noptimization and model training, considering the bi-level setting while\nmaintaining attack efficiency. Moreover, we reveal a potential reason for the\ninsufficient threat of existing attacks is their default assumption of\noptimizing attacks in undefended scenarios. This overly optimistic setting\nlimits the potential of attacks. Consequently, we put forth a Game-based\nCo-training Attack (GCoAttack), which frames the proposed CoAttack and TCD as a\ngame-theoretic process, thoroughly exploring CoAttack's attack potential in the\ncooperative training of attack and defense. Extensive experiments on three real\ndatasets demonstrate TCD's superiority in enhancing model robustness.\nAdditionally, we verify that the two proposed attack strategies significantly\noutperform existing attacks, with game-based GCoAttack posing a greater\npoisoning threat than CoAttack.\n",
    "link": "http://arxiv.org/abs/2401.12700v1"
  },
  {
    "title": "Energy-based Automated Model Evaluation",
    "authors": "Ru Peng, Heming Zou, Haobo Wang, Yawen Zeng, Zenan Huang, Junbo Zhao",
    "abstract": "  The conventional evaluation protocols on machine learning models rely heavily\non a labeled, i.i.d-assumed testing dataset, which is not often present in real\nworld applications. The Automated Model Evaluation (AutoEval) shows an\nalternative to this traditional workflow, by forming a proximal prediction\npipeline of the testing performance without the presence of ground-truth\nlabels. Despite its recent successes, the AutoEval frameworks still suffer from\nan overconfidence issue, substantial storage and computational cost. In that\nregard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that\nallows the AutoEval framework to be both more efficient and effective. The core\nof the MDE is to establish a meta-distribution statistic, on the information\n(energy) associated with individual samples, then offer a smoother\nrepresentation enabled by energy-based learning. We further provide our\ntheoretical insights by connecting the MDE with the classification loss. We\nprovide extensive experiments across modalities, datasets and different\narchitectural backbones to validate MDE's validity, together with its\nsuperiority compared with prior approaches. We also prove MDE's versatility by\nshowing its seamless integration with large-scale models, and easy adaption to\nlearning scenarios with noisy- or imbalanced- labels.\n",
    "link": "http://arxiv.org/abs/2401.12689v1"
  },
  {
    "title": "Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach",
    "authors": "Christian Fabian, Kai Cui, Heinz Koeppl",
    "abstract": "  Learning the behavior of large agent populations is an important task for\nnumerous research areas. Although the field of multi-agent reinforcement\nlearning (MARL) has made significant progress towards solving these systems,\nsolutions for many agents often remain computationally infeasible and lack\ntheoretical guarantees. Mean Field Games (MFGs) address both of these issues\nand can be extended to Graphon MFGs (GMFGs) to include network structures\nbetween agents. Despite their merits, the real world applicability of GMFGs is\nlimited by the fact that graphons only capture dense graphs. Since most\nempirically observed networks show some degree of sparsity, such as power law\ngraphs, the GMFG framework is insufficient for capturing these network\ntopologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which\nbuilds on the graph theoretical concept of graphexes. Graphexes are the\nlimiting objects to sparse graph sequences that also have other desirable\nfeatures such as the small world property. Learning equilibria in these games\nis challenging due to the rich and sparse structure of the underlying graphs.\nTo tackle these challenges, we design a new learning algorithm tailored to the\nGXMFG setup. This hybrid graphex learning approach leverages that the system\nmainly consists of a highly connected core and a sparse periphery. After\ndefining the system and providing a theoretical analysis, we state our learning\napproach and demonstrate its learning capabilities on both synthetic graphs and\nreal-world networks. This comparison shows that our GXMFG learning algorithm\nsuccessfully extends MFGs to a highly relevant class of hard, realistic\nlearning problems that are not accurately addressed by current MARL and MFG\nmethods.\n",
    "link": "http://arxiv.org/abs/2401.12686v1"
  },
  {
    "title": "Non-Neighbors Also Matter to Kriging: A New Contrastive-Prototypical\n  Learning",
    "authors": "Zhishuai Li, Yunhao Nie, Ziyue Li, Lei Bai, Yisheng Lv, Rui Zhao",
    "abstract": "  Kriging aims at estimating the attributes of unsampled geo-locations from\nobservations in the spatial vicinity or physical connections, which helps\nmitigate skewed monitoring caused by under-deployed sensors. Existing works\nassume that neighbors' information offers the basis for estimating the\nattributes of the unobserved target while ignoring non-neighbors. However,\nnon-neighbors could also offer constructive information, and neighbors could\nalso be misleading. To this end, we propose ``Contrastive-Prototypical''\nself-supervised learning for Kriging (KCP) to refine valuable information from\nneighbors and recycle the one from non-neighbors. As a pre-trained paradigm, we\nconduct the Kriging task from a new perspective of representation: we aim to\nfirst learn robust and general representations and then recover attributes from\nrepresentations. A neighboring contrastive module is designed that coarsely\nlearns the representations by narrowing the representation distance between the\ntarget and its neighbors while pushing away the non-neighbors. In parallel, a\nprototypical module is introduced to identify similar representations via\nexchanged prediction, thus refining the misleading neighbors and recycling the\nuseful non-neighbors from the neighboring contrast component. As a result, not\nall the neighbors and some of the non-neighbors will be used to infer the\ntarget. To encourage the two modules above to learn general and robust\nrepresentations, we design an adaptive augmentation module that incorporates\ndata-driven attribute augmentation and centrality-based topology augmentation\nover the spatiotemporal Kriging graph data. Extensive experiments on real-world\ndatasets demonstrate the superior performance of KCP compared to its peers with\n6% improvements and exceptional transferability and robustness. The code is\navailable at https://github.com/bonaldli/KCP\n",
    "link": "http://arxiv.org/abs/2401.12681v1"
  },
  {
    "title": "ChatGraph: Chat with Your Graphs",
    "authors": "Yun Peng, Sen Lin, Qian Chen, Lyu Xu, Xiaojun Ren, Yafei Li, Jianliang Xu",
    "abstract": "  Graph analysis is fundamental in real-world applications. Traditional\napproaches rely on SPARQL-like languages or clicking-and-dragging interfaces to\ninteract with graph data. However, these methods either require users to\npossess high programming skills or support only a limited range of graph\nanalysis functionalities. To address the limitations, we propose a large\nlanguage model (LLM)-based framework called ChatGraph. With ChatGraph, users\ncan interact with graphs through natural language, making it easier to use and\nmore flexible than traditional approaches. The core of ChatGraph lies in\ngenerating chains of graph analysis APIs based on the understanding of the\ntexts and graphs inputted in the user prompts. To achieve this, ChatGraph\nconsists of three main modules: an API retrieval module that searches for\nrelevant APIs, a graph-aware LLM module that enables the LLM to comprehend\ngraphs, and an API chain-oriented finetuning module that guides the LLM in\ngenerating API chains.\n",
    "link": "http://arxiv.org/abs/2401.12672v1"
  },
  {
    "title": "EL-VIT: Probing Vision Transformer with Interactive Visualization",
    "authors": "Hong Zhou, Rui Zhang, Peifeng Lai, Chaoran Guo, Yong Wang, Zhida Sun, Junjie Li",
    "abstract": "  Nowadays, Vision Transformer (ViT) is widely utilized in various computer\nvision tasks, owing to its unique self-attention mechanism. However, the model\narchitecture of ViT is complex and often challenging to comprehend, leading to\na steep learning curve. ViT developers and users frequently encounter\ndifficulties in interpreting its inner workings. Therefore, a visualization\nsystem is needed to assist ViT users in understanding its functionality. This\npaper introduces EL-VIT, an interactive visual analytics system designed to\nprobe the Vision Transformer and facilitate a better understanding of its\noperations. The system consists of four layers of visualization views. The\nfirst three layers include model overview, knowledge background graph, and\nmodel detail view. These three layers elucidate the operation process of ViT\nfrom three perspectives: the overall model architecture, detailed explanation,\nand mathematical operations, enabling users to understand the underlying\nprinciples and the transition process between layers. The fourth interpretation\nview helps ViT users and experts gain a deeper understanding by calculating the\ncosine similarity between patches. Our two usage scenarios demonstrate the\neffectiveness and usability of EL-VIT in helping ViT users understand the\nworking mechanism of ViT.\n",
    "link": "http://arxiv.org/abs/2401.12666v1"
  },
  {
    "title": "ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation",
    "authors": "Shengze Li, Jianjian Cao, Peng Ye, Yuhan Ding, Chongjun Tu, Tao Chen",
    "abstract": "  Recently, foundational models such as CLIP and SAM have shown promising\nperformance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However,\neither CLIP-based or SAM-based ZSAS methods still suffer from non-negligible\nkey drawbacks: 1) CLIP primarily focuses on global feature alignment across\ndifferent inputs, leading to imprecise segmentation of local anomalous parts;\n2) SAM tends to generate numerous redundant masks without proper prompt\nconstraints, resulting in complex post-processing requirements. In this work,\nwe innovatively propose a CLIP and SAM collaboration framework called ClipSAM\nfor ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding\ncapability for anomaly localization and rough segmentation, which is further\nused as the prompt constraints for SAM to refine the anomaly segmentation\nresults. In details, we introduce a crucial Unified Multi-scale Cross-modal\nInteraction (UMCI) module for interacting language with visual features at\nmultiple scales of CLIP to reason anomaly positions. Then, we design a novel\nMulti-level Mask Refinement (MMR) module, which utilizes the positional\ninformation as multi-level prompts for SAM to acquire hierarchical levels of\nmasks and merges them. Extensive experiments validate the effectiveness of our\napproach, achieving the optimal segmentation performance on the MVTec-AD and\nVisA datasets.\n",
    "link": "http://arxiv.org/abs/2401.12665v1"
  },
  {
    "title": "Integrating Human Expertise in Continuous Spaces: A Novel Interactive\n  Bayesian Optimization Framework with Preference Expected Improvement",
    "authors": "Nikolaus Feith, Elmar Rueckert",
    "abstract": "  Interactive Machine Learning (IML) seeks to integrate human expertise into\nmachine learning processes. However, most existing algorithms cannot be applied\nto Realworld Scenarios because their state spaces and/or action spaces are\nlimited to discrete values. Furthermore, the interaction of all existing\nmethods is restricted to deciding between multiple proposals. We therefore\npropose a novel framework based on Bayesian Optimization (BO). Interactive\nBayesian Optimization (IBO) enables collaboration between machine learning\nalgorithms and humans. This framework captures user preferences and provides an\ninterface for users to shape the strategy by hand. Additionally, we've\nincorporated a new acquisition function, Preference Expected Improvement (PEI),\nto refine the system's efficiency using a probabilistic model of the user\npreferences. Our approach is geared towards ensuring that machines can benefit\nfrom human expertise, aiming for a more aligned and effective learning process.\nIn the course of this work, we applied our method to simulations and in a real\nworld task using a Franka Panda robot to show human-robot collaboration.\n",
    "link": "http://arxiv.org/abs/2401.12662v1"
  },
  {
    "title": "Emergent Cooperation under Uncertain Incentive Alignment",
    "authors": "Nicole Orzan, Erman Acar, Davide Grossi, Roxana R\u0103dulescu",
    "abstract": "  Understanding the emergence of cooperation in systems of computational agents\nis crucial for the development of effective cooperative AI. Interaction among\nindividuals in real-world settings are often sparse and occur within a broad\nspectrum of incentives, which often are only partially known. In this work, we\nexplore how cooperation can arise among reinforcement learning agents in\nscenarios characterised by infrequent encounters, and where agents face\nuncertainty about the alignment of their incentives with those of others. To do\nso, we train the agents under a wide spectrum of environments ranging from\nfully competitive, to fully cooperative, to mixed-motives. Under this type of\nuncertainty we study the effects of mechanisms, such as reputation and\nintrinsic rewards, that have been proposed in the literature to foster\ncooperation in mixed-motives environments. Our findings show that uncertainty\nsubstantially lowers the agents' ability to engage in cooperative behaviour,\nwhen that would be the best course of action. In this scenario, the use of\neffective reputation mechanisms and intrinsic rewards boosts the agents'\ncapability to act nearly-optimally in cooperative environments, while greatly\nenhancing cooperation in mixed-motive environments as well.\n",
    "link": "http://arxiv.org/abs/2401.12646v1"
  },
  {
    "title": "Modeling Resilience of Collaborative AI Systems",
    "authors": "Diaeddin Rimawi, Antonio Liotta, Marco Todescato, Barbara Russo",
    "abstract": "  A Collaborative Artificial Intelligence System (CAIS) performs actions in\ncollaboration with the human to achieve a common goal. CAISs can use a trained\nAI model to control human-system interaction, or they can use human interaction\nto dynamically learn from humans in an online fashion. In online learning with\nhuman feedback, the AI model evolves by monitoring human interaction through\nthe system sensors in the learning state, and actuates the autonomous\ncomponents of the CAIS based on the learning in the operational state.\nTherefore, any disruptive event affecting these sensors may affect the AI\nmodel's ability to make accurate decisions and degrade the CAIS performance.\nConsequently, it is of paramount importance for CAIS managers to be able to\nautomatically track the system performance to understand the resilience of the\nCAIS upon such disruptive events. In this paper, we provide a new framework to\nmodel CAIS performance when the system experiences a disruptive event. With our\nframework, we introduce a model of performance evolution of CAIS. The model is\nequipped with a set of measures that aim to support CAIS managers in the\ndecision process to achieve the required resilience of the system. We tested\nour framework on a real-world case study of a robot collaborating online with\nthe human, when the system is experiencing a disruptive event. The case study\nshows that our framework can be adopted in CAIS and integrated into the online\nexecution of the CAIS activities.\n",
    "link": "http://arxiv.org/abs/2401.12632v1"
  },
  {
    "title": "A Reply to Makelov et al. (2023)'s \"Interpretability Illusion\" Arguments",
    "authors": "Zhengxuan Wu, Atticus Geiger, Jing Huang, Aryaman Arora, Thomas Icard, Christopher Potts, Noah D. Goodman",
    "abstract": "  We respond to the recent paper by Makelov et al. (2023), which reviews\nsubspace interchange intervention methods like distributed alignment search\n(DAS; Geiger et al. 2023) and claims that these methods potentially cause\n\"interpretability illusions\". We first review Makelov et al. (2023)'s technical\nnotion of what an \"interpretability illusion\" is, and then we show that even\nintuitive and desirable explanations can qualify as illusions in this sense. As\na result, their method of discovering \"illusions\" can reject explanations they\nconsider \"non-illusory\". We then argue that the illusions Makelov et al. (2023)\nsee in practice are artifacts of their training and evaluation paradigms. We\nclose by emphasizing that, though we disagree with their core characterization,\nMakelov et al. (2023)'s examples and discussion have undoubtedly pushed the\nfield of interpretability forward.\n",
    "link": "http://arxiv.org/abs/2401.12631v1"
  },
  {
    "title": "Knowledge Distillation from Language-Oriented to Emergent Communication\n  for Multi-Agent Remote Control",
    "authors": "Yongjun Kim, Sejin Seo, Jihong Park, Mehdi Bennis, Seong-Lyun Kim, Junil Choi",
    "abstract": "  In this work, we compare emergent communication (EC) built upon multi-agent\ndeep reinforcement learning (MADRL) and language-oriented semantic\ncommunication (LSC) empowered by a pre-trained large language model (LLM) using\nhuman language. In a multi-agent remote navigation task, with multimodal input\ndata comprising location and channel maps, it is shown that EC incurs high\ntraining cost and struggles when using multimodal data, whereas LSC yields high\ninference computing cost due to the LLM's large size. To address their\nrespective bottlenecks, we propose a novel framework of language-guided EC\n(LEC) by guiding the EC training using LSC via knowledge distillation (KD).\nSimulations corroborate that LEC achieves faster travel time while avoiding\nareas with poor channel conditions, as well as speeding up the MADRL training\nconvergence by up to 61.8% compared to EC.\n",
    "link": "http://arxiv.org/abs/2401.12624v1"
  },
  {
    "title": "Revolutionizing Retrieval-Augmented Generation with Enhanced PDF\n  Structure Recognition",
    "authors": "Demiao Lin",
    "abstract": "  With the rapid development of Large Language Models (LLMs),\nRetrieval-Augmented Generation (RAG) has become a predominant method in the\nfield of professional knowledge-based question answering. Presently, major\nfoundation model companies have opened up Embedding and Chat API interfaces,\nand frameworks like LangChain have already integrated the RAG process. It\nappears that the key models and steps in RAG have been resolved, leading to the\nquestion: are professional knowledge QA systems now approaching perfection?\nThis article discovers that current primary methods depend on the premise of\naccessing high-quality text corpora. However, since professional documents are\nmainly stored in PDFs, the low accuracy of PDF parsing significantly impacts\nthe effectiveness of professional knowledge-based QA. We conducted an empirical\nRAG experiment across hundreds of questions from the corresponding real-world\nprofessional documents. The results show that, ChatDOC, a RAG system equipped\nwith a panoptic and pinpoint PDF parser, retrieves more accurate and complete\nsegments, and thus better answers. Empirical experiments show that ChatDOC is\nsuperior to baseline on nearly 47% of questions, ties for 38% of cases, and\nfalls short on only 15% of cases. It shows that we may revolutionize RAG with\nenhanced PDF structure recognition.\n",
    "link": "http://arxiv.org/abs/2401.12599v1"
  },
  {
    "title": "MOReGIn: Multi-Objective Recommendation at the Global and Individual\n  Levels",
    "authors": "Elizabeth G\u00f3mez, David Contreras, Ludovico Boratto, Maria Salam\u00f3",
    "abstract": "  Multi-Objective Recommender Systems (MORSs) emerged as a paradigm to\nguarantee multiple (often conflicting) goals. Besides accuracy, a MORS can\noperate at the global level, where additional beyond-accuracy goals are met for\nthe system as a whole, or at the individual level, meaning that the\nrecommendations are tailored to the needs of each user. The state-of-the-art\nMORSs either operate at the global or individual level, without assuming the\nco-existence of the two perspectives. In this study, we show that when global\nand individual objectives co-exist, MORSs are not able to meet both types of\ngoals. To overcome this issue, we present an approach that regulates the\nrecommendation lists so as to guarantee both global and individual\nperspectives, while preserving its effectiveness. Specifically, as individual\nperspective, we tackle genre calibration and, as global perspective, provider\nfairness. We validate our approach on two real-world datasets, publicly\nreleased with this paper.\n",
    "link": "http://arxiv.org/abs/2401.12593v1"
  },
  {
    "title": "LLMCheckup: Conversational Examination of Large Language Models via\n  Interpretability Tools",
    "authors": "Qianli Wang, Tatiana Anikina, Nils Feldhus, Josef van Genabith, Leonhard Hennig, Sebastian M\u00f6ller",
    "abstract": "  Interpretability tools that offer explanations in the form of a dialogue have\ndemonstrated their efficacy in enhancing users' understanding, as one-off\nexplanations may occasionally fall short in providing sufficient information to\nthe user. Current solutions for dialogue-based explanations, however, require\nmany dependencies and are not easily transferable to tasks they were not\ndesigned for. With LLMCheckup, we present an easily accessible tool that allows\nusers to chat with any state-of-the-art large language model (LLM) about its\nbehavior. We enable LLMs to generate all explanations by themselves and take\ncare of intent recognition without fine-tuning, by connecting them with a broad\nspectrum of Explainable AI (XAI) tools, e.g. feature attributions,\nembedding-based similarity, and prompting strategies for counterfactual and\nrationale generation. LLM (self-)explanations are presented as an interactive\ndialogue that supports follow-up questions and generates suggestions.\nLLMCheckup provides tutorials for operations available in the system, catering\nto individuals with varying levels of expertise in XAI and supports multiple\ninput modalities. We introduce a new parsing strategy called multi-prompt\nparsing substantially enhancing the parsing accuracy of LLMs. Finally, we\nshowcase the tasks of fact checking and commonsense question answering.\n",
    "link": "http://arxiv.org/abs/2401.12576v1"
  },
  {
    "title": "DiffMoog: a Differentiable Modular Synthesizer for Sound Matching",
    "authors": "Noy Uzrad, Oren Barkan, Almog Elharar, Shlomi Shvartzman, Moshe Laufer, Lior Wolf, Noam Koenigstein",
    "abstract": "  This paper presents DiffMoog - a differentiable modular synthesizer with a\ncomprehensive set of modules typically found in commercial instruments. Being\ndifferentiable, it allows integration into neural networks, enabling automated\nsound matching, to replicate a given audio input. Notably, DiffMoog facilitates\nmodulation capabilities (FM/AM), low-frequency oscillators (LFOs), filters,\nenvelope shapers, and the ability for users to create custom signal chains. We\nintroduce an open-source platform that comprises DiffMoog and an end-to-end\nsound matching framework. This framework utilizes a novel signal-chain loss and\nan encoder network that self-programs its outputs to predict DiffMoogs\nparameters based on the user-defined modular architecture. Moreover, we provide\ninsights and lessons learned towards sound matching using differentiable\nsynthesis. Combining robust sound capabilities with a holistic platform,\nDiffMoog stands as a premier asset for expediting research in audio synthesis\nand machine learning.\n",
    "link": "http://arxiv.org/abs/2401.12570v1"
  },
  {
    "title": "Balancing the AI Strength of Roles in Self-Play Training with Regret\n  Matching+",
    "authors": "Xiaoxi Wang",
    "abstract": "  When training artificial intelligence for games encompassing multiple roles,\nthe development of a generalized model capable of controlling any character\nwithin the game presents a viable option. This strategy not only conserves\ncomputational resources and time during the training phase but also reduces\nresource requirements during deployment. training such a generalized model\noften encounters challenges related to uneven capabilities when controlling\ndifferent roles. A simple method is introduced based on Regret Matching+, which\nfacilitates a more balanced performance of strength by the model when\ncontrolling various roles.\n",
    "link": "http://arxiv.org/abs/2401.12557v1"
  },
  {
    "title": "Can Large Language Models Write Parallel Code?",
    "authors": "Daniel Nichols, Joshua H. Davis, Zhaojun Xie, Arjun Rajaram, Abhinav Bhatele",
    "abstract": "  Large Language Models are becoming an increasingly popular tool for software\ndevelopment. Their ability to model and generate source code has been\ndemonstrated in a variety of contexts, including code completion,\nsummarization, translation, and lookup. However, they often struggle to\ngenerate code for more complex tasks. In this paper, we explore the ability of\nstate-of-the-art language models to generate parallel code. We propose a\nbenchmark, PCGBench, consisting of a set of 420 tasks for evaluating the\nability of language models to generate parallel code, and we evaluate the\nperformance of several state-of-the-art open- and closed-source language models\non these tasks. We introduce novel metrics for comparing parallel code\ngeneration performance and use them to explore how well each LLM performs on\nvarious parallel programming models and computational problem types.\n",
    "link": "http://arxiv.org/abs/2401.12554v1"
  },
  {
    "title": "UR4NNV: Neural Network Verification, Under-approximation Reachability\n  Works!",
    "authors": "Zhen Liang, Taoran Wu, Ran Zhao, Bai Xue, Ji Wang, Wenjing Yang, Shaojun Deng, Wanwei Liu",
    "abstract": "  Recently, formal verification of deep neural networks (DNNs) has garnered\nconsiderable attention, and over-approximation based methods have become\npopular due to their effectiveness and efficiency. However, these strategies\nface challenges in addressing the \"unknown dilemma\" concerning whether the\nexact output region or the introduced approximation error violates the property\nin question. To address this, this paper introduces the UR4NNV verification\nframework, which utilizes under-approximation reachability analysis for DNN\nverification for the first time. UR4NNV focuses on DNNs with Rectified Linear\nUnit (ReLU) activations and employs a binary tree branch-based\nunder-approximation algorithm. In each epoch, UR4NNV under-approximates a\nsub-polytope of the reachable set and verifies this polytope against the given\nproperty. Through a trial-and-error approach, UR4NNV effectively falsifies DNN\nproperties while providing confidence levels when reaching verification epoch\nbounds and failing falsifying properties. Experimental comparisons with\nexisting verification methods demonstrate the effectiveness and efficiency of\nUR4NNV, significantly reducing the impact of the \"unknown dilemma\".\n",
    "link": "http://arxiv.org/abs/2401.12550v1"
  },
  {
    "title": "Efficient Constrained $k$-Center Clustering with Background Knowledge",
    "authors": "Longkun Guo, Chaoqi Jia, Kewen Liao, Zhigang Lu, Minhui Xue",
    "abstract": "  Center-based clustering has attracted significant research interest from both\ntheory and practice. In many practical applications, input data often contain\nbackground knowledge that can be used to improve clustering results. In this\nwork, we build on widely adopted $k$-center clustering and model its input\nbackground knowledge as must-link (ML) and cannot-link (CL) constraint sets.\nHowever, most clustering problems including $k$-center are inherently\n$\\mathcal{NP}$-hard, while the more complex constrained variants are known to\nsuffer severer approximation and computation barriers that significantly limit\ntheir applicability. By employing a suite of techniques including reverse\ndominating sets, linear programming (LP) integral polyhedron, and LP duality,\nwe arrive at the first efficient approximation algorithm for constrained\n$k$-center with the best possible ratio of 2. We also construct competitive\nbaseline algorithms and empirically evaluate our approximation algorithm\nagainst them on a variety of real datasets. The results validate our\ntheoretical findings and demonstrate the great advantages of our algorithm in\nterms of clustering cost, clustering quality, and running time.\n",
    "link": "http://arxiv.org/abs/2401.12533v1"
  },
  {
    "title": "DAFA: Distance-Aware Fair Adversarial Training",
    "authors": "Hyungyu Lee, Saehyung Lee, Hyemi Jang, Junsung Park, Ho Bae, Sungroh Yoon",
    "abstract": "  The disparity in accuracy between classes in standard training is amplified\nduring adversarial training, a phenomenon termed the robust fairness problem.\nExisting methodologies aimed to enhance robust fairness by sacrificing the\nmodel's performance on easier classes in order to improve its performance on\nharder ones. However, we observe that under adversarial attacks, the majority\nof the model's predictions for samples from the worst class are biased towards\nclasses similar to the worst class, rather than towards the easy classes.\nThrough theoretical and empirical analysis, we demonstrate that robust fairness\ndeteriorates as the distance between classes decreases. Motivated by these\ninsights, we introduce the Distance-Aware Fair Adversarial training (DAFA)\nmethodology, which addresses robust fairness by taking into account the\nsimilarities between classes. Specifically, our method assigns distinct loss\nweights and adversarial margins to each class and adjusts them to encourage a\ntrade-off in robustness among similar classes. Experimental results across\nvarious datasets demonstrate that our method not only maintains average robust\naccuracy but also significantly improves the worst robust accuracy, indicating\na marked improvement in robust fairness compared to existing methods.\n",
    "link": "http://arxiv.org/abs/2401.12532v1"
  }
]