[
  {
    "title": "Vlogger: Make Your Dream A Vlog",
    "authors": "Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu Qiao, Yali Wang",
    "abstract": "  In this work, we present Vlogger, a generic AI system for generating a\nminute-level video blog (i.e., vlog) of user descriptions. Different from short\nvideos with a few seconds, vlog often contains a complex storyline with\ndiversified scenes, which is challenging for most existing video generation\napproaches. To break through this bottleneck, our Vlogger smartly leverages\nLarge Language Model (LLM) as Director and decomposes a long video generation\ntask of vlog into four key stages, where we invoke various foundation models to\nplay the critical roles of vlog professionals, including (1) Script, (2) Actor,\n(3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings,\nour Vlogger can generate vlogs through explainable cooperation of top-down\nplanning and bottom-up shooting. Moreover, we introduce a novel video diffusion\nmodel, ShowMaker, which serves as a videographer in our Vlogger for generating\nthe video snippet of each shooting scene. By incorporating Script and Actor\nattentively as textual and visual prompts, it can effectively enhance\nspatial-temporal coherence in the snippet. Besides, we design a concise mixed\ntraining paradigm for ShowMaker, boosting its capacity for both T2V generation\nand prediction. Finally, the extensive experiments show that our method\nachieves state-of-the-art performance on zero-shot T2V generation and\nprediction tasks. More importantly, Vlogger can generate over 5-minute vlogs\nfrom open-world descriptions, without loss of video coherence on script and\nactor. The code and model is all available at\nhttps://github.com/zhuangshaobin/Vlogger.\n",
    "link": "http://arxiv.org/abs/2401.09414v1"
  },
  {
    "title": "Through the Looking-Glass: Transparency Implications and Challenges in\n  Enterprise AI Knowledge Systems",
    "authors": "Karina Corti\u00f1as-Lorenzo, Si\u00e2n Lindley, Ida Larsen-Ledet, Bhaskar Mitra",
    "abstract": "  Knowledge can't be disentangled from people. As AI knowledge systems mine\nvast volumes of work-related data, the knowledge that's being extracted and\nsurfaced is intrinsically linked to the people who create and use it. When\nthese systems get embedded in organizational settings, the information that is\nbrought to the foreground and the information that's pushed to the periphery\ncan influence how individuals see each other and how they see themselves at\nwork. In this paper, we present the looking-glass metaphor and use it to\nconceptualize AI knowledge systems as systems that reflect and distort,\nexpanding our view on transparency requirements, implications and challenges.\nWe formulate transparency as a key mediator in shaping different ways of\nseeing, including seeing into the system, which unveils its capabilities,\nlimitations and behavior, and seeing through the system, which shapes workers'\nperceptions of their own contributions and others within the organization.\nRecognizing the sociotechnical nature of these systems, we identify three\ntransparency dimensions necessary to realize the value of AI knowledge systems,\nnamely system transparency, procedural transparency and transparency of\noutcomes. We discuss key challenges hindering the implementation of these forms\nof transparency, bringing to light the wider sociotechnical gap and\nhighlighting directions for future Computer-supported Cooperative Work (CSCW)\nresearch.\n",
    "link": "http://arxiv.org/abs/2401.09410v1"
  },
  {
    "title": "Neural Contractive Dynamical Systems",
    "authors": "Hadi Beik-Mohammadi, S\u00f8ren Hauberg, Georgios Arvanitidis, Nadia Figueroa, Gerhard Neumann, Leonel Rozo",
    "abstract": "  Stability guarantees are crucial when ensuring a fully autonomous robot does\nnot take undesirable or potentially harmful actions. Unfortunately, global\nstability guarantees are hard to provide in dynamical systems learned from\ndata, especially when the learned dynamics are governed by neural networks. We\npropose a novel methodology to learn neural contractive dynamical systems,\nwhere our neural architecture ensures contraction, and hence, global stability.\nTo efficiently scale the method to high-dimensional dynamical systems, we\ndevelop a variant of the variational autoencoder that learns dynamics in a\nlow-dimensional latent representation space while retaining contractive\nstability after decoding. We further extend our approach to learning\ncontractive systems on the Lie group of rotations to account for full-pose\nend-effector dynamic motions. The result is the first highly flexible learning\narchitecture that provides contractive stability guarantees with capability to\nperform obstacle avoidance. Empirically, we demonstrate that our approach\nencodes the desired dynamics more accurately than the current state-of-the-art,\nwhich provides less strong stability guarantees.\n",
    "link": "http://arxiv.org/abs/2401.09352v1"
  },
  {
    "title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene\n  Understanding",
    "authors": "Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, Siyuan Huang",
    "abstract": "  3D vision-language grounding, which focuses on aligning language with the 3D\nphysical environment, stands as a cornerstone in the development of embodied\nagents. In comparison to recent advancements in the 2D domain, grounding\nlanguage in 3D scenes faces several significant challenges: (i) the inherent\ncomplexity of 3D scenes due to the diverse object configurations, their rich\nattributes, and intricate relationships; (ii) the scarcity of paired 3D\nvision-language data to support grounded learning; and (iii) the absence of a\nunified learning framework to distill knowledge from grounded 3D data. In this\nwork, we aim to address these three major challenges in 3D vision-language by\nexamining the potential of systematically upscaling 3D vision-language learning\nin indoor environments. We introduce the first million-scale 3D vision-language\ndataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising\n2.5M vision-language pairs derived from both human annotations and our scalable\nscene-graph-based generation approach. We demonstrate that this scaling allows\nfor a unified pre-training framework, Grounded Pre-training for Scenes (GPS),\nfor 3D vision-language learning. Through extensive experiments, we showcase the\neffectiveness of GPS by achieving state-of-the-art performance on all existing\n3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is\nunveiled through zero-shot transfer experiments in the challenging 3D\nvision-language tasks. Project website: https://scene-verse.github.io .\n",
    "link": "http://arxiv.org/abs/2401.09340v1"
  },
  {
    "title": "Large Language Models Are Neurosymbolic Reasoners",
    "authors": "Meng Fang, Shilong Deng, Yudi Zhang, Zijing Shi, Ling Chen, Mykola Pechenizkiy, Jun Wang",
    "abstract": "  A wide range of real-world applications is characterized by their symbolic\nnature, necessitating a strong capability for symbolic reasoning. This paper\ninvestigates the potential application of Large Language Models (LLMs) as\nsymbolic reasoners. We focus on text-based games, significant benchmarks for\nagents with natural language capabilities, particularly in symbolic tasks like\nmath, map reading, sorting, and applying common sense in text-based worlds. To\nfacilitate these agents, we propose an LLM agent designed to tackle symbolic\nchallenges and achieve in-game objectives. We begin by initializing the LLM\nagent and informing it of its role. The agent then receives observations and a\nset of valid actions from the text-based games, along with a specific symbolic\nmodule. With these inputs, the LLM agent chooses an action and interacts with\nthe game environments. Our experimental results demonstrate that our method\nsignificantly enhances the capability of LLMs as automated agents for symbolic\nreasoning, and our LLM agent is effective in text-based games involving\nsymbolic tasks, achieving an average performance of 88% across all tasks.\n",
    "link": "http://arxiv.org/abs/2401.09334v1"
  },
  {
    "title": "FIT-SLAM -- Fisher Information and Traversability estimation-based\n  Active SLAM for exploration in 3D environments",
    "authors": "Suchetan Saravanan, Corentin Chauffaut, Caroline Chanel, Damien Vivet",
    "abstract": "  Active visual SLAM finds a wide array of applications in GNSS-Denied\nsub-terrain environments and outdoor environments for ground robots. To achieve\nrobust localization and mapping accuracy, it is imperative to incorporate the\nperception considerations in the goal selection and path planning towards the\ngoal during an exploration mission. Through this work, we propose FIT-SLAM\n(Fisher Information and Traversability estimation-based Active SLAM), a new\nexploration method tailored for unmanned ground vehicles (UGVs) to explore 3D\nenvironments. This approach is devised with the dual objectives of sustaining\nan efficient exploration rate while optimizing SLAM accuracy. Initially, an\nestimation of a global traversability map is conducted, which accounts for the\nenvironmental constraints pertaining to traversability. Subsequently, we\npropose a goal candidate selection approach along with a path planning method\ntowards this goal that takes into account the information provided by the\nlandmarks used by the SLAM backend to achieve robust localization and\nsuccessful path execution . The entire algorithm is tested and evaluated first\nin a simulated 3D world, followed by a real-world environment and is compared\nto pre-existing exploration methods. The results obtained during this\nevaluation demonstrate a significant increase in the exploration rate while\neffectively minimizing the localization covariance.\n",
    "link": "http://arxiv.org/abs/2401.09322v1"
  },
  {
    "title": "T-FOLEY: A Controllable Waveform-Domain Diffusion Model for\n  Temporal-Event-Guided Foley Sound Synthesis",
    "authors": "Yoonjin Chung, Junwon Lee, Juhan Nam",
    "abstract": "  Foley sound, audio content inserted synchronously with videos, plays a\ncritical role in the user experience of multimedia content. Recently, there has\nbeen active research in Foley sound synthesis, leveraging the advancements in\ndeep generative models. However, such works mainly focus on replicating a\nsingle sound class or a textual sound description, neglecting temporal\ninformation, which is crucial in the practical applications of Foley sound. We\npresent T-Foley, a Temporal-event-guided waveform generation model for Foley\nsound synthesis. T-Foley generates high-quality audio using two conditions: the\nsound class and temporal event feature. For temporal conditioning, we devise a\ntemporal event feature and a novel conditioning technique named Block-FiLM.\nT-Foley achieves superior performance in both objective and subjective\nevaluation metrics and generates Foley sound well-synchronized with the\ntemporal events. Additionally, we showcase T-Foley's practical applications,\nparticularly in scenarios involving vocal mimicry for temporal event control.\nWe show the demo on our companion website.\n",
    "link": "http://arxiv.org/abs/2401.09294v1"
  },
  {
    "title": "Deployable Reinforcement Learning with Variable Control Rate",
    "authors": "Dong Wang, Giovanni Beltrame",
    "abstract": "  Deploying controllers trained with Reinforcement Learning (RL) on real robots\ncan be challenging: RL relies on agents' policies being modeled as Markov\nDecision Processes (MDPs), which assume an inherently discrete passage of time.\nThe use of MDPs results in that nearly all RL-based control systems employ a\nfixed-rate control strategy with a period (or time step) typically chosen based\non the developer's experience or specific characteristics of the application\nenvironment. Unfortunately, the system should be controlled at the highest,\nworst-case frequency to ensure stability, which can demand significant\ncomputational and energy resources and hinder the deployability of the\ncontroller on onboard hardware. Adhering to the principles of reactive\nprogramming, we surmise that applying control actions only when necessary\nenables the use of simpler hardware and helps reduce energy consumption. We\nchallenge the fixed frequency assumption by proposing a variant of RL with\nvariable control rate. In this approach, the policy decides the action the\nagent should take as well as the duration of the time step associated with that\naction. In our new setting, we expand Soft Actor-Critic (SAC) to compute the\noptimal policy with a variable control rate, introducing the Soft Elastic\nActor-Critic (SEAC) algorithm. We show the efficacy of SEAC through a\nproof-of-concept simulation driving an agent with Newtonian kinematics. Our\nexperiments show higher average returns, shorter task completion times, and\nreduced computational resources when compared to fixed rate policies.\n",
    "link": "http://arxiv.org/abs/2401.09286v1"
  },
  {
    "title": "3D Scene Geometry Estimation from 360$^\\circ$ Imagery: A Survey",
    "authors": "Thiago Lopes Trugillo da Silveira, Paulo Gamarra Lessa Pinto, Jeffri Erwin Murrugarra Llerena, Claudio Rosito Jung",
    "abstract": "  This paper provides a comprehensive survey on pioneer and state-of-the-art 3D\nscene geometry estimation methodologies based on single, two, or multiple\nimages captured under the omnidirectional optics. We first revisit the basic\nconcepts of the spherical camera model, and review the most common acquisition\ntechnologies and representation formats suitable for omnidirectional (also\ncalled 360$^\\circ$, spherical or panoramic) images and videos. We then survey\nmonocular layout and depth inference approaches, highlighting the recent\nadvances in learning-based solutions suited for spherical data. The classical\nstereo matching is then revised on the spherical domain, where methodologies\nfor detecting and describing sparse and dense features become crucial. The\nstereo matching concepts are then extrapolated for multiple view camera setups,\ncategorizing them among light fields, multi-view stereo, and structure from\nmotion (or visual simultaneous localization and mapping). We also compile and\ndiscuss commonly adopted datasets and figures of merit indicated for each\npurpose and list recent results for completeness. We conclude this paper by\npointing out current and future trends.\n",
    "link": "http://arxiv.org/abs/2401.09252v1"
  },
  {
    "title": "DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven\n  Policy Learning",
    "authors": "Sabariswaran Mani, Abhranil Chandra, Sreyas Venkataraman, Adyan Rizvi, Yash Sirvi, Soumojit Bhattacharya, Aritra Hazra",
    "abstract": "  Robot learning tasks are extremely compute-intensive and hardware-specific.\nThus the avenues of tackling these challenges, using a diverse dataset of\noffline demonstrations that can be used to train robot manipulation agents, is\nvery appealing. The Train-Offline-Test-Online (TOTO) Benchmark provides a\nwell-curated open-source dataset for offline training comprised mostly of\nexpert data and also benchmark scores of the common offline-RL and behaviour\ncloning agents. In this paper, we introduce DiffClone, an offline algorithm of\nenhanced behaviour cloning agent with diffusion-based policy learning, and\nmeasured the efficacy of our method on real online physical robots at test\ntime. This is also our official submission to the Train-Offline-Test-Online\n(TOTO) Benchmark Challenge organized at NeurIPS 2023. We experimented with both\npre-trained visual representation and agent policies. In our experiments, we\nfind that MOCO finetuned ResNet50 performs the best in comparison to other\nfinetuned representations. Goal state conditioning and mapping to transitions\nresulted in a minute increase in the success rate and mean-reward. As for the\nagent policy, we developed DiffClone, a behaviour cloning agent improved using\nconditional diffusion.\n",
    "link": "http://arxiv.org/abs/2401.09243v1"
  },
  {
    "title": "A Blockchain-based Model for Securing Data Pipeline in a Heterogeneous\n  Information System",
    "authors": "MN Ramahlosi, Y Madani, A Akanbi",
    "abstract": "  In our digital world, access to personal and public data has become an item\nof concern, with challenging security and privacy aspects. Modern information\nsystems are heterogeneous in nature and have an inherent security\nvulnerability, which is susceptible to data interception and data modification\ndue to unsecured communication data pipelines between connected endpoints. This\nre-search article presents a blockchain-based model for securing data pipelines\nin a heterogeneous information system using an integrated multi-hazard early\nwarning system (MHEWS) as a case study. The proposed model utilizes the\ninherent security features of blockchain technology to address the security and\nprivacy concerns that arise in data pipelines. The model is designed to ensure\ndata integrity, confidentiality, and authenticity in a decentralized manner.\nThe model is evaluated in a hybrid environment using a prototype implementation\nand simulation experiments with outcomes that demonstrate advantages over\ntraditional approaches for a tamper-proof and immutable data pipeline for data\nauthenticity and integrity using a confidential ledger.\n",
    "link": "http://arxiv.org/abs/2401.09240v1"
  },
  {
    "title": "DaFoEs: Mixing Datasets towards the generalization of vision-state\n  deep-learning Force Estimation in Minimally Invasive Robotic Surgery",
    "authors": "Mikel De Iturrate Reyzabal, Mingcong Chen, Wei Huang, Sebastien Ourselin, Hongbin Liu",
    "abstract": "  Precisely determining the contact force during safe interaction in Minimally\nInvasive Robotic Surgery (MIRS) is still an open research challenge. Inspired\nby post-operative qualitative analysis from surgical videos, the use of\ncross-modality data driven deep neural network models has been one of the\nnewest approaches to predict sensorless force trends. However, these methods\nrequired for large and variable datasets which are not currently available. In\nthis paper, we present a new vision-haptic dataset (DaFoEs) with variable soft\nenvironments for the training of deep neural models. In order to reduce the\nbias from a single dataset, we present a pipeline to generalize different\nvision and state data inputs for mixed dataset training, using a previously\nvalidated dataset with different setup. Finally, we present a variable\nencoder-decoder architecture to predict the forces done by the laparoscopic\ntool using single input or sequence of inputs. For input sequence, we use a\nrecurrent decoder, named with the prefix R, and a new temporal sampling to\nrepresent the acceleration of the tool. During our training, we demonstrate\nthat single dataset training tends to overfit to the training data domain, but\nhas difficulties on translating the results across new domains. However,\ndataset mixing presents a good translation with a mean relative estimated force\nerror of 5% and 12% for the recurrent and non-recurrent models respectively.\nOur method, also marginally increase the effectiveness of transformers for\nforce estimation up to a maximum of ~15%, as the volume of available data is\nincrease by 150%. In conclusion, we demonstrate that mixing experimental set\nups for vision-state force estimation in MIRS is a possible approach towards\nthe general solution of the problem.\n",
    "link": "http://arxiv.org/abs/2401.09239v1"
  },
  {
    "title": "A Characterization Theorem for Equivariant Networks with Point-wise\n  Activations",
    "authors": "Marco Pacini, Xiaowen Dong, Bruno Lepri, Gabriele Santin",
    "abstract": "  Equivariant neural networks have shown improved performance, expressiveness\nand sample complexity on symmetrical domains. But for some specific symmetries,\nrepresentations, and choice of coordinates, the most common point-wise\nactivations, such as ReLU, are not equivariant, hence they cannot be employed\nin the design of equivariant neural networks. The theorem we present in this\npaper describes all possible combinations of finite-dimensional\nrepresentations, choice of coordinates and point-wise activations to obtain an\nexactly equivariant layer, generalizing and strengthening existing\ncharacterizations. Notable cases of practical relevance are discussed as\ncorollaries. Indeed, we prove that rotation-equivariant networks can only be\ninvariant, as it happens for any network which is equivariant with respect to\nconnected compact groups. Then, we discuss implications of our findings when\napplied to important instances of exactly equivariant networks. First, we\ncompletely characterize permutation equivariant networks such as Invariant\nGraph Networks with point-wise nonlinearities and their geometric counterparts,\nhighlighting a plethora of models whose expressive power and performance are\nstill unknown. Second, we show that feature spaces of disentangled steerable\nconvolutional neural networks are trivial representations.\n",
    "link": "http://arxiv.org/abs/2401.09235v1"
  },
  {
    "title": "Preparing Lessons for Progressive Training on Language Models",
    "authors": "Yu Pan, Ye Yuan, Yichun Yin, Jiaxin Shi, Zenglin Xu, Ming Zhang, Lifeng Shang, Xin Jiang, Qun Liu",
    "abstract": "  The rapid progress of Transformers in artificial intelligence has come at the\ncost of increased resource consumption and greenhouse gas emissions due to\ngrowing model sizes. Prior work suggests using pretrained small models to\nimprove training efficiency, but this approach may not be suitable for new\nmodel structures. On the other hand, training from scratch can be slow, and\nprogressively stacking layers often fails to achieve significant acceleration.\nTo address these challenges, we propose a novel method called Apollo, which\nprep\\textbf{a}res lessons for ex\\textbf{p}anding \\textbf{o}perations by\n\\textbf{l}earning high-\\textbf{l}ayer functi\\textbf{o}nality during training of\nlow layers. Our approach involves low-value-prioritized sampling (LVPS) to\ntrain different depths and weight sharing to facilitate efficient expansion. We\nalso introduce an interpolation method for stable model depth extension.\nExperiments demonstrate that Apollo achieves state-of-the-art acceleration\nratios, even rivaling methods using pretrained models, making it a universal\nand efficient solution for training deep models while reducing time, financial,\nand environmental costs.\n",
    "link": "http://arxiv.org/abs/2401.09192v1"
  },
  {
    "title": "What makes for a 'good' social actor? Using respect as a lens to\n  evaluate interactions with language agents",
    "authors": "Lize Alberts, Geoff Keeling, Amanda McCroskery",
    "abstract": "  With the growing popularity of dialogue agents based on large language models\n(LLMs), urgent attention has been drawn to finding ways to ensure their\nbehaviour is ethical and appropriate. These are largely interpreted in terms of\nthe 'HHH' criteria: making outputs more helpful and honest, and avoiding\nharmful (biased, toxic, or inaccurate) statements. Whilst this semantic focus\nis useful from the perspective of viewing LLM agents as mere mediums for\ninformation, it fails to account for pragmatic factors that can make the same\nutterance seem more or less offensive or tactless in different social\nsituations. We propose an approach to ethics that is more centred on relational\nand situational factors, exploring what it means for a system, as a social\nactor, to treat an individual respectfully in a (series of) interaction(s). Our\nwork anticipates a set of largely unexplored risks at the level of situated\ninteraction, and offers practical suggestions to help LLM technologies behave\nas 'good' social actors and treat people respectfully.\n",
    "link": "http://arxiv.org/abs/2401.09082v1"
  },
  {
    "title": "GPT in Sheep's Clothing: The Risk of Customized GPTs",
    "authors": "Sagiv Antebi, Noam Azulay, Edan Habler, Ben Ganon, Asaf Shabtai, Yuval Elovici",
    "abstract": "  In November 2023, OpenAI introduced a new service allowing users to create\ncustom versions of ChatGPT (GPTs) by using specific instructions and knowledge\nto guide the model's behavior. We aim to raise awareness of the fact that GPTs\ncan be used maliciously, posing privacy and security risks to their users.\n",
    "link": "http://arxiv.org/abs/2401.09075v1"
  },
  {
    "title": "Code Simulation Challenges for Large Language Models",
    "authors": "Emanuele La Malfa, Christoph Weinhuber, Orazio Torre, Fangru Lin, Anthony Cohn, Nigel Shadbolt, Michael Wooldridge",
    "abstract": "  We investigate the extent to which Large Language Models (LLMs) can simulate\nthe execution of computer code and algorithms. We begin by looking straight\nline programs, and show that current LLMs demonstrate poor performance even\nwith such simple programs -- performance rapidly degrades with the length of\ncode. We then investigate the ability of LLMs to simulate programs that contain\ncritical paths and redundant instructions. We also go beyond straight line\nprogram simulation with sorting algorithms and nested loops, and we show the\ncomputational complexity of a routine directly affects the ability of an LLM to\nsimulate its execution. We observe that LLMs execute instructions sequentially\nand with a low error margin only for short programs or standard procedures.\nLLMs' code simulation is in tension with their pattern recognition and\nmemorisation capabilities: on tasks where memorisation is detrimental, we\npropose a novel prompting method to simulate code execution line by line.\nEmpirically, our new Chain of Simulation (CoSm) method improves on the standard\nChain of Thought prompting approach by avoiding the pitfalls of memorisation.\n",
    "link": "http://arxiv.org/abs/2401.09074v1"
  },
  {
    "title": "Fixed-Budget Differentially Private Best Arm Identification",
    "authors": "Zhirui Chen, P. N. Karthik, Yeow Meng Chee, Vincent Y. F. Tan",
    "abstract": "  We study best arm identification (BAI) in linear bandits in the fixed-budget\nregime under differential privacy constraints, when the arm rewards are\nsupported on the unit interval. Given a finite budget $T$ and a privacy\nparameter $\\varepsilon&gt;0$, the goal is to minimise the error probability in\nfinding the arm with the largest mean after $T$ sampling rounds, subject to the\nconstraint that the policy of the decision maker satisfies a certain {\\em\n$\\varepsilon$-differential privacy} ($\\varepsilon$-DP) constraint. We construct\na policy satisfying the $\\varepsilon$-DP constraint (called {\\sc DP-BAI}) by\nproposing the principle of {\\em maximum absolute determinants}, and derive an\nupper bound on its error probability. Furthermore, we derive a minimax lower\nbound on the error probability, and demonstrate that the lower and the upper\nbounds decay exponentially in $T$, with exponents in the two bounds matching\norder-wise in (a) the sub-optimality gaps of the arms, (b) $\\varepsilon$, and\n(c) the problem complexity that is expressible as the sum of two terms, one\ncharacterising the complexity of standard fixed-budget BAI (without privacy\nconstraints), and the other accounting for the $\\varepsilon$-DP constraint.\nAdditionally, we present some auxiliary results that contribute to the\nderivation of the lower bound on the error probability. These results, we\nposit, may be of independent interest and could prove instrumental in proving\nlower bounds on error probabilities in several other bandit problems. Whereas\nprior works provide results for BAI in the fixed-budget regime without privacy\nconstraints or in the fixed-confidence regime with privacy constraints, our\nwork fills the gap in the literature by providing the results for BAI in the\nfixed-budget regime under the $\\varepsilon$-DP constraint.\n",
    "link": "http://arxiv.org/abs/2401.09073v1"
  },
  {
    "title": "Rethinking Spectral Graph Neural Networks with Spatially Adaptive\n  Filtering",
    "authors": "Jingwei Guo, Kaizhu Huang, Xinping Yi, Zixian Su, Rui Zhang",
    "abstract": "  Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded\nin the spectral domain, their practical reliance on polynomial approximation\nimplies a profound linkage to the spatial domain. As previous studies rarely\nexamine spectral GNNs from the spatial perspective, their spatial-domain\ninterpretability remains elusive, e.g., what information is essentially encoded\nby spectral GNNs in the spatial domain? In this paper, to answer this question,\nwe establish a theoretical connection between spectral filtering and spatial\naggregation, unveiling an intrinsic interaction that spectral filtering\nimplicitly leads the original graph to an adapted new graph, explicitly\ncomputed for spatial aggregation. Both theoretical and empirical investigations\nreveal that the adapted new graph not only exhibits non-locality but also\naccommodates signed edge weights to reflect label consistency between nodes.\nThese findings thus highlight the interpretable role of spectral GNNs in the\nspatial domain and inspire us to rethink graph spectral filters beyond the\nfixed-order polynomials, which neglect global information. Built upon the\ntheoretical findings, we revisit the state-of-the-art spectral GNNs and propose\na novel Spatially Adaptive Filtering (SAF) framework, which leverages the\nadapted new graph by spectral filtering for an auxiliary non-local aggregation.\nNotably, our proposed SAF comprehensively models both node similarity and\ndissimilarity from a global perspective, therefore alleviating persistent\ndeficiencies of GNNs related to long-range dependencies and graph heterophily.\nExtensive experiments over 13 node classification benchmarks demonstrate the\nsuperiority of our proposed framework to the state-of-the-art models.\n",
    "link": "http://arxiv.org/abs/2401.09071v1"
  },
  {
    "title": "Knowledge Pyramid: A Novel Hierarchical Reasoning Structure for\n  Generalized Knowledge Augmentation and Inference",
    "authors": "Qinghua Huang, Yongzhen Wang",
    "abstract": "  Knowledge graph (KG) based reasoning has been regarded as an effective means\nfor the analysis of semantic networks and is of great usefulness in areas of\ninformation retrieval, recommendation, decision-making, and man-machine\ninteraction. It is widely used in recommendation, decision-making,\nquestion-answering, search, and other fields. However, previous studies mainly\nused low-level knowledge in the KG for reasoning, which may result in\ninsufficient generalization and poor robustness of reasoning. To this end, this\npaper proposes a new inference approach using a novel knowledge augmentation\nstrategy to improve the generalization capability of KG. This framework\nextracts high-level pyramidal knowledge from low-level knowledge and applies it\nto reasoning in a multi-level hierarchical KG, called knowledge pyramid in this\npaper. We tested some medical data sets using the proposed approach, and the\nexperimental results show that the proposed knowledge pyramid has improved the\nknowledge inference performance with better generalization. Especially, when\nthere are fewer training samples, the inference accuracy can be significantly\nimproved.\n",
    "link": "http://arxiv.org/abs/2401.09070v1"
  },
  {
    "title": "DTMM: Deploying TinyML Models on Extremely Weak IoT Devices with Pruning",
    "authors": "Lixiang Han, Zhen Xiao, Zhenjiang Li",
    "abstract": "  DTMM is a library designed for efficient deployment and execution of machine\nlearning models on weak IoT devices such as microcontroller units (MCUs). The\nmotivation for designing DTMM comes from the emerging field of tiny machine\nlearning (TinyML), which explores extending the reach of machine learning to\nmany low-end IoT devices to achieve ubiquitous intelligence. Due to the weak\ncapability of embedded devices, it is necessary to compress models by pruning\nenough weights before deploying. Although pruning has been studied extensively\non many computing platforms, two key issues with pruning methods are\nexacerbated on MCUs: models need to be deeply compressed without significantly\ncompromising accuracy, and they should perform efficiently after pruning.\nCurrent solutions only achieve one of these objectives, but not both. In this\npaper, we find that pruned models have great potential for efficient deployment\nand execution on MCUs. Therefore, we propose DTMM with pruning unit selection,\npre-execution pruning optimizations, runtime acceleration, and post-execution\nlow-cost storage to fill the gap for efficient deployment and execution of\npruned models. It can be integrated into commercial ML frameworks for practical\ndeployment, and a prototype system has been developed. Extensive experiments on\nvarious models show promising gains compared to state-of-the-art methods.\n",
    "link": "http://arxiv.org/abs/2401.09068v1"
  },
  {
    "title": "Towards Continual Learning Desiderata via HSIC-Bottleneck\n  Orthogonalization and Equiangular Embedding",
    "authors": "Depeng Li, Tianqi Wang, Junwei Chen, Qining Ren, Kenji Kawaguchi, Zhigang Zeng",
    "abstract": "  Deep neural networks are susceptible to catastrophic forgetting when trained\non sequential tasks. Various continual learning (CL) methods often rely on\nexemplar buffers or/and network expansion for balancing model stability and\nplasticity, which, however, compromises their practical value due to privacy\nand memory concerns. Instead, this paper considers a strict yet realistic\nsetting, where the training data from previous tasks is unavailable and the\nmodel size remains relatively constant during sequential training. To achieve\nsuch desiderata, we propose a conceptually simple yet effective method that\nattributes forgetting to layer-wise parameter overwriting and the resulting\ndecision boundary distortion. This is achieved by the synergy between two key\ncomponents: HSIC-Bottleneck Orthogonalization (HBO) implements non-overwritten\nparameter updates mediated by Hilbert-Schmidt independence criterion in an\northogonal space and EquiAngular Embedding (EAE) enhances decision boundary\nadaptation between old and new tasks with predefined basis vectors. Extensive\nexperiments demonstrate that our method achieves competitive accuracy\nperformance, even with absolute superiority of zero exemplar buffer and 1.02x\nthe base model.\n",
    "link": "http://arxiv.org/abs/2401.09067v1"
  },
  {
    "title": "LLMs for Relational Reasoning: How Far are We?",
    "authors": "Zhiming Li, Yushi Cao, Xiufeng Xu, Junzhe Jiang, Xu Liu, Yon Shin Teo, Shang-wei Lin, Yang Liu",
    "abstract": "  Large language models (LLMs) have revolutionized many areas (e.g. natural\nlanguage processing, software engineering, etc.) by achieving state-of-the-art\nperformance on extensive downstream tasks. Aiming to achieve robust and general\nartificial intelligence, there has been a surge of interest in investigating\nthe reasoning ability of the LLMs. Whereas the textual and numerical reasoning\nbenchmarks adopted by previous works are rather shallow and simple, it is hard\nto conclude that the LLMs possess strong reasoning ability by merely achieving\npositive results on these benchmarks. Recent efforts have demonstrated that the\nLLMs are poor at solving sequential decision-making problems that require\ncommon-sense planning by evaluating their performance on the reinforcement\nlearning benchmarks. In this work, we conduct an in-depth assessment of several\nstate-of-the-art LLMs' reasoning ability based on the inductive logic\nprogramming (ILP) benchmark, which is broadly recognized as a representative\nand challenging measurement for evaluating logic program induction/synthesis\nsystems as it requires inducing strict cause-effect logic to achieve robust\ndeduction on independent and identically distributed (IID) and\nout-of-distribution (OOD) test samples. Our evaluations illustrate that\ncompared with the neural program induction systems which are much smaller in\nmodel size, the state-of-the-art LLMs are much poorer in terms of reasoning\nability by achieving much lower performance and generalization using either\nnatural language prompting or truth-value matrix prompting.\n",
    "link": "http://arxiv.org/abs/2401.09042v1"
  },
  {
    "title": "UOEP: User-Oriented Exploration Policy for Enhancing Long-Term User\n  Experiences in Recommender Systems",
    "authors": "Changshuo Zhang, Sirui Chen, Xiao Zhang, Sunhao Dai, Weijie Yu, Jun Xu",
    "abstract": "  Reinforcement learning (RL) has gained traction for enhancing user long-term\nexperiences in recommender systems by effectively exploring users' interests.\nHowever, modern recommender systems exhibit distinct user behavioral patterns\namong tens of millions of items, which increases the difficulty of exploration.\nFor example, user behaviors with different activity levels require varying\nintensity of exploration, while previous studies often overlook this aspect and\napply a uniform exploration strategy to all users, which ultimately hurts user\nexperiences in the long run. To address these challenges, we propose\nUser-Oriented Exploration Policy (UOEP), a novel approach facilitating\nfine-grained exploration among user groups. We first construct a distributional\ncritic which allows policy optimization under varying quantile levels of\ncumulative reward feedbacks from users, representing user groups with varying\nactivity levels. Guided by this critic, we devise a population of distinct\nactors aimed at effective and fine-grained exploration within its respective\nuser group. To simultaneously enhance diversity and stability during the\nexploration process, we further introduce a population-level diversity\nregularization term and a supervision module. Experimental results on public\nrecommendation datasets demonstrate that our approach outperforms all other\nbaselines in terms of long-term performance, validating its user-oriented\nexploration effectiveness. Meanwhile, further analyses reveal our approach's\nbenefits of improved performance for low-activity users as well as increased\nfairness among users.\n",
    "link": "http://arxiv.org/abs/2401.09034v1"
  },
  {
    "title": "Cross-modality Guidance-aided Multi-modal Learning with Dual Attention\n  for MRI Brain Tumor Grading",
    "authors": "Dunyuan Xu, Xi Wang, Jinyue Cai, Pheng-Ann Heng",
    "abstract": "  Brain tumor represents one of the most fatal cancers around the world, and is\nvery common in children and the elderly. Accurate identification of the type\nand grade of tumor in the early stages plays an important role in choosing a\nprecise treatment plan. The Magnetic Resonance Imaging (MRI) protocols of\ndifferent sequences provide clinicians with important contradictory information\nto identify tumor regions. However, manual assessment is time-consuming and\nerror-prone due to big amount of data and the diversity of brain tumor types.\nHence, there is an unmet need for MRI automated brain tumor diagnosis. We\nobserve that the predictive capability of uni-modality models is limited and\ntheir performance varies widely across modalities, and the commonly used\nmodality fusion methods would introduce potential noise, which results in\nsignificant performance degradation. To overcome these challenges, we propose a\nnovel cross-modality guidance-aided multi-modal learning with dual attention\nfor addressing the task of MRI brain tumor grading. To balance the tradeoff\nbetween model efficiency and efficacy, we employ ResNet Mix Convolution as the\nbackbone network for feature extraction. Besides, dual attention is applied to\ncapture the semantic interdependencies in spatial and slice dimensions\nrespectively. To facilitate information interaction among modalities, we design\na cross-modality guidance-aided module where the primary modality guides the\nother secondary modalities during the process of training, which can\neffectively leverage the complementary information of different MRI modalities\nand meanwhile alleviate the impact of the possible noise.\n",
    "link": "http://arxiv.org/abs/2401.09029v1"
  },
  {
    "title": "Change Detection Between Optical Remote Sensing Imagery and Map Data via\n  Segment Anything Model (SAM)",
    "authors": "Hongruixuan Chen, Jian Song, Naoto Yokoya",
    "abstract": "  Unsupervised multimodal change detection is pivotal for time-sensitive tasks\nand comprehensive multi-temporal Earth monitoring. In this study, we explore\nunsupervised multimodal change detection between two key remote sensing data\nsources: optical high-resolution imagery and OpenStreetMap (OSM) data.\nSpecifically, we propose to utilize the vision foundation model Segmentation\nAnything Model (SAM), for addressing our task. Leveraging SAM's exceptional\nzero-shot transfer capability, high-quality segmentation maps of optical images\ncan be obtained. Thus, we can directly compare these two heterogeneous data\nforms in the so-called segmentation domain. We then introduce two strategies\nfor guiding SAM's segmentation process: the 'no-prompt' and 'box/mask prompt'\nmethods. The two strategies are designed to detect land-cover changes in\ngeneral scenarios and to identify new land-cover objects within existing\nbackgrounds, respectively. Experimental results on three datasets indicate that\nthe proposed approach can achieve more competitive results compared to\nrepresentative unsupervised multimodal change detection methods.\n",
    "link": "http://arxiv.org/abs/2401.09019v1"
  },
  {
    "title": "Inductive Models for Artificial Intelligence Systems are Insufficient\n  without Good Explanations",
    "authors": "Udesh Habaraduwa",
    "abstract": "  This paper discusses the limitations of machine learning (ML), particularly\ndeep artificial neural networks (ANNs), which are effective at approximating\ncomplex functions but often lack transparency and explanatory power. It\nhighlights the `problem of induction' : the philosophical issue that past\nobservations may not necessarily predict future events, a challenge that ML\nmodels face when encountering new, unseen data. The paper argues for the\nimportance of not just making predictions but also providing good explanations,\na feature that current models often fail to deliver. It suggests that for AI to\nprogress, we must seek models that offer insights and explanations, not just\npredictions.\n",
    "link": "http://arxiv.org/abs/2401.09011v1"
  },
  {
    "title": "Hybrid of DiffStride and Spectral Pooling in Convolutional Neural\n  Networks",
    "authors": "Sulthan Rafif, Mochamad Arfan Ravy Wahyu Pratama, Mohammad Faris Azhar, Ahmad Mustafidul Ibad, Lailil Muflikhah, Novanto Yudistira",
    "abstract": "  Stride determines the distance between adjacent filter positions as the\nfilter moves across the input. A fixed stride causes important information\ncontained in the image can not be captured, so that important information is\nnot classified. Therefore, in previous research, the DiffStride Method was\napplied, namely the Strided Convolution Method with which it can learn its own\nstride value. Severe Quantization and a constraining lower bound on preserved\ninformation are arises with Max Pooling Downsampling Method. Spectral Pooling\nreduce the constraint lower bound on preserved information by cutting off the\nrepresentation in the frequency domain. In this research a CNN Model is\nproposed with the Downsampling Learnable Stride Technique performed by\nBackpropagation combined with the Spectral Pooling Technique. Diffstride and\nSpectral Pooling techniques are expected to maintain most of the information\ncontained in the image. In this study, we compare the Hybrid Method, which is a\ncombined implementation of Spectral Pooling and DiffStride against the Baseline\nMethod, which is the DiffStride implementation on ResNet 18. The accuracy\nresult of the DiffStride combination with Spectral Pooling improves over\nDiffStride which is baseline method by 0.0094. This shows that the Hybrid\nMethod can maintain most of the information by cutting of the representation in\nthe frequency domain and determine the stride of the learning result through\nBackpropagation.\n",
    "link": "http://arxiv.org/abs/2401.09008v1"
  },
  {
    "title": "Augmenting Math Word Problems via Iterative Question Composing",
    "authors": "Haoxiong Liu, Andrew Chi-Chih Yao",
    "abstract": "  Despite recent progress in improving the mathematical reasoning ability of\nlarge language models(LLMs), solving competition-level math problems without\nthe use of external tools remains challenging for open-source LLMs. In this\nwork, we introduce the MMIQC dataset, a mixture of processed web data and\nsynthetic question-response pairs, to equip base models with better\nmathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by\nfine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\\% accuracy on\nMATH(arXiv:2103.03874), 5.8\\% higher than the previous (model size $\\sim$7B)\nSOTA. Our experiments also show that a large part of the improvement attributes\nto our novel augmentation method IQC(Iterative Question Composing), where we\niteratively ask an LLM to compose new questions from the given seed problems\nand do rejection sampling from another LLM. MMIQC has now been released on\nhttps://huggingface.co/datasets/Vivacem/MMIQC.\n",
    "link": "http://arxiv.org/abs/2401.09003v1"
  },
  {
    "title": "Continuous Time Continuous Space Homeostatic Reinforcement Learning\n  (CTCS-HRRL) : Towards Biological Self-Autonomous Agent",
    "authors": "Hugo Laurencon, Yesoda Bhargava, Riddhi Zantye, Charbel-Rapha\u00ebl S\u00e9gerie, Johann Lussange, Veeky Baths, Boris Gutkin",
    "abstract": "  Homeostasis is a biological process by which living beings maintain their\ninternal balance. Previous research suggests that homeostasis is a learned\nbehaviour. Recently introduced Homeostatic Regulated Reinforcement Learning\n(HRRL) framework attempts to explain this learned homeostatic behavior by\nlinking Drive Reduction Theory and Reinforcement Learning. This linkage has\nbeen proven in the discrete time-space, but not in the continuous time-space.\nIn this work, we advance the HRRL framework to a continuous time-space\nenvironment and validate the CTCS-HRRL (Continuous Time Continuous Space HRRL)\nframework. We achieve this by designing a model that mimics the homeostatic\nmechanisms in a real-world biological agent. This model uses the\nHamilton-Jacobian Bellman Equation, and function approximation based on neural\nnetworks and Reinforcement Learning. Through a simulation-based experiment we\ndemonstrate the efficacy of this model and uncover the evidence linked to the\nagent's ability to dynamically choose policies that favor homeostasis in a\ncontinuously changing internal-state milieu. Results of our experiments\ndemonstrate that agent learns homeostatic behaviour in a CTCS environment,\nmaking CTCS-HRRL a promising framework for modellng animal dynamics and\ndecision-making.\n",
    "link": "http://arxiv.org/abs/2401.08999v1"
  },
  {
    "title": "MicroNAS: Zero-Shot Neural Architecture Search for MCUs",
    "authors": "Ye Qiao, Haocheng Xu, Yifan Zhang, Sitao Huang",
    "abstract": "  Neural Architecture Search (NAS) effectively discovers new Convolutional\nNeural Network (CNN) architectures, particularly for accuracy optimization.\nHowever, prior approaches often require resource-intensive training on super\nnetworks or extensive architecture evaluations, limiting practical\napplications. To address these challenges, we propose MicroNAS, a\nhardware-aware zero-shot NAS framework designed for microcontroller units\n(MCUs) in edge computing. MicroNAS considers target hardware optimality during\nthe search, utilizing specialized performance indicators to identify optimal\nneural architectures without high computational costs. Compared to previous\nworks, MicroNAS achieves up to 1104x improvement in search efficiency and\ndiscovers models with over 3.23x faster MCU inference while maintaining similar\naccuracy\n",
    "link": "http://arxiv.org/abs/2401.08996v1"
  },
  {
    "title": "A GAN-based data poisoning framework against anomaly detection in\n  vertical federated learning",
    "authors": "Xiaolin Chen, Daoguang Zan, Wei Li, Bei Guan, Yongji Wang",
    "abstract": "  In vertical federated learning (VFL), commercial entities collaboratively\ntrain a model while preserving data privacy. However, a malicious participant's\npoisoning attack may degrade the performance of this collaborative model. The\nmain challenge in achieving the poisoning attack is the absence of access to\nthe server-side top model, leaving the malicious participant without a clear\ntarget model. To address this challenge, we introduce an innovative end-to-end\npoisoning framework P-GAN. Specifically, the malicious participant initially\nemploys semi-supervised learning to train a surrogate target model.\nSubsequently, this participant employs a GAN-based method to produce\nadversarial perturbations to degrade the surrogate target model's performance.\nFinally, the generator is obtained and tailored for VFL poisoning. Besides, we\ndevelop an anomaly detection algorithm based on a deep auto-encoder (DAE),\noffering a robust defense mechanism to VFL scenarios. Through extensive\nexperiments, we evaluate the efficacy of P-GAN and DAE, and further analyze the\nfactors that influence their performance.\n",
    "link": "http://arxiv.org/abs/2401.08984v1"
  },
  {
    "title": "FedLoGe: Joint Local and Generic Federated Learning under Long-tailed\n  Data",
    "authors": "Zikai Xiao, Zihan Chen, Liyinglan Liu, Yang Feng, Jian Wu, Wanlu Liu, Joey Tianyi Zhou, Howard Hao Yang, Zuozhu Liu",
    "abstract": "  Federated Long-Tailed Learning (Fed-LT), a paradigm wherein data collected\nfrom decentralized local clients manifests a globally prevalent long-tailed\ndistribution, has garnered considerable attention in recent times. In the\ncontext of Fed-LT, existing works have predominantly centered on addressing the\ndata imbalance issue to enhance the efficacy of the generic global model while\nneglecting the performance at the local level. In contrast, conventional\nPersonalized Federated Learning (pFL) techniques are primarily devised to\noptimize personalized local models under the presumption of a balanced global\ndata distribution. This paper introduces an approach termed Federated Local and\nGeneric Model Training in Fed-LT (FedLoGe), which enhances both local and\ngeneric model performance through the integration of representation learning\nand classifier alignment within a neural collapse framework. Our investigation\nreveals the feasibility of employing a shared backbone as a foundational\nframework for capturing overarching global trends, while concurrently employing\nindividualized classifiers to encapsulate distinct refinements stemming from\neach client's local features. Building upon this discovery, we establish the\nStatic Sparse Equiangular Tight Frame Classifier (SSE-C), inspired by neural\ncollapse principles that naturally prune extraneous noisy features and foster\nthe acquisition of potent data representations. Furthermore, leveraging\ninsights from imbalance neural collapse's classifier norm patterns, we develop\nGlobal and Local Adaptive Feature Realignment (GLA-FR) via an auxiliary global\nclassifier and personalized Euclidean norm transfer to align global features\nwith client preferences. Extensive experimental results on CIFAR-10/100-LT,\nImageNet, and iNaturalist demonstrate the advantage of our method over\nstate-of-the-art pFL and Fed-LT approaches.\n",
    "link": "http://arxiv.org/abs/2401.08977v1"
  },
  {
    "title": "OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed\n  Reality",
    "authors": "Aditya Sharma, Luke Yoffe, Tobias H\u00f6llerer",
    "abstract": "  One key challenge in Augmented Reality is the placement of virtual content in\nnatural locations. Most existing automated techniques can only work with a\nclosed-vocabulary, fixed set of objects. In this paper, we introduce and\nevaluate several methods for automatic object placement using recent advances\nin open-vocabulary vision-language models. Through a multifaceted evaluation,\nwe identify a new state-of-the-art method, OCTO+. We also introduce a benchmark\nfor automatically evaluating the placement of virtual objects in augmented\nreality, alleviating the need for costly user studies. Through this, in\naddition to human evaluations, we find that OCTO+ places objects in a valid\nregion over 70% of the time, outperforming other methods on a range of metrics.\n",
    "link": "http://arxiv.org/abs/2401.08973v1"
  },
  {
    "title": "From User Surveys to Telemetry-Driven Agents: Exploring the Potential of\n  Personalized Productivity Solutions",
    "authors": "Subigya Nepal, Javier Hernandez, Talie Massachi, Kael Rowan, Judith Amores, Jina Suh, Gonzalo Ramos, Brian Houck, Shamsi T. Iqbal, Mary Czerwinski",
    "abstract": "  We present a comprehensive, user-centric approach to understand preferences\nin AI-based productivity agents and develop personalized solutions tailored to\nusers' needs. Utilizing a two-phase method, we first conducted a survey with\n363 participants, exploring various aspects of productivity, communication\nstyle, agent approach, personality traits, personalization, and privacy.\nDrawing on the survey insights, we developed a GPT-4 powered personalized\nproductivity agent that utilizes telemetry data gathered via Viva Insights from\ninformation workers to provide tailored assistance. We compared its performance\nwith alternative productivity-assistive tools, such as dashboard and narrative,\nin a study involving 40 participants. Our findings highlight the importance of\nuser-centric design, adaptability, and the balance between personalization and\nprivacy in AI-assisted productivity tools. By building on the insights\ndistilled from our study, we believe that our work can enable and guide future\nresearch to further enhance productivity solutions, ultimately leading to\noptimized efficiency and user experiences for information workers.\n",
    "link": "http://arxiv.org/abs/2401.08960v1"
  },
  {
    "title": "Towards Off-Policy Reinforcement Learning for Ranking Policies with\n  Human Feedback",
    "authors": "Teng Xiao, Suhang Wang",
    "abstract": "  Probabilistic learning to rank (LTR) has been the dominating approach for\noptimizing the ranking metric, but cannot maximize long-term rewards.\nReinforcement learning models have been proposed to maximize user long-term\nrewards by formulating the recommendation as a sequential decision-making\nproblem, but could only achieve inferior accuracy compared to LTR counterparts,\nprimarily due to the lack of online interactions and the characteristics of\nranking. In this paper, we propose a new off-policy value ranking (VR)\nalgorithm that can simultaneously maximize user long-term rewards and optimize\nthe ranking metric offline for improved sample efficiency in a unified\nExpectation-Maximization (EM) framework. We theoretically and empirically show\nthat the EM process guides the leaned policy to enjoy the benefit of\nintegration of the future reward and ranking metric, and learn without any\nonline interactions. Extensive offline and online experiments demonstrate the\neffectiveness of our methods.\n",
    "link": "http://arxiv.org/abs/2401.08959v1"
  },
  {
    "title": "SWBT: Similarity Weighted Behavior Transformer with the Imperfect\n  Demonstration for Robotic Manipulation",
    "authors": "Kun Wu, Ning Liu, Zhen Zhao, Di Qiu, Jinming Li, Zhengping Che, Zhiyuan Xu, Qinru Qiu, Jian Tang",
    "abstract": "  Imitation learning (IL), aiming to learn optimal control policies from expert\ndemonstrations, has been an effective method for robot manipulation tasks.\nHowever, previous IL methods either only use expensive expert demonstrations\nand omit imperfect demonstrations or rely on interacting with the environment\nand learning from online experiences. In the context of robotic manipulation,\nwe aim to conquer the above two challenges and propose a novel framework named\nSimilarity Weighted Behavior Transformer (SWBT). SWBT effectively learn from\nboth expert and imperfect demonstrations without interaction with environments.\nWe reveal that the easy-to-get imperfect demonstrations, such as forward and\ninverse dynamics, significantly enhance the network by learning fruitful\ninformation. To the best of our knowledge, we are the first to attempt to\nintegrate imperfect demonstrations into the offline imitation learning setting\nfor robot manipulation tasks. Extensive experiments on the ManiSkill2 benchmark\nbuilt on the high-fidelity Sapien simulator and real-world robotic manipulation\ntasks demonstrated that the proposed method can extract better features and\nimprove the success rates for all tasks. Our code will be released upon\nacceptance of the paper.\n",
    "link": "http://arxiv.org/abs/2401.08957v1"
  },
  {
    "title": "CEL: A Continual Learning Model for Disease Outbreak Prediction by\n  Leveraging Domain Adaptation via Elastic Weight Consolidation",
    "authors": "Saba Aslam, Abdur Rasool, Hongyan Wu, Xiaoli Li",
    "abstract": "  Continual learning, the ability of a model to learn over time without\nforgetting previous knowledge and, therefore, be adaptive to new data, is\nparamount in dynamic fields such as disease outbreak prediction. Deep neural\nnetworks, i.e., LSTM, are prone to error due to catastrophic forgetting. This\nstudy introduces a novel CEL model for continual learning by leveraging domain\nadaptation via Elastic Weight Consolidation (EWC). This model aims to mitigate\nthe catastrophic forgetting phenomenon in a domain incremental setting. The\nFisher Information Matrix (FIM) is constructed with EWC to develop a\nregularization term that penalizes changes to important parameters, namely, the\nimportant previous knowledge. CEL's performance is evaluated on three distinct\ndiseases, Influenza, Mpox, and Measles, with different metrics. The high\nR-squared values during evaluation and reevaluation outperform the other\nstate-of-the-art models in several contexts, indicating that CEL adapts to\nincremental data well. CEL's robustness and reliability are underscored by its\nminimal 65% forgetting rate and 18% higher memory stability compared to\nexisting benchmark studies. This study highlights CEL's versatility in disease\noutbreak prediction, addressing evolving data with temporal patterns. It offers\na valuable model for proactive disease control with accurate, timely\npredictions.\n",
    "link": "http://arxiv.org/abs/2401.08940v1"
  },
  {
    "title": "DeLF: Designing Learning Environments with Foundation Models",
    "authors": "Aida Afshar, Wenchao Li",
    "abstract": "  Reinforcement learning (RL) offers a capable and intuitive structure for the\nfundamental sequential decision-making problem. Despite impressive\nbreakthroughs, it can still be difficult to employ RL in practice in many\nsimple applications. In this paper, we try to address this issue by introducing\na method for designing the components of the RL environment for a given,\nuser-intended application. We provide an initial formalization for the problem\nof RL component design, that concentrates on designing a good representation\nfor observation and action space. We propose a method named DeLF: Designing\nLearning Environments with Foundation Models, that employs large language\nmodels to design and codify the user's intended learning scenario. By testing\nour method on four different learning environments, we demonstrate that DeLF\ncan obtain executable environment codes for the corresponding RL problems.\n",
    "link": "http://arxiv.org/abs/2401.08936v1"
  },
  {
    "title": "Learning to detect cloud and snow in remote sensing images from noisy\n  labels",
    "authors": "Zili Liu, Hao Chen, Wenyuan Li, Keyan Chen, Zipeng Qi, Chenyang Liu, Zhengxia Zou, Zhenwei Shi",
    "abstract": "  Detecting clouds and snow in remote sensing images is an essential\npreprocessing task for remote sensing imagery. Previous works draw inspiration\nfrom semantic segmentation models in computer vision, with most research\nfocusing on improving model architectures to enhance detection performance.\nHowever, unlike natural images, the complexity of scenes and the diversity of\ncloud types in remote sensing images result in many inaccurate labels in cloud\nand snow detection datasets, introducing unnecessary noises into the training\nand testing processes. By constructing a new dataset and proposing a novel\ntraining strategy with the curriculum learning paradigm, we guide the model in\nreducing overfitting to noisy labels. Additionally, we design a more\nappropriate model performance evaluation method, that alleviates the\nperformance assessment bias caused by noisy labels. By conducting experiments\non models with UNet and Segformer, we have validated the effectiveness of our\nproposed method. This paper is the first to consider the impact of label noise\non the detection of clouds and snow in remote sensing images.\n",
    "link": "http://arxiv.org/abs/2401.08932v1"
  },
  {
    "title": "3D Human Pose Analysis via Diffusion Synthesis",
    "authors": "Haorui Ji, Hongdong Li",
    "abstract": "  Diffusion models have demonstrated remarkable success in generative modeling.\nIn this paper, we propose PADS (Pose Analysis by Diffusion Synthesis), a novel\nframework designed to address various challenges in 3D human pose analysis\nthrough a unified pipeline. Central to PADS are two distinctive strategies: i)\nlearning a task-agnostic pose prior using a diffusion synthesis process to\neffectively capture the kinematic constraints in human pose data, and ii)\nunifying multiple pose analysis tasks like estimation, completion, denoising,\netc, as instances of inverse problems. The learned pose prior will be treated\nas a regularization imposing on task-specific constraints, guiding the\noptimization process through a series of conditional denoising steps. PADS\nrepresents the first diffusion-based framework for tackling general 3D human\npose analysis within the inverse problem framework. Its performance has been\nvalidated on different benchmarks, signaling the adaptability and robustness of\nthis pipeline.\n",
    "link": "http://arxiv.org/abs/2401.08930v1"
  },
  {
    "title": "Bridging State and History Representations: Understanding\n  Self-Predictive RL",
    "authors": "Tianwei Ni, Benjamin Eysenbach, Erfan Seyedsalehi, Michel Ma, Clement Gehring, Aditya Mahajan, Pierre-Luc Bacon",
    "abstract": "  Representations are at the core of all deep reinforcement learning (RL)\nmethods for both Markov decision processes (MDPs) and partially observable\nMarkov decision processes (POMDPs). Many representation learning methods and\ntheoretical frameworks have been developed to understand what constitutes an\neffective representation. However, the relationships between these methods and\nthe shared properties among them remain unclear. In this paper, we show that\nmany of these seemingly distinct methods and frameworks for state and history\nabstractions are, in fact, based on a common idea of self-predictive\nabstraction. Furthermore, we provide theoretical insights into the widely\nadopted objectives and optimization, such as the stop-gradient technique, in\nlearning self-predictive representations. These findings together yield a\nminimalist algorithm to learn self-predictive representations for states and\nhistories. We validate our theories by applying our algorithm to standard MDPs,\nMDPs with distractors, and POMDPs with sparse rewards. These findings culminate\nin a set of practical guidelines for RL practitioners.\n",
    "link": "http://arxiv.org/abs/2401.08898v1"
  },
  {
    "title": "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in\n  Variational AutoEncoder",
    "authors": "Hee-Jun Jung, Jaehyoung Jeong, Kangil Kim",
    "abstract": "  Symmetries of input and latent vectors have provided valuable insights for\ndisentanglement learning in VAEs.However, only a few works were proposed as an\nunsupervised method, and even these works require known factor information in\ntraining data. We propose a novel method, Composite Factor-Aligned Symmetry\nLearning (CFASL), which is integrated into VAEs for learning symmetry-based\ndisentanglement in unsupervised learning without any knowledge of the dataset\nfactor information.CFASL incorporates three novel features for learning\nsymmetry-based disentanglement: 1) Injecting inductive bias to align latent\nvector dimensions to factor-aligned symmetries within an explicit learnable\nsymmetry codebook 2) Learning a composite symmetry to express unknown factors\nchange between two random samples by learning factor-aligned symmetries within\nthe codebook 3) Inducing group equivariant encoder and decoder in training VAEs\nwith the two conditions. In addition, we propose an extended evaluation metric\nfor multi-factor changes in comparison to disentanglement evaluation in VAEs.\nIn quantitative and in-depth qualitative analysis, CFASL demonstrates a\nsignificant improvement of disentanglement in single-factor change, and\nmulti-factor change conditions compared to state-of-the-art methods.\n",
    "link": "http://arxiv.org/abs/2401.08897v1"
  }
]