[
  {
    "title": "ChatQA: Building GPT-4 Level Conversational QA Models",
    "authors": "Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro",
    "abstract": "  In this work, we introduce ChatQA, a family of conversational question\nanswering (QA) models, that obtain GPT-4 level accuracies. Specifically, we\npropose a two-stage instruction tuning method that can significantly improve\nthe zero-shot conversational QA results from large language models (LLMs). To\nhandle retrieval in conversational QA, we fine-tune a dense retriever on a\nmulti-turn QA dataset, which provides comparable results to using the\nstate-of-the-art query rewriting model while largely reducing deployment cost.\nNotably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10\nconversational QA datasets (54.14 vs. 53.90), without relying on any synthetic\ndata from OpenAI GPT models.\n",
    "link": "http://arxiv.org/abs/2401.10225v1"
  },
  {
    "title": "Supervised Fine-tuning in turn Improves Visual Foundation Models",
    "authors": "Xiaohu Jiang, Yixiao Ge, Yuying Ge, Chun Yuan, Ying Shan",
    "abstract": "  Image-text training like CLIP has dominated the pretraining of vision\nfoundation models in recent years. Subsequent efforts have been made to\nintroduce region-level visual learning into CLIP's pretraining but face\nscalability challenges due to the lack of large-scale region-level datasets.\nDrawing inspiration from supervised fine-tuning (SFT) in natural language\nprocessing such as instruction tuning, we explore the potential of fine-grained\nSFT in enhancing the generation of vision foundation models after their\npretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash\nthe fine-grained knowledge of vision foundation models. In ViSFT, the vision\nfoundation model is enhanced by performing visual joint learning on some\nin-domain tasks and then tested on out-of-domain benchmarks. With updating\nusing ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over\n4.4B parameters shows improvements across various out-of-domain benchmarks\nincluding vision and vision-linguistic scenarios.\n",
    "link": "http://arxiv.org/abs/2401.10222v1"
  },
  {
    "title": "Eclectic Rule Extraction for Explainability of Deep Neural Network based\n  Intrusion Detection Systems",
    "authors": "Jesse Ables, Nathaniel Childers, William Anderson, Sudip Mittal, Shahram Rahimi, Ioana Banicescu, Maria Seale",
    "abstract": "  This paper addresses trust issues created from the ubiquity of black box\nalgorithms and surrogate explainers in Explainable Intrusion Detection Systems\n(X-IDS). While Explainable Artificial Intelligence (XAI) aims to enhance\ntransparency, black box surrogate explainers, such as Local Interpretable\nModel-Agnostic Explanation (LIME) and SHapley Additive exPlanation (SHAP), are\ndifficult to trust. The black box nature of these surrogate explainers makes\nthe process behind explanation generation opaque and difficult to understand.\nTo avoid this problem, one can use transparent white box algorithms such as\nRule Extraction (RE). There are three types of RE algorithms: pedagogical,\ndecompositional, and eclectic. Pedagogical methods offer fast but untrustworthy\nwhite-box explanations, while decompositional RE provides trustworthy\nexplanations with poor scalability. This work explores eclectic rule\nextraction, which strikes a balance between scalability and trustworthiness. By\ncombining techniques from pedagogical and decompositional approaches, eclectic\nrule extraction leverages the advantages of both, while mitigating some of\ntheir drawbacks. The proposed Hybrid X-IDS architecture features eclectic RE as\na white box surrogate explainer for black box Deep Neural Networks (DNN). The\npresented eclectic RE algorithm extracts human-readable rules from hidden\nlayers, facilitating explainable and trustworthy rulesets. Evaluations on\nUNSW-NB15 and CIC-IDS-2017 datasets demonstrate the algorithm's ability to\ngenerate rulesets with 99.9% accuracy, mimicking DNN outputs. The contributions\nof this work include the hybrid X-IDS architecture, the eclectic rule\nextraction algorithm applicable to intrusion detection datasets, and a thorough\nanalysis of performance and explainability, demonstrating the trade-offs\ninvolved in rule extraction speed and accuracy.\n",
    "link": "http://arxiv.org/abs/2401.10207v1"
  },
  {
    "title": "Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through\n  Text Reconstruction",
    "authors": "Qingyun Wang, Zixuan Zhang, Hongxiang Li, Xuan Liu, Jiawei Han, Heng Ji, Huimin Zhao",
    "abstract": "  Fine-grained few-shot entity extraction in the chemical domain faces two\nunique challenges. First, compared with entity extraction tasks in the general\ndomain, sentences from chemical papers usually contain more entities. Moreover,\nentity extraction models usually have difficulty extracting entities of\nlong-tailed types. In this paper, we propose Chem-FINESE, a novel\nsequence-to-sequence (seq2seq) based few-shot entity extraction approach, to\naddress these two challenges. Our Chem-FINESE has two components: a seq2seq\nentity extractor to extract named entities from the input sentence and a\nseq2seq self-validation module to reconstruct the original input sentence from\nextracted entities. Inspired by the fact that a good entity extraction system\nneeds to extract entities faithfully, our new self-validation module leverages\nentity extraction results to reconstruct the original input sentence. Besides,\nwe design a new contrastive loss to reduce excessive copying during the\nextraction process. Finally, we release ChemNER+, a new fine-grained chemical\nentity extraction dataset that is annotated by domain experts with the ChemNER\nschema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets\nshow that our newly proposed framework has contributed up to 8.26% and 6.84%\nabsolute F1-score gains respectively.\n",
    "link": "http://arxiv.org/abs/2401.10189v1"
  },
  {
    "title": "Neural Echos: Depthwise Convolutional Filters Replicate Biological\n  Receptive Fields",
    "authors": "Zahra Babaiee, Peyman M. Kiasari, Daniela Rus, Radu Grosu",
    "abstract": "  In this study, we present evidence suggesting that depthwise convolutional\nkernels are effectively replicating the structural intricacies of the\nbiological receptive fields observed in the mammalian retina. We provide\nanalytics of trained kernels from various state-of-the-art models\nsubstantiating this evidence. Inspired by this intriguing discovery, we propose\nan initialization scheme that draws inspiration from the biological receptive\nfields. Experimental analysis of the ImageNet dataset with multiple CNN\narchitectures featuring depthwise convolutions reveals a marked enhancement in\nthe accuracy of the learned model when initialized with biologically derived\nweights. This underlies the potential for biologically inspired computational\nmodels to further our understanding of vision processing systems and to improve\nthe efficacy of convolutional networks.\n",
    "link": "http://arxiv.org/abs/2401.10178v1"
  },
  {
    "title": "Explicitly Disentangled Representations in Object-Centric Learning",
    "authors": "Riccardo Majellaro, Jonathan Collu, Aske Plaat, Thomas M. Moerland",
    "abstract": "  Extracting structured representations from raw visual data is an important\nand long-standing challenge in machine learning. Recently, techniques for\nunsupervised learning of object-centric representations have raised growing\ninterest. In this context, enhancing the robustness of the latent features can\nimprove the efficiency and effectiveness of the training of downstream tasks. A\npromising step in this direction is to disentangle the factors that cause\nvariation in the data. Previously, Invariant Slot Attention disentangled\nposition, scale, and orientation from the remaining features. Extending this\napproach, we focus on separating the shape and texture components. In\nparticular, we propose a novel architecture that biases object-centric models\ntoward disentangling shape and texture components into two non-overlapping\nsubsets of the latent space dimensions. These subsets are known a priori, hence\nbefore the training process. Experiments on a range of object-centric\nbenchmarks reveal that our approach achieves the desired disentanglement while\nalso numerically improving baseline performance in most cases. In addition, we\nshow that our method can generate novel textures for a specific object or\ntransfer textures between objects with distinct shapes.\n",
    "link": "http://arxiv.org/abs/2401.10148v1"
  },
  {
    "title": "Model Compression Techniques in Biometrics Applications: A Survey",
    "authors": "Eduarda Caldeira, Pedro C. Neto, Marco Huber, Naser Damer, Ana F. Sequeira",
    "abstract": "  The development of deep learning algorithms has extensively empowered\nhumanity's task automatization capacity. However, the huge improvement in the\nperformance of these models is highly correlated with their increasing level of\ncomplexity, limiting their usefulness in human-oriented applications, which are\nusually deployed in resource-constrained devices. This led to the development\nof compression techniques that drastically reduce the computational and memory\ncosts of deep learning models without significant performance degradation. This\npaper aims to systematize the current literature on this topic by presenting a\ncomprehensive survey of model compression techniques in biometrics\napplications, namely quantization, knowledge distillation and pruning. We\nconduct a critical analysis of the comparative value of these techniques,\nfocusing on their advantages and disadvantages and presenting suggestions for\nfuture work directions that can potentially improve the current methods.\nAdditionally, we discuss and analyze the link between model bias and model\ncompression, highlighting the need to direct compression research toward model\nfairness in future works.\n",
    "link": "http://arxiv.org/abs/2401.10139v1"
  },
  {
    "title": "Towards Principled Graph Transformers",
    "authors": "Luis M\u00fcller, Christopher Morris",
    "abstract": "  Graph learning architectures based on the k-dimensional Weisfeiler-Leman\n(k-WL) hierarchy offer a theoretically well-understood expressive power.\nHowever, such architectures often fail to deliver solid predictive performance\non real-world tasks, limiting their practical impact. In contrast, global\nattention-based models such as graph transformers demonstrate strong\nperformance in practice, but comparing their expressive power with the k-WL\nhierarchy remains challenging, particularly since these architectures rely on\npositional or structural encodings for their expressivity and predictive\nperformance. To address this, we show that the recently proposed Edge\nTransformer, a global attention model operating on node pairs instead of nodes,\nhas at least 3-WL expressive power. Empirically, we demonstrate that the Edge\nTransformer surpasses other theoretically aligned architectures regarding\npredictive performance while not relying on positional or structural encodings.\n",
    "link": "http://arxiv.org/abs/2401.10119v1"
  },
  {
    "title": "Counterfactual Reasoning with Probabilistic Graphical Models for\n  Analyzing Socioecological Systems",
    "authors": "Rafael Caba\u00f1as, Ana D. Maldonado, Mar\u00eda Morales, Pedro A. Aguilera, Antonio Salmer\u00f3n",
    "abstract": "  Causal and counterfactual reasoning are emerging directions in data science\nthat allow us to reason about hypothetical scenarios. This is particularly\nuseful in domains where experimental data are usually not available. In the\ncontext of environmental and ecological sciences, causality enables us, for\nexample, to predict how an ecosystem would respond to hypothetical\ninterventions. A structural causal model is a class of probabilistic graphical\nmodels for causality, which, due to its intuitive nature, can be easily\nunderstood by experts in multiple fields. However, certain queries, called\nunidentifiable, cannot be calculated in an exact and precise manner. This paper\nproposes applying a novel and recent technique for bounding unidentifiable\nqueries within the domain of socioecological systems. Our findings indicate\nthat traditional statistical analysis, including probabilistic graphical\nmodels, can identify the influence between variables. However, such methods do\nnot offer insights into the nature of the relationship, specifically whether it\ninvolves necessity or sufficiency. This is where counterfactual reasoning\nbecomes valuable.\n",
    "link": "http://arxiv.org/abs/2401.10101v1"
  },
  {
    "title": "DiffusionGPT: LLM-Driven Text-to-Image Generation System",
    "authors": "Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, Shilei Wen",
    "abstract": "  Diffusion models have opened up new avenues for the field of image\ngeneration, resulting in the proliferation of high-quality models shared on\nopen-source platforms. However, a major challenge persists in current\ntext-to-image systems are often unable to handle diverse inputs, or are limited\nto single model results. Current unified attempts often fall into two\northogonal aspects: i) parse Diverse Prompts in input stage; ii) activate\nexpert model to output. To combine the best of both worlds, we propose\nDiffusionGPT, which leverages Large Language Models (LLM) to offer a unified\ngeneration system capable of seamlessly accommodating various types of prompts\nand integrating domain-expert models. DiffusionGPT constructs domain-specific\nTrees for various generative models based on prior knowledge. When provided\nwith an input, the LLM parses the prompt and employs the Trees-of-Thought to\nguide the selection of an appropriate model, thereby relaxing input constraints\nand ensuring exceptional performance across diverse domains. Moreover, we\nintroduce Advantage Databases, where the Tree-of-Thought is enriched with human\nfeedback, aligning the model selection process with human preferences. Through\nextensive experiments and comparisons, we demonstrate the effectiveness of\nDiffusionGPT, showcasing its potential for pushing the boundaries of image\nsynthesis in diverse domains.\n",
    "link": "http://arxiv.org/abs/2401.10061v1"
  },
  {
    "title": "Large Language Models for Scientific Information Extraction: An\n  Empirical Study for Virology",
    "authors": "Mahsa Shamsabadi, Jennifer D'Souza, S\u00f6ren Auer",
    "abstract": "  In this paper, we champion the use of structured and semantic content\nrepresentation of discourse-based scholarly communication, inspired by tools\nlike Wikipedia infoboxes or structured Amazon product descriptions. These\nrepresentations provide users with a concise overview, aiding scientists in\nnavigating the dense academic landscape. Our novel automated approach leverages\nthe robust text generation capabilities of LLMs to produce structured scholarly\ncontribution summaries, offering both a practical solution and insights into\nLLMs' emergent abilities.\n  For LLMs, the prime focus is on improving their general intelligence as\nconversational agents. We argue that these models can also be applied\neffectively in information extraction (IE), specifically in complex IE tasks\nwithin terse domains like Science. This paradigm shift replaces the traditional\nmodular, pipelined machine learning approach with a simpler objective expressed\nthrough instructions. Our results show that finetuned FLAN-T5 with 1000x fewer\nparameters than the state-of-the-art GPT-davinci is competitive for the task.\n",
    "link": "http://arxiv.org/abs/2401.10040v1"
  },
  {
    "title": "LOCALINTEL: Generating Organizational Threat Intelligence from Global\n  and Local Cyber Knowledge",
    "authors": "Shaswata Mitra, Subash Neupane, Trisha Chakraborty, Sudip Mittal, Aritran Piplai, Manas Gaur, Shahram Rahimi",
    "abstract": "  Security Operations Center (SoC) analysts gather threat reports from openly\naccessible global threat databases and customize them manually to suit a\nparticular organization's needs. These analysts also depend on internal\nrepositories, which act as private local knowledge database for an\norganization. Credible cyber intelligence, critical operational details, and\nrelevant organizational information are all stored in these local knowledge\ndatabases. Analysts undertake a labor intensive task utilizing these global and\nlocal knowledge databases to manually create organization's unique threat\nresponse and mitigation strategies. Recently, Large Language Models (LLMs) have\nshown the capability to efficiently process large diverse knowledge sources. We\nleverage this ability to process global and local knowledge databases to\nautomate the generation of organization-specific threat intelligence.\n  In this work, we present LOCALINTEL, a novel automated knowledge\ncontextualization system that, upon prompting, retrieves threat reports from\nthe global threat repositories and uses its local knowledge database to\ncontextualize them for a specific organization. LOCALINTEL comprises of three\nkey phases: global threat intelligence retrieval, local knowledge retrieval,\nand contextualized completion generation. The former retrieves intelligence\nfrom global threat repositories, while the second retrieves pertinent knowledge\nfrom the local knowledge database. Finally, the fusion of these knowledge\nsources is orchestrated through a generator to produce a contextualized\ncompletion.\n",
    "link": "http://arxiv.org/abs/2401.10036v1"
  },
  {
    "title": "Evolutionary Computation in the Era of Large Language Model: Survey and\n  Roadmap",
    "authors": "Xingyu Wu, Sheng-hao Wu, Jibin Wu, Liang Feng, Kay Chen Tan",
    "abstract": "  Large Language Models (LLMs), built upon Transformer-based architectures with\nmassive pretraining on diverse data, have not only revolutionized natural\nlanguage processing but also extended their prowess to various domains, marking\na significant stride towards artificial general intelligence. The interplay\nbetween LLMs and Evolutionary Algorithms (EAs), despite differing in objectives\nand methodologies, reveals intriguing parallels, especially in their shared\noptimization nature, black-box characteristics, and proficiency in handling\ncomplex problems. Meanwhile, EA can not only provide an optimization framework\nfor LLM's further enhancement under black-box settings but also empower LLM\nwith flexible global search and iterative mechanism in applications. On the\nother hand, LLM's abundant domain knowledge enables EA to perform smarter\nsearches, while its text processing capability assist in deploying EA across\nvarious tasks. Based on their complementary advantages, this paper presents a\ncomprehensive review and forward-looking roadmap, categorizing their mutual\ninspiration into LLM-enhanced evolutionary optimization and EA-enhanced LLM.\nSome integrated synergy methods are further introduced to exemplify the\namalgamation of LLMs and EAs in various application scenarios, including neural\narchitecture search, code generation, software engineering, and text\ngeneration. As the first comprehensive review specifically focused on the EA\nresearch in the era of LLMs, this paper provides a foundational stepping stone\nfor understanding and harnessing the collaborative potential of LLMs and EAs.\nBy presenting a comprehensive review, categorization, and critical analysis, we\ncontribute to the ongoing discourse on the cross-disciplinary study of these\ntwo powerful paradigms. The identified challenges and future directions offer\nguidance to unlock the full potential of this innovative collaboration.\n",
    "link": "http://arxiv.org/abs/2401.10034v1"
  },
  {
    "title": "FreGrad: Lightweight and Fast Frequency-aware Diffusion Vocoder",
    "authors": "Tan Dat Nguyen, Ji-Hoon Kim, Youngjoon Jang, Jaehun Kim, Joon Son Chung",
    "abstract": "  The goal of this paper is to generate realistic audio with a lightweight and\nfast diffusion-based vocoder, named FreGrad. Our framework consists of the\nfollowing three key components: (1) We employ discrete wavelet transform that\ndecomposes a complicated waveform into sub-band wavelets, which helps FreGrad\nto operate on a simple and concise feature space, (2) We design a\nfrequency-aware dilated convolution that elevates frequency awareness,\nresulting in generating speech with accurate frequency information, and (3) We\nintroduce a bag of tricks that boosts the generation quality of the proposed\nmodel. In our experiments, FreGrad achieves 3.7 times faster training time and\n2.2 times faster inference speed compared to our baseline while reducing the\nmodel size by 0.6 times (only 1.78M parameters) without sacrificing the output\nquality. Audio samples are available at:\nhttps://mm.kaist.ac.kr/projects/FreGrad.\n",
    "link": "http://arxiv.org/abs/2401.10032v1"
  },
  {
    "title": "Self-Rewarding Language Models",
    "authors": "Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston",
    "abstract": "  We posit that to achieve superhuman agents, future models require superhuman\nfeedback in order to provide an adequate training signal. Current approaches\ncommonly train reward models from human preferences, which may then be\nbottlenecked by human performance level, and secondly these separate frozen\nreward models cannot then learn to improve during LLM training. In this work,\nwe study Self-Rewarding Language Models, where the language model itself is\nused via LLM-as-a-Judge prompting to provide its own rewards during training.\nWe show that during Iterative DPO training that not only does instruction\nfollowing ability improve, but also the ability to provide high-quality rewards\nto itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a\nmodel that outperforms many existing systems on the AlpacaEval 2.0 leaderboard,\nincluding Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study,\nthis work opens the door to the possibility of models that can continually\nimprove in both axes.\n",
    "link": "http://arxiv.org/abs/2401.10020v1"
  },
  {
    "title": "R-Judge: Benchmarking Safety Risk Awareness for LLM Agents",
    "authors": "Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, Rui Wang, Gongshen Liu",
    "abstract": "  Large language models (LLMs) have exhibited great potential in autonomously\ncompleting tasks across real-world applications. Despite this, these LLM agents\nintroduce unexpected safety risks when operating in interactive environments.\nInstead of centering on LLM-generated content safety in most prior studies,\nthis work addresses the imperative need for benchmarking the behavioral safety\nof LLM agents within diverse environments. We introduce R-Judge, a benchmark\ncrafted to evaluate the proficiency of LLMs in judging safety risks given agent\ninteraction records. R-Judge comprises 162 agent interaction records,\nencompassing 27 key risk scenarios among 7 application categories and 10 risk\ntypes. It incorporates human consensus on safety with annotated safety risk\nlabels and high-quality risk descriptions. Utilizing R-Judge, we conduct a\ncomprehensive evaluation of 8 prominent LLMs commonly employed as the backbone\nfor agents. The best-performing model, GPT-4, achieves 72.29% in contrast to\nthe human score of 89.38%, showing considerable room for enhancing the risk\nawareness of LLMs. Notably, leveraging risk descriptions as environment\nfeedback significantly improves model performance, revealing the importance of\nsalient safety risk feedback. Furthermore, we design an effective chain of\nsafety analysis technique to help the judgment of safety risks and conduct an\nin-depth case study to facilitate future research. R-Judge is publicly\navailable at https://github.com/Lordog/R-Judge.\n",
    "link": "http://arxiv.org/abs/2401.10019v1"
  },
  {
    "title": "Gender Bias in Machine Translation and The Era of Large Language Models",
    "authors": "Eva Vanmassenhove",
    "abstract": "  This chapter examines the role of Machine Translation in perpetuating gender\nbias, highlighting the challenges posed by cross-linguistic settings and\nstatistical dependencies. A comprehensive overview of relevant existing work\nrelated to gender bias in both conventional Neural Machine Translation\napproaches and Generative Pretrained Transformer models employed as Machine\nTranslation systems is provided. Through an experiment using ChatGPT (based on\nGPT-3.5) in an English-Italian translation context, we further assess ChatGPT's\ncurrent capacity to address gender bias. The findings emphasize the ongoing\nneed for advancements in mitigating bias in Machine Translation systems and\nunderscore the importance of fostering fairness and inclusivity in language\ntechnologies.\n",
    "link": "http://arxiv.org/abs/2401.10016v1"
  },
  {
    "title": "A-KIT: Adaptive Kalman-Informed Transformer",
    "authors": "Nadav Cohen, Itzik Klein",
    "abstract": "  The extended Kalman filter (EKF) is a widely adopted method for sensor fusion\nin navigation applications. A crucial aspect of the EKF is the online\ndetermination of the process noise covariance matrix reflecting the model\nuncertainty. While common EKF implementation assumes a constant process noise,\nin real-world scenarios, the process noise varies, leading to inaccuracies in\nthe estimated state and potentially causing the filter to diverge. To cope with\nsuch situations, model-based adaptive EKF methods were proposed and\ndemonstrated performance improvements, highlighting the need for a robust\nadaptive approach. In this paper, we derive and introduce A-KIT, an adaptive\nKalman-informed transformer to learn the varying process noise covariance\nonline. The A-KIT framework is applicable to any type of sensor fusion. Here,\nwe present our approach to nonlinear sensor fusion based on an inertial\nnavigation system and Doppler velocity log. By employing real recorded data\nfrom an autonomous underwater vehicle, we show that A-KIT outperforms the\nconventional EKF by more than 49.5% and model-based adaptive EKF by an average\nof 35.4% in terms of position accuracy.\n",
    "link": "http://arxiv.org/abs/2401.09987v1"
  },
  {
    "title": "FLex&amp;Chill: Improving Local Federated Learning Training with Logit\n  Chilling",
    "authors": "Kichang Lee, Songkuk Kim, JeongGil Ko",
    "abstract": "  Federated learning are inherently hampered by data heterogeneity: non-iid\ndistributed training data over local clients. We propose a novel model training\napproach for federated learning, FLex&amp;Chill, which exploits the Logit Chilling\nmethod. Through extensive evaluations, we demonstrate that, in the presence of\nnon-iid data characteristics inherent in federated learning systems, this\napproach can expedite model convergence and improve inference accuracy.\nQuantitatively, from our experiments, we observe up to 6X improvement in the\nglobal federated learning model convergence time, and up to 3.37% improvement\nin inference accuracy.\n",
    "link": "http://arxiv.org/abs/2401.09986v1"
  },
  {
    "title": "Multiobjective Optimization Analysis for Finding Infrastructure-as-Code\n  Deployment Configurations",
    "authors": "Eneko Osaba, Josu Diaz-de-Arcaya, Juncal Alonso, Jesus L. Lobo, Gorka Benguria, I\u00f1aki Etxaniz",
    "abstract": "  Multiobjective optimization is a hot topic in the artificial intelligence and\noperations research communities. The design and development of multiobjective\nmethods is a frequent task for researchers and practitioners. As a result of\nthis vibrant activity, a myriad of techniques have been proposed in the\nliterature to date, demonstrating a significant effectiveness for dealing with\nsituations coming from a wide range of real-world areas. This paper is focused\non a multiobjective problem related to optimizing Infrastructure-as-Code\ndeployment configurations. The system implemented for solving this problem has\nbeen coined as IaC Optimizer Platform (IOP). Despite the fact that a\nprototypical version of the IOP has been introduced in the literature before, a\ndeeper analysis focused on the resolution of the problem is needed, in order to\ndetermine which is the most appropriate multiobjective method for embedding in\nthe IOP. The main motivation behind the analysis conducted in this work is to\nenhance the IOP performance as much as possible. This is a crucial aspect of\nthis system, deeming that it will be deployed in a real environment, as it is\nbeing developed as part of a H2020 European project. Going deeper, we resort in\nthis paper to nine different evolutionary computation-based multiobjective\nalgorithms. For assessing the quality of the considered solvers, 12 different\nproblem instances have been generated based on real-world settings. Results\nobtained by each method after 10 independent runs have been compared using\nFriedman's non-parametric tests. Findings reached from the tests carried out\nlad to the creation of a multi-algorithm system, capable of applying different\ntechniques according to the user's needs.\n",
    "link": "http://arxiv.org/abs/2401.09983v1"
  },
  {
    "title": "Towards Generative Abstract Reasoning: Completing Raven's Progressive\n  Matrix via Rule Abstraction and Selection",
    "authors": "Fan Shi, Bin Li, Xiangyang Xue",
    "abstract": "  Endowing machines with abstract reasoning ability has been a long-term\nresearch topic in artificial intelligence. Raven's Progressive Matrix (RPM) is\nwidely used to probe abstract visual reasoning in machine intelligence, where\nmodels need to understand the underlying rules and select the missing\nbottom-right images out of candidate sets to complete image matrices. The\nparticipators can display powerful reasoning ability by inferring the\nunderlying attribute-changing rules and imagining the missing images at\narbitrary positions. However, existing solvers can hardly manifest such an\nability in realistic RPM problems. In this paper, we propose a conditional\ngenerative model to solve answer generation problems through Rule AbstractIon\nand SElection (RAISE) in the latent space. RAISE encodes image attributes as\nlatent concepts and decomposes underlying rules into atomic rules by means of\nconcepts, which are abstracted as global learnable parameters. When generating\nthe answer, RAISE selects proper atomic rules out of the global knowledge set\nfor each concept and composes them into the integrated rule of an RPM. In most\nconfigurations, RAISE outperforms the compared generative solvers in tasks of\ngenerating bottom-right and arbitrary-position answers. We test RAISE in the\nodd-one-out task and two held-out configurations to demonstrate how learning\ndecoupled latent concepts and atomic rules helps find the image breaking the\nunderlying rules and handle RPMs with unseen combinations of rules and\nattributes.\n",
    "link": "http://arxiv.org/abs/2401.09966v1"
  },
  {
    "title": "When Neural Code Completion Models Size up the Situation: Attaining\n  Cheaper and Faster Completion through Dynamic Model Inference",
    "authors": "Zhensu Sun, Xiaoning Du, Fu Song, Shangwen Wang, Li Li",
    "abstract": "  Leveraging recent advancements in large language models, modern neural code\ncompletion models have demonstrated the capability to generate highly accurate\ncode suggestions. However, their massive size poses challenges in terms of\ncomputational costs and environmental impact, hindering their widespread\nadoption in practical scenarios. Dynamic inference emerges as a promising\nsolution, as it allocates minimal computation during inference while\nmaintaining the model's performance. In this research, we explore dynamic\ninference within the context of code completion. Initially, we conducted an\nempirical investigation on GPT-2, focusing on the inference capabilities of\nintermediate layers for code completion. We found that 54.4% of tokens can be\naccurately generated using just the first layer, signifying significant\ncomputational savings potential. Moreover, despite using all layers, the model\nstill fails to predict 14.5% of tokens correctly, and the subsequent\ncompletions continued from them are rarely considered helpful, with only a 4.2%\nAcceptance Rate. These findings motivate our exploration of dynamic inference\nin code completion and inspire us to enhance it with a decision-making\nmechanism that stops the generation of incorrect code. We thus propose a novel\ndynamic inference method specifically tailored for code completion models. This\nmethod aims not only to produce correct predictions with largely reduced\ncomputation but also to prevent incorrect predictions proactively. Our\nextensive evaluation shows that it can averagely skip 1.7 layers out of 16\nlayers in the models, leading to an 11.2% speedup with only a marginal 1.1%\nreduction in ROUGE-L.\n",
    "link": "http://arxiv.org/abs/2401.09964v1"
  },
  {
    "title": "WindSeer: Real-time volumetric wind prediction over complex terrain\n  aboard a small UAV",
    "authors": "Florian Achermann, Thomas Stastny, Bogdan Danciu, Andrey Kolobov, Jen Jen Chung, Roland Siegwart, Nicholas Lawrance",
    "abstract": "  Real-time high-resolution wind predictions are beneficial for various\napplications including safe manned and unmanned aviation. Current weather\nmodels require too much compute and lack the necessary predictive capabilities\nas they are valid only at the scale of multiple kilometers and hours - much\nlower spatial and temporal resolutions than these applications require. Our\nwork, for the first time, demonstrates the ability to predict low-altitude wind\nin real-time on limited-compute devices, from only sparse measurement data. We\ntrain a neural network, WindSeer, using only synthetic data from computational\nfluid dynamics simulations and show that it can successfully predict real wind\nfields over terrain with known topography from just a few noisy and spatially\nclustered wind measurements. WindSeer can generate accurate predictions at\ndifferent resolutions and domain sizes on previously unseen topography without\nretraining. We demonstrate that the model successfully predicts historical wind\ndata collected by weather stations and wind measured onboard drones.\n",
    "link": "http://arxiv.org/abs/2401.09944v1"
  },
  {
    "title": "Multi-task Learning for Joint Re-identification, Team Affiliation, and\n  Role Classification for Sports Visual Tracking",
    "authors": "Amir M. Mansourian, Vladimir Somers, Christophe De Vleeschouwer, Shohreh Kasaei",
    "abstract": "  Effective tracking and re-identification of players is essential for\nanalyzing soccer videos. But, it is a challenging task due to the non-linear\nmotion of players, the similarity in appearance of players from the same team,\nand frequent occlusions. Therefore, the ability to extract meaningful\nembeddings to represent players is crucial in developing an effective tracking\nand re-identification system. In this paper, a multi-purpose part-based person\nrepresentation method, called PRTreID, is proposed that performs three tasks of\nrole classification, team affiliation, and re-identification, simultaneously.\nIn contrast to available literature, a single network is trained with\nmulti-task supervision to solve all three tasks, jointly. The proposed joint\nmethod is computationally efficient due to the shared backbone. Also, the\nmulti-task learning leads to richer and more discriminative representations, as\ndemonstrated by both quantitative and qualitative results. To demonstrate the\neffectiveness of PRTreID, it is integrated with a state-of-the-art tracking\nmethod, using a part-based post-processing module to handle long-term tracking.\nThe proposed tracking method outperforms all existing tracking methods on the\nchallenging SoccerNet tracking dataset.\n",
    "link": "http://arxiv.org/abs/2401.09942v1"
  },
  {
    "title": "XAI-Enhanced Semantic Segmentation Models for Visual Quality Inspection",
    "authors": "Tobias Clement, Truong Thanh Hung Nguyen, Mohamed Abdelaal, Hung Cao",
    "abstract": "  Visual quality inspection systems, crucial in sectors like manufacturing and\nlogistics, employ computer vision and machine learning for precise, rapid\ndefect detection. However, their unexplained nature can hinder trust, error\nidentification, and system improvement. This paper presents a framework to\nbolster visual quality inspection by using CAM-based explanations to refine\nsemantic segmentation models. Our approach consists of 1) Model Training, 2)\nXAI-based Model Explanation, 3) XAI Evaluation, and 4) Annotation Augmentation\nfor Model Enhancement, informed by explanations and expert insights.\nEvaluations show XAI-enhanced models surpass original DeepLabv3-ResNet101\nmodels, especially in intricate object segmentation.\n",
    "link": "http://arxiv.org/abs/2401.09900v1"
  },
  {
    "title": "Cooperative Edge Caching Based on Elastic Federated and Multi-Agent Deep\n  Reinforcement Learning in Next-Generation Network",
    "authors": "Qiong Wu, Wenhua Wang, Pingyi Fan, Qiang Fan, Huiling Zhu, Khaled B. Letaief",
    "abstract": "  Edge caching is a promising solution for next-generation networks by\nempowering caching units in small-cell base stations (SBSs), which allows user\nequipments (UEs) to fetch users' requested contents that have been pre-cached\nin SBSs. It is crucial for SBSs to predict accurate popular contents through\nlearning while protecting users' personal information. Traditional federated\nlearning (FL) can protect users' privacy but the data discrepancies among UEs\ncan lead to a degradation in model quality. Therefore, it is necessary to train\npersonalized local models for each UE to predict popular contents accurately.\nIn addition, the cached contents can be shared among adjacent SBSs in\nnext-generation networks, thus caching predicted popular contents in different\nSBSs may affect the cost to fetch contents. Hence, it is critical to determine\nwhere the popular contents are cached cooperatively. To address these issues,\nwe propose a cooperative edge caching scheme based on elastic federated and\nmulti-agent deep reinforcement learning (CEFMR) to optimize the cost in the\nnetwork. We first propose an elastic FL algorithm to train the personalized\nmodel for each UE, where adversarial autoencoder (AAE) model is adopted for\ntraining to improve the prediction accuracy, then {a popular} content\nprediction algorithm is proposed to predict the popular contents for each SBS\nbased on the trained AAE model. Finally, we propose a multi-agent deep\nreinforcement learning (MADRL) based algorithm to decide where the predicted\npopular contents are collaboratively cached among SBSs. Our experimental\nresults demonstrate the superiority of our proposed scheme to existing baseline\ncaching schemes.\n",
    "link": "http://arxiv.org/abs/2401.09886v1"
  },
  {
    "title": "Attention-Based Recurrent Neural Network For Automatic Behavior Laying\n  Hen Recognition",
    "authors": "Fr\u00e9jus A. A. Laleye, Mika\u00ebl A. Mousse",
    "abstract": "  One of the interests of modern poultry farming is the vocalization of laying\nhens which contain very useful information on health behavior. This information\nis used as health and well-being indicators that help breeders better monitor\nlaying hens, which involves early detection of problems for rapid and more\neffective intervention. In this work, we focus on the sound analysis for the\nrecognition of the types of calls of the laying hens in order to propose a\nrobust system of characterization of their behavior for a better monitoring. To\ndo this, we first collected and annotated laying hen call signals, then\ndesigned an optimal acoustic characterization based on the combination of time\nand frequency domain features. We then used these features to build the\nmulti-label classification models based on recurrent neural network to assign a\nsemantic class to the vocalization that characterize the laying hen behavior.\nThe results show an overall performance with our model based on the combination\nof time and frequency domain features that obtained the highest F1-score\n(F1=92.75) with a gain of 17% on the models using the frequency domain features\nand of 8% on the compared approaches from the litterature.\n",
    "link": "http://arxiv.org/abs/2401.09880v1"
  },
  {
    "title": "Reconciling Spatial and Temporal Abstractions for Goal Representation",
    "authors": "Mehdi Zadem, Sergio Mover, Sao Mai Nguyen",
    "abstract": "  Goal representation affects the performance of Hierarchical Reinforcement\nLearning (HRL) algorithms by decomposing the complex learning problem into\neasier subtasks. Recent studies show that representations that preserve\ntemporally abstract environment dynamics are successful in solving difficult\nproblems and provide theoretical guarantees for optimality. These methods\nhowever cannot scale to tasks where environment dynamics increase in complexity\ni.e. the temporally abstract transition relations depend on larger number of\nvariables. On the other hand, other efforts have tried to use spatial\nabstraction to mitigate the previous issues. Their limitations include\nscalability to high dimensional environments and dependency on prior knowledge.\n  In this paper, we propose a novel three-layer HRL algorithm that introduces,\nat different levels of the hierarchy, both a spatial and a temporal goal\nabstraction. We provide a theoretical study of the regret bounds of the learned\npolicies. We evaluate the approach on complex continuous control tasks,\ndemonstrating the effectiveness of spatial and temporal abstractions learned by\nthis approach.\n",
    "link": "http://arxiv.org/abs/2401.09870v1"
  },
  {
    "title": "Improving fine-grained understanding in image-text pre-training",
    "authors": "Ioana Bica, Anastasija Ili\u0107, Matthias Bauer, Goker Erdogan, Matko Bo\u0161njak, Christos Kaplanis, Alexey A. Gritsenko, Matthias Minderer, Charles Blundell, Razvan Pascanu, Jovana Mitrovi\u0107",
    "abstract": "  We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple\nmethod for pretraining more fine-grained multimodal representations from\nimage-text pairs. Given that multiple image patches often correspond to single\nwords, we propose to learn a grouping of image patches for every token in the\ncaption. To achieve this, we use a sparse similarity metric between image\npatches and language tokens and compute for each token a language-grouped\nvision embedding as the weighted average of patches. The token and\nlanguage-grouped vision embeddings are then contrasted through a fine-grained\nsequence-wise loss that only depends on individual samples and does not require\nother batch samples as negatives. This enables more detailed information to be\nlearned in a computationally inexpensive manner. SPARC combines this\nfine-grained loss with a contrastive loss between global image and text\nembeddings to learn representations that simultaneously encode global and local\ninformation. We thoroughly evaluate our proposed method and show improved\nperformance over competing approaches both on image-level tasks relying on\ncoarse-grained information, e.g. classification, as well as region-level tasks\nrelying on fine-grained information, e.g. retrieval, object detection, and\nsegmentation. Moreover, SPARC improves model faithfulness and captioning in\nfoundational vision-language models.\n",
    "link": "http://arxiv.org/abs/2401.09865v1"
  },
  {
    "title": "Evolutionary Multi-Objective Optimization of Large Language Model\n  Prompts for Balancing Sentiments",
    "authors": "Jill Baumann, Oliver Kramer",
    "abstract": "  The advent of large language models (LLMs) such as ChatGPT has attracted\nconsiderable attention in various domains due to their remarkable performance\nand versatility. As the use of these models continues to grow, the importance\nof effective prompt engineering has come to the fore. Prompt optimization\nemerges as a crucial challenge, as it has a direct impact on model performance\nand the extraction of relevant information. Recently, evolutionary algorithms\n(EAs) have shown promise in addressing this issue, paving the way for novel\noptimization strategies. In this work, we propose a evolutionary\nmulti-objective (EMO) approach specifically tailored for prompt optimization\ncalled EMO-Prompts, using sentiment analysis as a case study. We use sentiment\nanalysis capabilities as our experimental targets. Our results demonstrate that\nEMO-Prompts effectively generates prompts capable of guiding the LLM to produce\ntexts embodying two conflicting emotions simultaneously.\n",
    "link": "http://arxiv.org/abs/2401.09862v1"
  },
  {
    "title": "Temporal Insight Enhancement: Mitigating Temporal Hallucination in\n  Multimodal Large Language Models",
    "authors": "Li Sun, Liuan Wang, Jun Sun, Takayuki Okatani",
    "abstract": "  Recent advancements in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced the comprehension of multimedia content, bringing\ntogether diverse modalities such as text, images, and videos. However, a\ncritical challenge faced by these models, especially when processing video\ninputs, is the occurrence of hallucinations - erroneous perceptions or\ninterpretations, particularly at the event level. This study introduces an\ninnovative method to address event-level hallucinations in MLLMs, focusing on\nspecific temporal understanding in video content. Our approach leverages a\nnovel framework that extracts and utilizes event-specific information from both\nthe event query and the provided video to refine MLLMs' response. We propose a\nunique mechanism that decomposes on-demand event queries into iconic actions.\nSubsequently, we employ models like CLIP and BLIP2 to predict specific\ntimestamps for event occurrences. Our evaluation, conducted using the\nCharades-STA dataset, demonstrates a significant reduction in temporal\nhallucinations and an improvement in the quality of event-related responses.\nThis research not only provides a new perspective in addressing a critical\nlimitation of MLLMs but also contributes a quantitatively measurable method for\nevaluating MLLMs in the context of temporal-related questions.\n",
    "link": "http://arxiv.org/abs/2401.09861v1"
  },
  {
    "title": "Enhancing the Fairness and Performance of Edge Cameras with Explainable\n  AI",
    "authors": "Truong Thanh Hung Nguyen, Vo Thanh Khang Nguyen, Quoc Hung Cao, Van Binh Truong, Quoc Khanh Nguyen, Hung Cao",
    "abstract": "  The rising use of Artificial Intelligence (AI) in human detection on Edge\ncamera systems has led to accurate but complex models, challenging to interpret\nand debug. Our research presents a diagnostic method using Explainable AI (XAI)\nfor model debugging, with expert-driven problem identification and solution\ncreation. Validated on the Bytetrack model in a real-world office Edge network,\nwe found the training dataset as the main bias source and suggested model\naugmentation as a solution. Our approach helps identify model biases, essential\nfor achieving fair and trustworthy models.\n",
    "link": "http://arxiv.org/abs/2401.09852v1"
  },
  {
    "title": "Behavioral Simulation: Exploring A Possible Next Paradigm for Science",
    "authors": "Cheng Wang, Chuwen Wang, Yu Zhao, Shirong Zeng, Wang Zhang, Ronghui Ning",
    "abstract": "  Simulation technologies have been widely utilized in many scientific research\nfields such as weather forecasting, fluid mechanics and biological populations.\nIt is the best tool to handle problems in complex systems, where closed-form\nexpressions are unavailable and the target distribution in the representation\nspace is too complex to be fully represented by a deep learning (DL) model. We\nbelieve that the development of simulation technologies is consistent with\nscientific paradigms. This paper induces the evolution of scientific paradigms\nfrom the perspective of data, algorithms, and computational power. Building\nupon this perspective, we divide simulation technologies into three stages\naligning with the emergence of new paradigms, and find that advanced simulation\ntechnologies are typical instances of paradigms integration. Moreover, we\npropose the concept of behavioral simulation (BS), specifically sophisticated\nbehavioral simulation (SBS), representing a higher degree of paradigms\nintegration based on foundation models to simulate complex social systems\ninvolving sophisticated human strategies and behaviors. BS and further SBS are\ndesigned to tackle challenges concerning the complex human system that\nsurpasses the capacity of traditional agent-based modeling simulation (ABMS),\nwhich can be regarded as a possible next paradigm for science. Through this\nwork, we look forward to more powerful BS and SBS applications in scientific\nresearch branches within social science.\n",
    "link": "http://arxiv.org/abs/2401.09851v1"
  },
  {
    "title": "Slicer Networks",
    "authors": "Hang Zhang, Xiang Chen, Rongguang Wang, Renjiu Hu, Dongdong Liu, Gaolei Li",
    "abstract": "  In medical imaging, scans often reveal objects with varied contrasts but\nconsistent internal intensities or textures. This characteristic enables the\nuse of low-frequency approximations for tasks such as segmentation and\ndeformation field estimation. Yet, integrating this concept into neural network\narchitectures for medical image analysis remains underexplored. In this paper,\nwe propose the Slicer Network, a novel architecture designed to leverage these\ntraits. Comprising an encoder utilizing models like vision transformers for\nfeature extraction and a slicer employing a learnable bilateral grid, the\nSlicer Network strategically refines and upsamples feature maps via a\nsplatting-blurring-slicing process. This introduces an edge-preserving\nlow-frequency approximation for the network outcome, effectively enlarging the\neffective receptive field. The enhancement not only reduces computational\ncomplexity but also boosts overall performance. Experiments across different\nmedical imaging applications, including unsupervised and keypoints-based image\nregistration and lesion segmentation, have verified the Slicer Network's\nimproved accuracy and efficiency.\n",
    "link": "http://arxiv.org/abs/2401.09833v1"
  },
  {
    "title": "PPNet: A Novel Neural Network Structure for End-to-End Near-Optimal Path\n  Planning",
    "authors": "Qinglong Meng, Chongkun Xia, Xueqian Wang, Songping Mai, Bin Liang",
    "abstract": "  The classical path planners, such as sampling-based path planners, have the\nlimitations of sensitivity to the initial solution and slow convergence to the\noptimal solution. However, finding a near-optimal solution in a short period is\nchallenging in many applications such as the autonomous vehicle with limited\npower/fuel. To achieve an end-to-end near-optimal path planner, we first divide\nthe path planning problem into two subproblems, which are path's space\nsegmentation and waypoints generation in the given path's space. We further\npropose a two-level cascade neural network named Path Planning Network (PPNet)\nto solve the path planning problem by solving the abovementioned subproblems.\nMoreover, we propose a novel efficient data generation method for path planning\nnamed EDaGe-PP. The results show the total computation time is less than 1/33\nand the success rate of PPNet trained by the dataset that is generated by\nEDaGe-PP is about $2 \\times$ compared to other methods. We validate PPNet\nagainst state-of-the-art path planning methods. The results show PPNet can find\na near-optimal solution in 15.3ms, which is much shorter than the\nstate-of-the-art path planners.\n",
    "link": "http://arxiv.org/abs/2401.09819v1"
  },
  {
    "title": "All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks",
    "authors": "Kazuhiro Takemoto",
    "abstract": "  Large Language Models (LLMs) like ChatGPT face `jailbreak' challenges, where\nsafeguards are bypassed to produce ethically harmful prompts. This study\nintroduces a simple black-box method to effectively generate jailbreak prompts,\novercoming the limitations of high complexity and computational costs\nassociated with existing methods. The proposed technique iteratively rewrites\nharmful prompts into non-harmful expressions using the target LLM itself, based\non the hypothesis that LLMs can directly sample safeguard-bypassing\nexpressions. Demonstrated through experiments with ChatGPT (GPT-3.5 and GPT-4)\nand Gemini-Pro, this method achieved an attack success rate of over 80% within\nan average of 5 iterations and remained effective despite model updates. The\njailbreak prompts generated were naturally-worded and concise, suggesting they\nare less detectable. The results indicate that creating effective jailbreak\nprompts is simpler than previously considered, and black-box jailbreak attacks\npose a more serious security threat.\n",
    "link": "http://arxiv.org/abs/2401.09798v1"
  },
  {
    "title": "A Comparative Analysis on Metaheuristic Algorithms Based Vision\n  Transformer Model for Early Detection of Alzheimer's Disease",
    "authors": "Anuvab Sen, Udayon Sen, Subhabrata Roy",
    "abstract": "  A number of life threatening neuro-degenerative disorders had degraded the\nquality of life for the older generation in particular. Dementia is one such\nsymptom which may lead to a severe condition called Alzheimer's disease if not\ndetected at an early stage. It has been reported that the progression of such\ndisease from a normal stage is due to the change in several parameters inside\nthe human brain. In this paper, an innovative metaheuristic algorithms based\nViT model has been proposed for the identification of dementia at different\nstage. A sizeable number of test data have been utilized for the validation of\nthe proposed scheme. It has also been demonstrated that our model exhibits\nsuperior performance in terms of accuracy, precision, recall as well as\nF1-score.\n",
    "link": "http://arxiv.org/abs/2401.09795v1"
  },
  {
    "title": "A Semantic Approach for Big Data Exploration in Industry 4.0",
    "authors": "Idoia Berges, V\u00edctor Julio Ram\u00edrez-Dur\u00e1n, Arantza Illarramendi",
    "abstract": "  The growing trends in automation, Internet of Things, big data and cloud\ncomputing technologies have led to the fourth industrial revolution (Industry\n4.0), where it is possible to visualize and identify patterns and insights,\nwhich results in a better understanding of the data and can improve the\nmanufacturing process. However, many times, the task of data exploration\nresults difficult for manufacturing experts because they might be interested in\nanalyzing also data that does not appear in pre-designed visualizations and\ntherefore they must be assisted by Information Technology experts. In this\npaper, we present a proposal materialized in a semantic-based visual query\nsystem developed for a real Industry 4.0 scenario that allows domain experts to\nexplore and visualize data in a friendly way. The main novelty of the system is\nthe combined use that it makes of captured data that are semantically annotated\nfirst, and a 2D customized digital representation of a machine that is also\nlinked with semantic descriptions. Those descriptions are expressed using terms\nof an ontology, where, among others, the sensors that are used to capture\nindicators about the performance of a machine that belongs to a Industry 4.0\nscenario have been modeled. Moreover, this semantic description allows to:\nformulate queries at a higher level of abstraction, provide customized\ngraphical visualizations of the results based on the format and nature of the\ndata, and download enriched data enabling further types of analysis.\n",
    "link": "http://arxiv.org/abs/2401.09789v1"
  },
  {
    "title": "Querying Easily Flip-flopped Samples for Deep Active Learning",
    "authors": "Seong Jin Cho, Gwangsu Kim, Junghyun Lee, Jinwoo Shin, Chang D. Yoo",
    "abstract": "  Active learning is a machine learning paradigm that aims to improve the\nperformance of a model by strategically selecting and querying unlabeled data.\nOne effective selection strategy is to base it on the model's predictive\nuncertainty, which can be interpreted as a measure of how informative a sample\nis. The sample's distance to the decision boundary is a natural measure of\npredictive uncertainty, but it is often intractable to compute, especially for\ncomplex decision boundaries formed in multiclass classification tasks. To\naddress this issue, this paper proposes the {\\it least disagree metric} (LDM),\ndefined as the smallest probability of disagreement of the predicted label, and\nan estimator for LDM proven to be asymptotically consistent under mild\nassumptions. The estimator is computationally efficient and can be easily\nimplemented for deep learning models using parameter perturbation. The\nLDM-based active learning is performed by querying unlabeled data with the\nsmallest LDM. Experimental results show that our LDM-based active learning\nalgorithm obtains state-of-the-art overall performance on all considered\ndatasets and deep architectures.\n",
    "link": "http://arxiv.org/abs/2401.09787v1"
  },
  {
    "title": "Adaptive Self-training Framework for Fine-grained Scene Graph Generation",
    "authors": "Kibum Kim, Kanghoon Yoon, Yeonjun In, Jinyoung Moon, Donghyun Kim, Chanyoung Park",
    "abstract": "  Scene graph generation (SGG) models have suffered from inherent problems\nregarding the benchmark datasets such as the long-tailed predicate distribution\nand missing annotation problems. In this work, we aim to alleviate the\nlong-tailed problem of SGG by utilizing unannotated triplets. To this end, we\nintroduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels\nfor unannotated triplets based on which the SGG models are trained. While there\nhas been significant progress in self-training for image recognition, designing\na self-training framework for the SGG task is more challenging due to its\ninherent nature such as the semantic ambiguity and the long-tailed distribution\nof predicate classes. Hence, we propose a novel pseudo-labeling technique for\nSGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is\na model-agnostic framework that can be applied to any existing SGG models.\nFurthermore, we devise a graph structure learner (GSL) that is beneficial when\nadopting our proposed self-training framework to the state-of-the-art\nmessage-passing neural network (MPNN)-based SGG models. Our extensive\nexperiments verify the effectiveness of ST-SGG on various SGG models,\nparticularly in enhancing the performance on fine-grained predicate classes.\n",
    "link": "http://arxiv.org/abs/2401.09786v1"
  },
  {
    "title": "SEINE: Structure Encoding and Interaction Network for Nuclei Instance\n  Segmentation",
    "authors": "Ye Zhang, Linghan Cai, Ziyue Wang, Yongbing Zhang",
    "abstract": "  Nuclei instance segmentation in histopathological images is of great\nimportance for biological analysis and cancer diagnosis but remains challenging\nfor two reasons. (1) Similar visual presentation of intranuclear and\nextranuclear regions of chromophobe nuclei often causes under-segmentation, and\n(2) current methods lack the exploration of nuclei structure, resulting in\nfragmented instance predictions. To address these problems, this paper proposes\na structure encoding and interaction network, termed SEINE, which develops the\nstructure modeling scheme of nuclei and exploits the structure similarity\nbetween nuclei to improve the integrality of each segmented instance.\nConcretely, SEINE introduces a contour-based structure encoding (SE) that\nconsiders the correlation between nuclei structure and semantics, realizing a\nreasonable representation of the nuclei structure. Based on the encoding, we\npropose a structure-guided attention (SGA) that takes the clear nuclei as\nprototypes to enhance the structure learning for the fuzzy nuclei. To\nstrengthen the structural learning ability, a semantic feature fusion (SFF) is\npresented to boost the semantic consistency of semantic and structure branches.\nFurthermore, a position enhancement (PE) method is applied to suppress\nincorrect nuclei boundary predictions. Extensive experiments demonstrate the\nsuperiority of our approaches, and SEINE achieves state-of-the-art (SOTA)\nperformance on four datasets. The code is available at\n\\href{https://github.com/zhangye-zoe/SEINE}{https://github.com/zhangye-zoe/SEINE}.\n",
    "link": "http://arxiv.org/abs/2401.09773v1"
  },
  {
    "title": "Towards Learning from Graphs with Heterophily: Progress and Future",
    "authors": "Chenghua Gong, Yao Cheng, Xiang Li, Caihua Shan, Siqiang Luo, Chuan Shi",
    "abstract": "  Graphs are structured data that models complex relations between real-world\nentities. Heterophilous graphs, where linked nodes are prone to be with\ndifferent labels or dissimilar features, have recently attracted significant\nattention and found many applications. Meanwhile, increasing efforts have been\nmade to advance learning from heterophilous graphs. Although there exist\nsurveys on the relevant topic, they focus on heterophilous GNNs, which are only\nsub-topics of heterophilous graph learning. In this survey, we comprehensively\noverview existing works on learning from graphs with heterophily.First, we\ncollect over 180 publications and introduce the development of this field.\nThen, we systematically categorize existing methods based on a hierarchical\ntaxonomy including learning strategies, model architectures and practical\napplications. Finally, we discuss the primary challenges of existing studies\nand highlight promising avenues for future research.More publication details\nand corresponding open-source codes can be accessed and will be continuously\nupdated at our\nrepositories:https://github.com/gongchenghua/Awesome-Survey-Graphs-with-Heterophily.\n",
    "link": "http://arxiv.org/abs/2401.09769v1"
  },
  {
    "title": "CLIP Model for Images to Textual Prompts Based on Top-k Neighbors",
    "authors": "Xin Zhang, Xin Zhang, YeMing Cai, Tianzhi Jia",
    "abstract": "  Text-to-image synthesis, a subfield of multimodal generation, has gained\nsignificant attention in recent years. We propose a cost-effective approach for\nimage-to-prompt generation that leverages generative models to generate textual\nprompts without the need for large amounts of annotated data. We divide our\nmethod into two stages: online stage and offline stage. We use a combination of\nthe CLIP model and K-nearest neighbors (KNN) algorithm. The proposed system\nconsists of two main parts: an offline task and an online task. Our method owns\nthe highest metric 0.612 among these models, which is 0.013, 0.055, 0.011\nhigher than Clip, Clip + KNN(top 10) respectively.\n",
    "link": "http://arxiv.org/abs/2401.09763v1"
  },
  {
    "title": "Cooperative Tri-Point Model-Based Ground-to-Air Coverage Extension in\n  Beyond 5G Networks",
    "authors": "Ziwei Cai, Min Sheng, Junju Liu, Chenxi Zhao, Jiandong Li",
    "abstract": "  The utilization of existing terrestrial infrastructures to provide coverage\nfor aerial users is a potentially low-cost solution. However, the already\ndeployed terrestrial base stations (TBSs) result in weak ground-to-air (G2A)\ncoverage due to the down-tilted antennas. Furthermore, achieving optimal\ncoverage across the entire airspace through antenna adjustment is challenging\ndue to the complex signal coverage requirements in three-dimensional space,\nespecially in the vertical direction. In this paper, we propose a cooperative\ntri-point (CoTP) model-based method that utilizes cooperative beams to enhance\nthe G2A coverage extension. To utilize existing TBSs for establishing effective\ncooperation, we prove that the cooperation among three TBSs can ensure G2A\ncoverage with a minimum coverage overlap, and design the CoTP model to analyze\nthe G2A coverage extension. Using the model, a cooperative coverage structure\nbased on Delaunay triangulation is designed to divide triangular prism-shaped\nsubspaces and corresponding TBS cooperation sets. To enable TBSs in the\ncooperation set to cover different height subspaces while maintaining ground\ncoverage, we design a cooperative beam generation algorithm to maximize the\ncoverage in the triangular prism-shaped airspace. The simulation results and\nfield trials demonstrate that the proposed method can efficiently enhance the\nG2A coverage extension while guaranteeing ground coverage.\n",
    "link": "http://arxiv.org/abs/2401.09757v1"
  },
  {
    "title": "Explaining Drift using Shapley Values",
    "authors": "Narayanan U. Edakunni, Utkarsh Tekriwal, Anukriti Jain",
    "abstract": "  Machine learning models often deteriorate in their performance when they are\nused to predict the outcomes over data on which they were not trained. These\nscenarios can often arise in real world when the distribution of data changes\ngradually or abruptly due to major events like a pandemic. There have been many\nattempts in machine learning research to come up with techniques that are\nresilient to such Concept drifts. However, there is no principled framework to\nidentify the drivers behind the drift in model performance. In this paper, we\npropose a novel framework - DBShap that uses Shapley values to identify the\nmain contributors of the drift and quantify their respective contributions. The\nproposed framework not only quantifies the importance of individual features in\ndriving the drift but also includes the change in the underlying relation\nbetween the input and output as a possible driver. The explanation provided by\nDBShap can be used to understand the root cause behind the drift and use it to\nmake the model resilient to the drift.\n",
    "link": "http://arxiv.org/abs/2401.09756v1"
  },
  {
    "title": "Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- A Comprehensive\n  Symbolic Regression Framework",
    "authors": "Tianhao Chen, Pengbo Xu, Haibiao Zheng",
    "abstract": "  In the field of scientific computing, many problem-solving approaches tend to\nfocus only on the process and final outcome, even in AI for science, there is a\nlack of deep multimodal information mining behind the data, missing a\nmultimodal framework akin to that in the image-text domain. In this paper, we\ntake Symbolic Regression(SR) as our focal point and, drawing inspiration from\nthe BLIP model in the image-text domain, propose a scientific computing\nmultimodal framework based on Function Images (Funcimg) and Operation Tree\nSequence (OTS), named Bootstrapping OTS-Funcimg Pre-training Model (Botfip). In\nSR experiments, we validate the advantages of Botfip in low-complexity SR\nproblems, showcasing its potential. As a MED framework, Botfip holds promise\nfor future applications in a broader range of scientific computing problems.\n",
    "link": "http://arxiv.org/abs/2401.09748v1"
  },
  {
    "title": "Parameter Selection for Analyzing Conversations with Autism Spectrum\n  Disorder",
    "authors": "Tahiya Chowdhury, Veronica Romero, Amanda Stent",
    "abstract": "  The diagnosis of autism spectrum disorder (ASD) is a complex, challenging\ntask as it depends on the analysis of interactional behaviors by psychologists\nrather than the use of biochemical diagnostics. In this paper, we present a\nmodeling approach to ASD diagnosis by analyzing acoustic/prosodic and\nlinguistic features extracted from diagnostic conversations between a\npsychologist and children who either are typically developing (TD) or have ASD.\nWe compare the contributions of different features across a range of\nconversation tasks. We focus on finding a minimal set of parameters that\ncharacterize conversational behaviors of children with ASD. Because ASD is\ndiagnosed through conversational interaction, in addition to analyzing the\nbehavior of the children, we also investigate whether the psychologist's\nconversational behaviors vary across diagnostic groups. Our results can\nfacilitate fine-grained analysis of conversation data for children with ASD to\nsupport diagnosis and intervention.\n",
    "link": "http://arxiv.org/abs/2401.09717v1"
  },
  {
    "title": "HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain\n  Generalization",
    "authors": "Guanglin Zhou, Zhongyi Han, Shiming Chen, Biwei Huang, Liming Zhu, Tongliang Liu, Lina Yao, Kun Zhang",
    "abstract": "  Domain Generalization (DG) endeavors to create machine learning models that\nexcel in unseen scenarios by learning invariant features. In DG, the prevalent\npractice of constraining models to a fixed structure or uniform\nparameterization to encapsulate invariant features can inadvertently blend\nspecific aspects. Such an approach struggles with nuanced differentiation of\ninter-domain variations and may exhibit bias towards certain domains, hindering\nthe precise learning of domain-invariant features. Recognizing this, we\nintroduce a novel method designed to supplement the model with domain-level and\ntask-specific characteristics. This approach aims to guide the model in more\neffectively separating invariant features from specific characteristics,\nthereby boosting the generalization. Building on the emerging trend of visual\nprompts in the DG paradigm, our work introduces the novel \\textbf{H}ierarchical\n\\textbf{C}ontrastive \\textbf{V}isual \\textbf{P}rompt (HCVP) methodology. This\nrepresents a significant advancement in the field, setting itself apart with a\nunique generative approach to prompts, alongside an explicit model structure\nand specialized loss functions. Differing from traditional visual prompts that\nare often shared across entire datasets, HCVP utilizes a hierarchical prompt\ngeneration network enhanced by prompt contrastive learning. These generative\nprompts are instance-dependent, catering to the unique characteristics inherent\nto different domains and tasks. Additionally, we devise a prompt modulation\nnetwork that serves as a bridge, effectively incorporating the generated visual\nprompts into the vision transformer backbone. Experiments conducted on five DG\ndatasets demonstrate the effectiveness of HCVP, outperforming both established\nDG algorithms and adaptation protocols.\n",
    "link": "http://arxiv.org/abs/2401.09716v1"
  },
  {
    "title": "Curriculum Recommendations Using Transformer Base Model with InfoNCE\n  Loss And Language Switching Method",
    "authors": "Xiaonan Xu, Bin Yuan, Yongyao Mo, Tianbo Song, Shulin Li",
    "abstract": "  The Curriculum Recommendations paradigm is dedicated to fostering learning\nequality within the ever-evolving realms of educational technology and\ncurriculum development. In acknowledging the inherent obstacles posed by\nexisting methodologies, such as content conflicts and disruptions from language\ntranslation, this paradigm aims to confront and overcome these challenges.\nNotably, it addresses content conflicts and disruptions introduced by language\ntranslation, hindrances that can impede the creation of an all-encompassing and\npersonalized learning experience. The paradigm's objective is to cultivate an\neducational environment that not only embraces diversity but also customizes\nlearning experiences to suit the distinct needs of each learner. To overcome\nthese challenges, our approach builds upon notable contributions in curriculum\ndevelopment and personalized learning, introducing three key innovations. These\ninclude the integration of Transformer Base Model to enhance computational\nefficiency, the implementation of InfoNCE Loss for accurate content-topic\nmatching, and the adoption of a language switching strategy to alleviate\ntranslation-related ambiguities. Together, these innovations aim to\ncollectively tackle inherent challenges and contribute to forging a more\nequitable and effective learning journey for a diverse range of learners.\nCompetitive cross-validation scores underscore the efficacy of\nsentence-transformers/LaBSE, achieving 0.66314, showcasing our methodology's\neffectiveness in diverse linguistic nuances for content alignment prediction.\nIndex Terms-Curriculum Recommendation, Transformer model with InfoNCE Loss,\nLanguage Switching.\n",
    "link": "http://arxiv.org/abs/2401.09699v1"
  },
  {
    "title": "Should ChatGPT Write Your Breakup Text? Exploring the Role of AI in\n  Relationship Dissolution",
    "authors": "Yue Fu, Yixin Chen, Zelia Gomes Da Costa Lai, Alexis Hiniker",
    "abstract": "  Relationships are essential to our happiness and wellbeing. The dissolution\nof a relationship, the final stage of relationship's lifecycle and one of the\nmost stressful events in an individual's life, can have profound and\nlong-lasting impacts on people. With the breakup process increasingly\nfacilitated by computer-mediated communication (CMC), and the likely future\ninfluence of AI-mediated communication (AIMC) tools, we conducted a\nsemi-structured interview study with 21 participants. We aim to understand: 1)\nthe current role of technology in the breakup process, 2) the needs and support\nindividuals have during the process, and 3) how AI might address these needs.\nOur research shows that people have distinct needs at various stages of ending\na relationship. Presently, technology is used for information gathering and\ncommunity support, acting as a catalyst for breakups, enabling ghosting and\nblocking, and facilitating communication. Participants anticipate that AI could\naid in sense-making of their relationship leading up to the breakup, act as a\nmediator, assist in crafting appropriate wording, tones, and language during\nbreakup conversations, and support companionship, reflection, recovery, and\ngrowth after a breakup. Our findings also demonstrate an overlap between the\nbreakup process and the Transtheoretical Model (TTM) of behavior change.\nThrough the lens of TTM, we explore the potential support and affordances AI\ncould offer in breakups, including its benefits and the necessary precautions\nregarding AI's role in this sensitive process.\n",
    "link": "http://arxiv.org/abs/2401.09695v1"
  }
]