[
  {
    "title": "E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image\n  Translation",
    "authors": "Yifan Gong, Zheng Zhan, Qing Jin, Yanyu Li, Yerlan Idelbayev, Xian Liu, Andrey Zharkov, Kfir Aberman, Sergey Tulyakov, Yanzhi Wang, Jian Ren",
    "abstract": "  One highly promising direction for enabling flexible real-time on-device\nimage editing is utilizing data distillation by leveraging large-scale\ntext-to-image diffusion models, such as Stable Diffusion, to generate paired\ndatasets used for training generative adversarial networks (GANs). This\napproach notably alleviates the stringent requirements typically imposed by\nhigh-end commercial GPUs for performing image editing with diffusion models.\nHowever, unlike text-to-image diffusion models, each distilled GAN is\nspecialized for a specific image editing task, necessitating costly training\nefforts to obtain models for various concepts. In this work, we introduce and\naddress a novel research direction: can the process of distilling GANs from\ndiffusion models be made significantly more efficient? To achieve this goal, we\npropose a series of innovative techniques. First, we construct a base GAN model\nwith generalized features, adaptable to different concepts through fine-tuning,\neliminating the need for training from scratch. Second, we identify crucial\nlayers within the base GAN model and employ Low-Rank Adaptation (LoRA) with a\nsimple yet effective rank search process, rather than fine-tuning the entire\nbase model. Third, we investigate the minimal amount of data necessary for\nfine-tuning, further reducing the overall training time. Extensive experiments\nshow that we can efficiently empower GANs with the ability to perform real-time\nhigh-quality image editing on mobile devices with remarkable reduced training\ncost and storage for each concept.\n",
    "link": "http://arxiv.org/abs/2401.06127v1"
  },
  {
    "title": "Manipulating Feature Visualizations with Gradient Slingshots",
    "authors": "Dilyara Bareeva, Marina M. -C. H\u00f6hne, Alexander Warnecke, Lukas Pirch, Klaus-Robert M\u00fcller, Konrad Rieck, Kirill Bykov",
    "abstract": "  Deep Neural Networks (DNNs) are capable of learning complex and versatile\nrepresentations, however, the semantic nature of the learned concepts remains\nunknown. A common method used to explain the concepts learned by DNNs is\nActivation Maximization (AM), which generates a synthetic input signal that\nmaximally activates a particular neuron in the network. In this paper, we\ninvestigate the vulnerability of this approach to adversarial model\nmanipulations and introduce a novel method for manipulating feature\nvisualization without altering the model architecture or significantly\nimpacting the model's decision-making process. We evaluate the effectiveness of\nour method on several neural network models and demonstrate its capabilities to\nhide the functionality of specific neurons by masking the original explanations\nof neurons with chosen target explanations during model auditing. As a remedy,\nwe propose a protective measure against such manipulations and provide\nquantitative evidence which substantiates our findings.\n",
    "link": "http://arxiv.org/abs/2401.06122v1"
  },
  {
    "title": "Patchscope: A Unifying Framework for Inspecting Hidden Representations\n  of Language Models",
    "authors": "Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, Mor Geva",
    "abstract": "  Inspecting the information encoded in hidden representations of large\nlanguage models (LLMs) can explain models' behavior and verify their alignment\nwith human values. Given the capabilities of LLMs in generating\nhuman-understandable text, we propose leveraging the model itself to explain\nits internal representations in natural language. We introduce a framework\ncalled Patchscopes and show how it can be used to answer a wide range of\nresearch questions about an LLM's computation. We show that prior\ninterpretability methods based on projecting representations into the\nvocabulary space and intervening on the LLM computation, can be viewed as\nspecial instances of this framework. Moreover, several of their shortcomings\nsuch as failure in inspecting early layers or lack of expressivity can be\nmitigated by a Patchscope. Beyond unifying prior inspection techniques,\nPatchscopes also opens up new possibilities such as using a more capable model\nto explain the representations of a smaller model, and unlocks new applications\nsuch as self-correction in multi-hop reasoning.\n",
    "link": "http://arxiv.org/abs/2401.06102v1"
  },
  {
    "title": "Autocompletion of Chief Complaints in the Electronic Health Records\n  using Large Language Models",
    "authors": "K M Sajjadul Islam, Ayesha Siddika Nipu, Praveen Madiraju, Priya Deshpande",
    "abstract": "  The Chief Complaint (CC) is a crucial component of a patient's medical record\nas it describes the main reason or concern for seeking medical care. It\nprovides critical information for healthcare providers to make informed\ndecisions about patient care. However, documenting CCs can be time-consuming\nfor healthcare providers, especially in busy emergency departments. To address\nthis issue, an autocompletion tool that suggests accurate and well-formatted\nphrases or sentences for clinical notes can be a valuable resource for triage\nnurses. In this study, we utilized text generation techniques to develop\nmachine learning models using CC data. In our proposed work, we train a Long\nShort-Term Memory (LSTM) model and fine-tune three different variants of\nBiomedical Generative Pretrained Transformers (BioGPT), namely\nmicrosoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA.\nAdditionally, we tune a prompt by incorporating exemplar CC sentences,\nutilizing the OpenAI API of GPT-4. We evaluate the models' performance based on\nthe perplexity score, modified BERTScore, and cosine similarity score. The\nresults show that BioGPT-Large exhibits superior performance compared to the\nother models. It consistently achieves a remarkably low perplexity score of\n1.65 when generating CC, whereas the baseline LSTM model achieves the best\nperplexity score of 170. Further, we evaluate and assess the proposed models'\nperformance and the outcome of GPT-4.0. Our study demonstrates that utilizing\nLLMs such as BioGPT, leads to the development of an effective autocompletion\ntool for generating CC documentation in healthcare settings.\n",
    "link": "http://arxiv.org/abs/2401.06088v1"
  },
  {
    "title": "XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an\n  Agent-Based Model of a Sports Betting Exchange",
    "authors": "Chawin Terawong, Dave Cliff",
    "abstract": "  We present first results from the use of XGBoost, a highly effective machine\nlearning (ML) method, within the Bristol Betting Exchange (BBE), an open-source\nagent-based model (ABM) designed to simulate a contemporary sports-betting\nexchange with in-play betting during track-racing events such as horse races.\nWe use the BBE ABM and its array of minimally-simple bettor-agents as a\nsynthetic data generator which feeds into our XGBoost ML system, with the\nintention that XGBoost discovers profitable dynamic betting strategies by\nlearning from the more profitable bets made by the BBE bettor-agents. After\nthis XGBoost training, which results in one or more decision trees, a\nbettor-agent with a betting strategy determined by the XGBoost-learned decision\ntree(s) is added to the BBE ABM and made to bet on a sequence of races under\nvarious conditions and betting-market scenarios, with profitability serving as\nthe primary metric of comparison and evaluation. Our initial findings presented\nhere show that XGBoost trained in this way can indeed learn profitable betting\nstrategies, and can generalise to learn strategies that outperform each of the\nset of strategies used for creation of the training data. To foster further\nresearch and enhancements, the complete version of our extended BBE, including\nthe XGBoost integration, has been made freely available as an open-source\nrelease on GitHub.\n",
    "link": "http://arxiv.org/abs/2401.06086v1"
  },
  {
    "title": "Secrets of RLHF in Large Language Models Part II: Reward Modeling",
    "authors": "Binghai Wang, Rui Zheng, Lu Chen, Yan Liu, Shihan Dou, Caishuang Huang, Wei Shen, Senjie Jin, Enyu Zhou, Chenyu Shi, Songyang Gao, Nuo Xu, Yuhao Zhou, Xiaoran Fan, Zhiheng Xi, Jun Zhao, Xiao Wang, Tao Ji, Hang Yan, Lixing Shen, Zhan Chen, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang",
    "abstract": "  Reinforcement Learning from Human Feedback (RLHF) has become a crucial\ntechnology for aligning language models with human values and intentions,\nenabling models to produce more helpful and harmless responses. Reward models\nare trained as proxies for human preferences to drive reinforcement learning\noptimization. While reward models are often considered central to achieving\nhigh performance, they face the following challenges in practical applications:\n(1) Incorrect and ambiguous preference pairs in the dataset may hinder the\nreward model from accurately capturing human intent. (2) Reward models trained\non data from a specific distribution often struggle to generalize to examples\noutside that distribution and are not suitable for iterative RLHF training.\n  In this report, we attempt to address these two issues. (1) From a data\nperspective, we propose a method to measure the strength of preferences within\nthe data, based on a voting mechanism of multiple reward models. Experimental\nresults confirm that data with varying preference strengths have different\nimpacts on reward model performance. We introduce a series of novel methods to\nmitigate the influence of incorrect and ambiguous preferences in the dataset\nand fully leverage high-quality preference data. (2) From an algorithmic\nstandpoint, we introduce contrastive learning to enhance the ability of reward\nmodels to distinguish between chosen and rejected responses, thereby improving\nmodel generalization. Furthermore, we employ meta-learning to enable the reward\nmodel to maintain the ability to differentiate subtle differences in\nout-of-distribution samples, and this approach can be utilized for iterative\nRLHF optimization.\n",
    "link": "http://arxiv.org/abs/2401.06080v1"
  },
  {
    "title": "Chain of History: Learning and Forecasting with LLMs for Temporal\n  Knowledge Graph Completion",
    "authors": "Ruilin Luo, Tianle Gu, Haoling Li, Junzhe Li, Zicheng Lin, Jiayi Li, Yujiu Yang",
    "abstract": "  Temporal Knowledge Graph Completion (TKGC) is a challenging task of\npredicting missing event links at future timestamps by leveraging established\ntemporal structural knowledge. Given the formidable generative capabilities\ninherent in LLMs (LLMs), this paper proposes a novel approach to conceptualize\ntemporal link prediction as an event generation task within the context of a\nhistorical event chain. We employ efficient fine-tuning methods to make LLMs\nadapt to specific graph textual information and patterns discovered in temporal\ntimelines. Furthermore, we introduce structure-based historical data\naugmentation and the integration of reverse knowledge to emphasize LLMs'\nawareness of structural information, thereby enhancing their reasoning\ncapabilities. We conduct thorough experiments on multiple widely used datasets\nand find that our fine-tuned model outperforms existing embedding-based models\non multiple metrics, achieving SOTA results. We also carry out sufficient\nablation experiments to explore the key influencing factors when LLMs perform\nstructured temporal knowledge inference tasks.\n",
    "link": "http://arxiv.org/abs/2401.06072v1"
  },
  {
    "title": "Investigating Data Contamination for Pre-training Language Models",
    "authors": "Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, Sanmi Koyejo",
    "abstract": "  Language models pre-trained on web-scale corpora demonstrate impressive\ncapabilities on diverse downstream tasks. However, there is increasing concern\nwhether such capabilities might arise from evaluation datasets being included\nin the pre-training corpus -- a phenomenon known as \\textit{data contamination}\n-- in a manner that artificially increases performance. There has been little\nunderstanding of how this potential contamination might influence LMs'\nperformance on downstream tasks. In this paper, we explore the impact of data\ncontamination at the pre-training stage by pre-training a series of GPT-2\nmodels \\textit{from scratch}. We highlight the effect of both text\ncontamination (\\textit{i.e.}\\ input text of the evaluation samples) and\nground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and\nthe desired outputs) from evaluation data. We also investigate the effects of\nrepeating contamination for various downstream tasks. Additionally, we examine\nthe prevailing n-gram-based definitions of contamination within current LLM\nreports, pinpointing their limitations and inadequacy. Our findings offer new\ninsights into data contamination's effects on language model capabilities and\nunderscore the need for independent, comprehensive contamination assessments in\nLLM studies.\n",
    "link": "http://arxiv.org/abs/2401.06059v1"
  },
  {
    "title": "On the Power of Graph Neural Networks and Feature Augmentation\n  Strategies to Classify Social Networks",
    "authors": "Walid Guettala, L\u00e1szl\u00f3 Guly\u00e1s",
    "abstract": "  This paper studies four Graph Neural Network architectures (GNNs) for a graph\nclassification task on a synthetic dataset created using classic generative\nmodels of Network Science. Since the synthetic networks do not contain (node or\nedge) features, five different augmentation strategies (artificial feature\ntypes) are applied to nodes. All combinations of the 4 GNNs (GCN with\nHierarchical and Global aggregation, GIN and GATv2) and the 5 feature types\n(constant 1, noise, degree, normalized degree and ID -- a vector of the number\nof cycles of various lengths) are studied and their performances compared as a\nfunction of the hidden dimension of artificial neural networks used in the\nGNNs. The generalisation ability of these models is also analysed using a\nsecond synthetic network dataset (containing networks of different sizes).Our\nresults point towards the balanced importance of the computational power of the\nGNN architecture and the the information level provided by the artificial\nfeatures. GNN architectures with higher computational power, like GIN and\nGATv2, perform well for most augmentation strategies. On the other hand,\nartificial features with higher information content, like ID or degree, not\nonly consistently outperform other augmentation strategies, but can also help\nGNN architectures with lower computational power to achieve good performance.\n",
    "link": "http://arxiv.org/abs/2401.06048v1"
  },
  {
    "title": "Surgical-DINO: Adapter Learning of Foundation Model for Depth Estimation\n  in Endoscopic Surgery",
    "authors": "Cui Beilei, Islam Mobarakol, Bai Long, Ren Hongliang",
    "abstract": "  Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction,\nsurgical navigation and augmented reality visualization. Although the\nfoundation model exhibits outstanding performance in many vision tasks,\nincluding depth estimation (e.g., DINOv2), recent works observed its\nlimitations in medical and surgical domain-specific applications. This work\npresents a low-ranked adaptation (LoRA) of the foundation model for surgical\ndepth estimation. Methods: We design a foundation model-based depth estimation\nmethod, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for\ndepth estimation in endoscopic surgery. We build LoRA layers and integrate them\ninto DINO to adapt with surgery-specific domain knowledge instead of\nconventional fine-tuning. During training, we freeze the DINO image encoder,\nwhich shows excellent visual representation capacity, and only optimize the\nLoRA layers and depth decoder to integrate features from the surgical scene.\nResults: Our model is extensively validated on a MICCAI challenge dataset of\nSCARED, which is collected from da Vinci Xi endoscope surgery. We empirically\nshow that Surgical-DINO significantly outperforms all the state-of-the-art\nmodels in endoscopic depth estimation tasks. The analysis with ablation studies\nhas shown evidence of the remarkable effect of our LoRA layers and adaptation.\nConclusion: Surgical-DINO shed some light on the successful adaptation of the\nfoundation models into the surgical domain for depth estimation. There is clear\nevidence in the results that zero-shot prediction on pre-trained weights in\ncomputer vision datasets or naive fine-tuning is not sufficient to use the\nfoundation model in the surgical domain directly. Code is available at\nhttps://github.com/BeileiCui/SurgicalDINO.\n",
    "link": "http://arxiv.org/abs/2401.06013v1"
  },
  {
    "title": "How does the primate brain combine generative and discriminative\n  computations in vision?",
    "authors": "Benjamin Peters, James J. DiCarlo, Todd Gureckis, Ralf Haefner, Leyla Isik, Joshua Tenenbaum, Talia Konkle, Thomas Naselaris, Kimberly Stachenfeld, Zenna Tavares, Doris Tsao, Ilker Yildirim, Nikolaus Kriegeskorte",
    "abstract": "  Vision is widely understood as an inference problem. However, two contrasting\nconceptions of the inference process have each been influential in research on\nbiological vision as well as the engineering of machine vision. The first\nemphasizes bottom-up signal flow, describing vision as a largely feedforward,\ndiscriminative inference process that filters and transforms the visual\ninformation to remove irrelevant variation and represent behaviorally relevant\ninformation in a format suitable for downstream functions of cognition and\nbehavioral control. In this conception, vision is driven by the sensory data,\nand perception is direct because the processing proceeds from the data to the\nlatent variables of interest. The notion of \"inference\" in this conception is\nthat of the engineering literature on neural networks, where feedforward\nconvolutional neural networks processing images are said to perform inference.\nThe alternative conception is that of vision as an inference process in\nHelmholtz's sense, where the sensory evidence is evaluated in the context of a\ngenerative model of the causal processes giving rise to it. In this conception,\nvision inverts a generative model through an interrogation of the evidence in a\nprocess often thought to involve top-down predictions of sensory data to\nevaluate the likelihood of alternative hypotheses. The authors include\nscientists rooted in roughly equal numbers in each of the conceptions and\nmotivated to overcome what might be a false dichotomy between them and engage\nthe other perspective in the realm of theory and experiment. The primate brain\nemploys an unknown algorithm that may combine the advantages of both\nconceptions. We explain and clarify the terminology, review the key empirical\nevidence, and propose an empirical research program that transcends the\ndichotomy and sets the stage for revealing the mysterious hybrid algorithm of\nprimate vision.\n",
    "link": "http://arxiv.org/abs/2401.06005v1"
  },
  {
    "title": "Combating Adversarial Attacks with Multi-Agent Debate",
    "authors": "Steffi Chern, Zhen Fan, Andy Liu",
    "abstract": "  While state-of-the-art language models have achieved impressive results, they\nremain susceptible to inference-time adversarial attacks, such as adversarial\nprompts generated by red teams arXiv:2209.07858. One approach proposed to\nimprove the general quality of language model generations is multi-agent\ndebate, where language models self-evaluate through discussion and feedback\narXiv:2305.14325. We implement multi-agent debate between current\nstate-of-the-art language models and evaluate models' susceptibility to red\nteam attacks in both single- and multi-agent settings. We find that multi-agent\ndebate can reduce model toxicity when jailbroken or less capable models are\nforced to debate with non-jailbroken or more capable models. We also find\nmarginal improvements through the general usage of multi-agent interactions. We\nfurther perform adversarial prompt content classification via embedding\nclustering, and analyze the susceptibility of different models to different\ntypes of attack topics.\n",
    "link": "http://arxiv.org/abs/2401.05998v1"
  },
  {
    "title": "End-to-end Learnable Clustering for Intent Learning in Recommendation",
    "authors": "Yue Liu, Shihao Zhu, Jun Xia, Yingwei Ma, Jian Ma, Wenliang Zhong, Guannan Zhang, Kejun Zhang, Xinwang Liu",
    "abstract": "  Mining users' intents plays a crucial role in sequential recommendation. The\nrecent approach, ICLRec, was introduced to extract underlying users' intents\nusing contrastive learning and clustering. While it has shown effectiveness,\nthe existing method suffers from complex and cumbersome alternating\noptimization, leading to two main issues. Firstly, the separation of\nrepresentation learning and clustering optimization within a generalized\nexpectation maximization (EM) framework often results in sub-optimal\nperformance. Secondly, performing clustering on the entire dataset hampers\nscalability for large-scale industry data. To address these challenges, we\npropose a novel intent learning method called \\underline{ELCRec}, which\nintegrates representation learning into an \\underline{E}nd-to-end\n\\underline{L}earnable \\underline{C}lustering framework for\n\\underline{Rec}ommendation. Specifically, we encode users' behavior sequences\nand initialize the cluster centers as learnable network parameters.\nAdditionally, we design a clustering loss that guides the networks to\ndifferentiate between different cluster centers and pull similar samples\ntowards their respective cluster centers. This allows simultaneous optimization\nof recommendation and clustering using mini-batch data. Moreover, we leverage\nthe learned cluster centers as self-supervision signals for representation\nlearning, resulting in further enhancement of recommendation performance.\nExtensive experiments conducted on open benchmarks and industry data validate\nthe superiority, effectiveness, and efficiency of our proposed ELCRec method.\nCode is available at: https://github.com/yueliu1999/ELCRec.\n",
    "link": "http://arxiv.org/abs/2401.05975v1"
  },
  {
    "title": "Spatial-Aware Deep Reinforcement Learning for the Traveling Officer\n  Problem",
    "authors": "Niklas Strau\u00df, Matthias Schubert",
    "abstract": "  The traveling officer problem (TOP) is a challenging stochastic optimization\ntask. In this problem, a parking officer is guided through a city equipped with\nparking sensors to fine as many parking offenders as possible. A major\nchallenge in TOP is the dynamic nature of parking offenses, which randomly\nappear and disappear after some time, regardless of whether they have been\nfined. Thus, solutions need to dynamically adjust to currently fineable parking\noffenses while also planning ahead to increase the likelihood that the officer\narrives during the offense taking place. Though various solutions exist, these\nmethods often struggle to take the implications of actions on the ability to\nfine future parking violations into account. This paper proposes SATOP, a novel\nspatial-aware deep reinforcement learning approach for TOP. Our novel state\nencoder creates a representation of each action, leveraging the spatial\nrelationships between parking spots, the agent, and the action. Furthermore, we\npropose a novel message-passing module for learning future inter-action\ncorrelations in the given environment. Thus, the agent can estimate the\npotential to fine further parking violations after executing an action. We\nevaluate our method using an environment based on real-world data from\nMelbourne. Our results show that SATOP consistently outperforms\nstate-of-the-art TOP agents and is able to fine up to 22% more parking\noffenses.\n",
    "link": "http://arxiv.org/abs/2401.05969v1"
  },
  {
    "title": "An attempt to generate new bridge types from latent space of PixelCNN",
    "authors": "Hongjun Zhang",
    "abstract": "  Try to generate new bridge types using generative artificial intelligence\ntechnology. Using symmetric structured image dataset of three-span beam bridge,\narch bridge, cable-stayed bridge and suspension bridge , based on Python\nprogramming language, TensorFlow and Keras deep learning platform framework ,\nPixelCNN is constructed and trained. The model can capture the statistical\nstructure of the images and calculate the probability distribution of the next\npixel when the previous pixels are given. From the obtained latent space\nsampling, new bridge types different from the training dataset can be\ngenerated. PixelCNN can organically combine different structural components on\nthe basis of human original bridge types, creating new bridge types that have a\ncertain degree of human original ability. Autoregressive models cannot\nunderstand the meaning of the sequence, while multimodal models combine\nregression and autoregressive models to understand the sequence. Multimodal\nmodels should be the way to achieve artificial general intelligence in the\nfuture.\n",
    "link": "http://arxiv.org/abs/2401.05964v1"
  },
  {
    "title": "Machine Learning Insides OptVerse AI Solver: Design Principles and\n  Applications",
    "authors": "Xijun Li, Fangzhou Zhu, Hui-Ling Zhen, Weilin Luo, Meng Lu, Yimin Huang, Zhenan Fan, Zirui Zhou, Yufei Kuang, Zhihai Wang, Zijie Geng, Yang Li, Haoyang Liu, Zhiwu An, Muming Yang, Jianshu Li, Jie Wang, Junchi Yan, Defeng Sun, Tao Zhong, Yong Zhang, Jia Zeng, Mingxuan Yuan, Jianye Hao, Jun Yao, Kun Mao",
    "abstract": "  In an era of digital ubiquity, efficient resource management and\ndecision-making are paramount across numerous industries. To this end, we\npresent a comprehensive study on the integration of machine learning (ML)\ntechniques into Huawei Cloud's OptVerse AI Solver, which aims to mitigate the\nscarcity of real-world mathematical programming instances, and to surpass the\ncapabilities of traditional optimization techniques. We showcase our methods\nfor generating complex SAT and MILP instances utilizing generative models that\nmirror multifaceted structures of real-world problem. Furthermore, we introduce\na training framework leveraging augmentation policies to maintain solvers'\nutility in dynamic environments. Besides the data generation and augmentation,\nour proposed approaches also include novel ML-driven policies for personalized\nsolver strategies, with an emphasis on applications like graph convolutional\nnetworks for initial basis selection and reinforcement learning for advanced\npresolving and cut selection. Additionally, we detail the incorporation of\nstate-of-the-art parameter tuning algorithms which markedly elevate solver\nperformance. Compared with traditional solvers such as Gurobi and SCIP, our\nML-augmented OptVerse AI Solver demonstrates superior speed and precision\nacross both established benchmarks and real-world scenarios, reinforcing the\npractical imperative and effectiveness of machine learning techniques in\nmathematical programming solvers.\n",
    "link": "http://arxiv.org/abs/2401.05960v1"
  },
  {
    "title": "Universal Vulnerabilities in Large Language Models: In-context Learning\n  Backdoor Attacks",
    "authors": "Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Jinming Wen",
    "abstract": "  In-context learning, a paradigm bridging the gap between pre-training and\nfine-tuning, has demonstrated high efficacy in several NLP tasks, especially in\nfew-shot settings. Unlike traditional fine-tuning methods, in-context learning\nadapts pre-trained models to unseen tasks without updating any parameters.\nDespite being widely applied, in-context learning is vulnerable to malicious\nattacks. In this work, we raise security concerns regarding this paradigm. Our\nstudies demonstrate that an attacker can manipulate the behavior of large\nlanguage models by poisoning the demonstration context, without the need for\nfine-tuning the model. Specifically, we have designed a new backdoor attack\nmethod, named ICLAttack, to target large language models based on in-context\nlearning. Our method encompasses two types of attacks: poisoning demonstration\nexamples and poisoning prompts, which can make models behave in accordance with\npredefined intentions. ICLAttack does not require additional fine-tuning to\nimplant a backdoor, thus preserving the model's generality. Furthermore, the\npoisoned examples are correctly labeled, enhancing the natural stealth of our\nattack method. Extensive experimental results across several language models,\nranging in size from 1.3B to 40B parameters, demonstrate the effectiveness of\nour attack method, exemplified by a high average attack success rate of 95.0%\nacross the three datasets on OPT models. Our findings highlight the\nvulnerabilities of language models, and we hope this work will raise awareness\nof the possible security threats associated with in-context learning.\n",
    "link": "http://arxiv.org/abs/2401.05949v1"
  },
  {
    "title": "Learning Cognitive Maps from Transformer Representations for Efficient\n  Planning in Partially Observed Environments",
    "authors": "Antoine Dedieu, Wolfgang Lehrach, Guangyao Zhou, Dileep George, Miguel L\u00e1zaro-Gredilla",
    "abstract": "  Despite their stellar performance on a wide range of tasks, including\nin-context tasks only revealed during inference, vanilla transformers and\nvariants trained for next-token predictions (a) do not learn an explicit world\nmodel of their environment which can be flexibly queried and (b) cannot be used\nfor planning or navigation. In this paper, we consider partially observed\nenvironments (POEs), where an agent receives perceptually aliased observations\nas it navigates, which makes path planning hard. We introduce a transformer\nwith (multiple) discrete bottleneck(s), TDB, whose latent codes learn a\ncompressed representation of the history of observations and actions. After\ntraining a TDB to predict the future observation(s) given the history, we\nextract interpretable cognitive maps of the environment from its active\nbottleneck(s) indices. These maps are then paired with an external solver to\nsolve (constrained) path planning problems. First, we show that a TDB trained\non POEs (a) retains the near perfect predictive performance of a vanilla\ntransformer or an LSTM while (b) solving shortest path problems exponentially\nfaster. Second, a TDB extracts interpretable representations from text\ndatasets, while reaching higher in-context accuracy than vanilla sequence\nmodels. Finally, in new POEs, a TDB (a) reaches near-perfect in-context\naccuracy, (b) learns accurate in-context cognitive maps (c) solves in-context\npath planning problems.\n",
    "link": "http://arxiv.org/abs/2401.05946v1"
  },
  {
    "title": "Mutation-based Consistency Testing for Evaluating the Code Understanding\n  Capability of LLMs",
    "authors": "Ziyu Li, Donghwan Shin",
    "abstract": "  Large Language Models (LLMs) have shown remarkable capabilities in processing\nboth natural and programming languages, which have enabled various applications\nin software engineering, such as requirement engineering, code generation, and\nsoftware testing. However, existing code generation benchmarks do not\nnecessarily assess the code understanding performance of LLMs, especially for\nthe subtle inconsistencies that may arise between code and its semantics\ndescribed in natural language.\n  In this paper, we propose a novel method to systematically assess the code\nunderstanding performance of LLMs, particularly focusing on subtle differences\nbetween code and its descriptions, by introducing code mutations to existing\ncode generation datasets. Code mutations are small changes that alter the\nsemantics of the original code, creating a mismatch with the natural language\ndescription. We apply different types of code mutations, such as operator\nreplacement and statement deletion, to generate inconsistent code-description\npairs. We then use these pairs to test the ability of LLMs to correctly detect\nthe inconsistencies.\n  We propose a new LLM testing method, called Mutation-based Consistency\nTesting (MCT), and conduct a case study on the two popular LLMs, GPT-3.5 and\nGPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which\nconsists of six programming languages (Python, C++, Java, Go, JavaScript, and\nRust). We compare the performance of the LLMs across different types of code\nmutations and programming languages and analyze the results. We find that the\nLLMs show significant variation in their code understanding performance and\nthat they have different strengths and weaknesses depending on the mutation\ntype and language.\n",
    "link": "http://arxiv.org/abs/2401.05940v1"
  },
  {
    "title": "DREQ: Document Re-Ranking Using Entity-based Query Understanding",
    "authors": "Shubham Chatterjee, Iain Mackie, Jeff Dalton",
    "abstract": "  While entity-oriented neural IR models have advanced significantly, they\noften overlook a key nuance: the varying degrees of influence individual\nentities within a document have on its overall relevance. Addressing this gap,\nwe present DREQ, an entity-oriented dense document re-ranking model. Uniquely,\nwe emphasize the query-relevant entities within a document's representation\nwhile simultaneously attenuating the less relevant ones, thus obtaining a\nquery-specific entity-centric document representation. We then combine this\nentity-centric document representation with the text-centric representation of\nthe document to obtain a \"hybrid\" representation of the document. We learn a\nrelevance score for the document using this hybrid representation. Using four\nlarge-scale benchmarks, we show that DREQ outperforms state-of-the-art neural\nand non-neural re-ranking methods, highlighting the effectiveness of our\nentity-oriented representation approach.\n",
    "link": "http://arxiv.org/abs/2401.05939v1"
  },
  {
    "title": "DiffDA: a diffusion model for weather-scale data assimilation",
    "authors": "Langwen Huang, Lukas Gianinazzi, Yuejiang Yu, Peter D. Dueben, Torsten Hoefler",
    "abstract": "  The generation of initial conditions via accurate data assimilation is\ncrucial for reliable weather forecasting and climate modeling. We propose the\nDiffDA as a machine learning based data assimilation method capable of\nassimilating atmospheric variables using predicted states and sparse\nobservations. We adapt the pretrained GraphCast weather forecast model as a\ndenoising diffusion model. Our method applies two-phase conditioning: on the\npredicted state during both training and inference, and on sparse observations\nduring inference only. As a byproduct, this strategy also enables the\npost-processing of predictions into the future, for which no observations are\navailable.Through experiments based on a reanalysis dataset, we have verified\nthat our method can produce assimilated global atmospheric data consistent with\nobservations at 0.25degree resolution. The experiments also show that the\ninitial conditions that are generated via our approach can be used for forecast\nmodels with a loss of lead time of at most 24 hours when compared to initial\nconditions of state-of-the-art data assimilation suites. This enables to apply\nthe method to real world applications such as the creation of reanalysis\ndatasets with autoregressive data assimilation.\n",
    "link": "http://arxiv.org/abs/2401.05932v1"
  },
  {
    "title": "SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully",
    "authors": "Jushi Kai, Tianhang Zhang, Hai Hu, Zhouhan Lin",
    "abstract": "  Large language models (LLMs) demonstrate great performance in text\ngeneration. However, LLMs are still suffering from hallucinations. In this\nwork, we propose an inference-time method, Self-Highlighted Hesitation (SH2),\nto help LLMs decode more truthfully. SH2 is based on a simple fact rooted in\ninformation theory that for an LLM, the tokens predicted with lower\nprobabilities are prone to be more informative than others. Our analysis shows\nthat the tokens assigned with lower probabilities by an LLM are more likely to\nbe closely related to factual information, such as nouns, proper nouns, and\nadjectives. Therefore, we propose to ''highlight'' the factual information by\nselecting the tokens with the lowest probabilities and concatenating them to\nthe original context, thus forcing the model to repeatedly read and hesitate on\nthese tokens before generation. During decoding, we also adopt contrastive\ndecoding to emphasize the difference in the output probabilities brought by the\nhesitation. Experimental results demonstrate that our SH2, requiring no\nadditional data or models, can effectively help LLMs elicit factual knowledge\nand distinguish hallucinated contexts. Significant and consistent improvements\nare achieved by SH2 for LLaMA-7b and LLaMA2-7b on multiple hallucination tasks.\n",
    "link": "http://arxiv.org/abs/2401.05930v1"
  },
  {
    "title": "CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians",
    "authors": "Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan",
    "abstract": "  We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a\nmethod for compact 3D-consistent scene segmentation at fast rendering speed\nwith only RGB images input. Previous NeRF-based 3D segmentation methods have\nrelied on implicit or voxel neural scene representation and ray-marching volume\nrendering which are time consuming. Recent 3D Gaussian Splatting significantly\nimproves the rendering speed, however, existing Gaussians-based segmentation\nmethods(eg: Gaussian Grouping) fail to provide compact segmentation masks\nespecially in zero-shot segmentation, which is mainly caused by the lack of\nrobustness and compactness for straightforwardly assigning learnable parameters\nto each Gaussian when encountering inconsistent 2D machine-generated labels.\nOur method aims to achieve compact and reliable zero-shot scene segmentation\nswiftly by mapping fused spatial and semantically meaningful features for each\nGaussian point with a shallow decoding network. Specifically, our method\nfirstly optimizes Gaussian points' position, convariance and color attributes\nunder the supervision of RGB images. After Gaussian Locating, we distill\nmulti-scale DINO features extracted from images through unprojection to each\nGaussian, which is then incorporated with spatial features from the fast point\nfeatures processing network, i.e. RandLA-Net. Then the shallow decoding MLP is\napplied to the multi-scale fused features to obtain compact segmentation.\nExperimental results show that our model can perform high-quality zero-shot\nscene segmentation, as our model outperforms other segmentation methods on both\nsemantic and panoptic segmentation task, meanwhile consumes approximately only\n10% segmenting time compared to NeRF-based segmentation. Code and more results\nwill be available at https://David-Dou.github.io/CoSSegGaussians\n",
    "link": "http://arxiv.org/abs/2401.05925v1"
  },
  {
    "title": "How Teachers Can Use Large Language Models and Bloom's Taxonomy to\n  Create Educational Quizzes",
    "authors": "Sabina Elkins, Ekaterina Kochmar, Jackie C. K. Cheung, Iulian Serban",
    "abstract": "  Question generation (QG) is a natural language processing task with an\nabundance of potential benefits and use cases in the educational domain. In\norder for this potential to be realized, QG systems must be designed and\nvalidated with pedagogical needs in mind. However, little research has assessed\nor designed QG approaches with the input from real teachers or students. This\npaper applies a large language model-based QG approach where questions are\ngenerated with learning goals derived from Bloom's taxonomy. The automatically\ngenerated questions are used in multiple experiments designed to assess how\nteachers use them in practice. The results demonstrate that teachers prefer to\nwrite quizzes with automatically generated questions, and that such quizzes\nhave no loss in quality compared to handwritten versions. Further, several\nmetrics indicate that automatically generated questions can even improve the\nquality of the quizzes created, showing the promise for large scale use of QG\nin the classroom setting.\n",
    "link": "http://arxiv.org/abs/2401.05914v1"
  },
  {
    "title": "Binary Linear Tree Commitment-based Ownership Protection for Distributed\n  Machine Learning",
    "authors": "Tianxiu Xie, Keke Gai, Jing Yu, Liehuang Zhu",
    "abstract": "  Distributed machine learning enables parallel training of extensive datasets\nby delegating computing tasks across multiple workers. Despite the cost\nreduction benefits of distributed machine learning, the dissemination of final\nmodel weights often leads to potential conflicts over model ownership as\nworkers struggle to substantiate their involvement in the training computation.\nTo address the above ownership issues and prevent accidental failures and\nmalicious attacks, verifying the computational integrity and effectiveness of\nworkers becomes particularly crucial in distributed machine learning. In this\npaper, we proposed a novel binary linear tree commitment-based ownership\nprotection model to ensure computational integrity with limited overhead and\nconcise proof. Due to the frequent updates of parameters during training, our\ncommitment scheme introduces a maintainable tree structure to reduce the costs\nof updating proofs. Distinguished from SNARK-based verifiable computation, our\nmodel achieves efficient proof aggregation by leveraging inner product\narguments. Furthermore, proofs of model weights are watermarked by worker\nidentity keys to prevent commitments from being forged or duplicated. The\nperformance analysis and comparison with SNARK-based hash commitments validate\nthe efficacy of our model in preserving computational integrity within\ndistributed machine learning.\n",
    "link": "http://arxiv.org/abs/2401.05895v1"
  },
  {
    "title": "HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced\n  Diffusion Models",
    "authors": "Hanzhang Wang, Haoran Wang, Jinze Yang, Zhongrui Yu, Zeke Xie, Lei Tian, Xinyan Xiao, Junjun Jiang, Xianming Liu, Mingming Sun",
    "abstract": "  The goal of Arbitrary Style Transfer (AST) is injecting the artistic features\nof a style reference into a given image/video. Existing methods usually focus\non pursuing the balance between style and content, whereas ignoring the\nsignificant demand for flexible and customized stylization results and thereby\nlimiting their practical application. To address this critical issue, a novel\nAST approach namely HiCAST is proposed, which is capable of explicitly\ncustomizing the stylization results according to various source of semantic\nclues. In the specific, our model is constructed based on Latent Diffusion\nModel (LDM) and elaborately designed to absorb content and style instance as\nconditions of LDM. It is characterized by introducing of \\textit{Style\nAdapter}, which allows user to flexibly manipulate the output results by\naligning multi-level style information and intrinsic knowledge in LDM. Lastly,\nwe further extend our model to perform video AST. A novel learning objective is\nleveraged for video diffusion model training, which significantly improve\ncross-frame temporal consistency in the premise of maintaining stylization\nstrength. Qualitative and quantitative comparisons as well as comprehensive\nuser studies demonstrate that our HiCAST outperforms the existing SoTA methods\nin generating visually plausible stylization results.\n",
    "link": "http://arxiv.org/abs/2401.05870v1"
  },
  {
    "title": "Inferring Intentions to Speak Using Accelerometer Data In-the-Wild",
    "authors": "Litian Li, Jord Molhoek, Jing Zhou",
    "abstract": "  Humans have good natural intuition to recognize when another person has\nsomething to say. It would be interesting if an AI can also recognize\nintentions to speak. Especially in scenarios when an AI is guiding a group\ndiscussion, this can be a useful skill. This work studies the inference of\nsuccessful and unsuccessful intentions to speak from accelerometer data. This\nis chosen because it is privacy-preserving and feasible for in-the-wild\nsettings since it can be placed in a smart badge. Data from a real-life social\nnetworking event is used to train a machine-learning model that aims to infer\nintentions to speak. A subset of unsuccessful intention-to-speak cases in the\ndata is annotated. The model is trained on the successful intentions to speak\nand evaluated on both the successful and unsuccessful cases. In conclusion,\nthere is useful information in accelerometer data, but not enough to reliably\ncapture intentions to speak. For example, posture shifts are correlated with\nintentions to speak, but people also often shift posture without having an\nintention to speak, or have an intention to speak without shifting their\nposture. More modalities are likely needed to reliably infer intentions to\nspeak.\n",
    "link": "http://arxiv.org/abs/2401.05849v1"
  },
  {
    "title": "Pushing the Pareto front of band gap and permittivity: ML-guided search\n  for dielectric materials",
    "authors": "Janosh Riebesell, T. Wesley Surta, Rhys Goodall, Michael Gaultois, Alpha A Lee",
    "abstract": "  Materials with high-dielectric constant easily polarize under external\nelectric fields, allowing them to perform essential functions in many modern\nelectronic devices. Their practical utility is determined by two conflicting\nproperties: high dielectric constants tend to occur in materials with narrow\nband gaps, limiting the operating voltage before dielectric breakdown. We\npresent a high-throughput workflow that combines element substitution, ML\npre-screening, ab initio simulation and human expert intuition to efficiently\nexplore the vast space of unknown materials for potential dielectrics, leading\nto the synthesis and characterization of two novel dielectric materials,\nCsTaTeO6 and Bi2Zr2O7. Our key idea is to deploy ML in a multi-objective\noptimization setting with concave Pareto front. While usually considered more\nchallenging than single-objective optimization, we argue and show preliminary\nevidence that the $1/x$-correlation between band gap and permittivity in fact\nmakes the task more amenable to ML methods by allowing separate models for band\ngap and permittivity to each operate in regions of good training support while\nstill predicting materials of exceptional merit. To our knowledge, this is the\nfirst instance of successful ML-guided multi-objective materials optimization\nachieving experimental synthesis and characterization. CsTaTeO6 is a structure\ngenerated via element substitution not present in our reference data sources,\nthus exemplifying successful de-novo materials design. Meanwhile, we report the\nfirst high-purity synthesis and dielectric characterization of Bi2Zr2O7 with a\nband gap of 2.27 eV and a permittivity of 20.5, meeting all target metrics of\nour multi-objective search.\n",
    "link": "http://arxiv.org/abs/2401.05848v1"
  },
  {
    "title": "Decoding AI's Nudge: A Unified Framework to Predict Human Behavior in\n  AI-assisted Decision Making",
    "authors": "Zhuoyan Li, Zhuoran Lu, Ming Yin",
    "abstract": "  With the rapid development of AI-based decision aids, different forms of AI\nassistance have been increasingly integrated into the human decision making\nprocesses. To best support humans in decision making, it is essential to\nquantitatively understand how diverse forms of AI assistance influence humans'\ndecision making behavior. To this end, much of the current research focuses on\nthe end-to-end prediction of human behavior using ``black-box'' models, often\nlacking interpretations of the nuanced ways in which AI assistance impacts the\nhuman decision making process. Meanwhile, methods that prioritize the\ninterpretability of human behavior predictions are often tailored for one\nspecific form of AI assistance, making adaptations to other forms of assistance\ndifficult. In this paper, we propose a computational framework that can provide\nan interpretable characterization of the influence of different forms of AI\nassistance on decision makers in AI-assisted decision making. By\nconceptualizing AI assistance as the ``{\\em nudge}'' in human decision making\nprocesses, our approach centers around modelling how different forms of AI\nassistance modify humans' strategy in weighing different information in making\ntheir decisions. Evaluations on behavior data collected from real human\ndecision makers show that the proposed framework outperforms various baselines\nin accurately predicting human behavior in AI-assisted decision making. Based\non the proposed framework, we further provide insights into how individuals\nwith different cognitive styles are nudged by AI assistance differently.\n",
    "link": "http://arxiv.org/abs/2401.05840v1"
  },
  {
    "title": "Revisiting Silhouette: From Micro to Macro Aggregation",
    "authors": "Georgios Vardakas, John Pavlopoulos, Aristidis Likas",
    "abstract": "  Silhouette coefficient is an established internal clustering evaluation\nmeasure that produces a score per data point, assessing the quality of its\nclustering assignment. To assess the quality of the clustering of the whole\ndataset, the scores of all the points in the dataset are typically averaged\ninto a single value, a strategy which we call as micro-averaging. As we\nillustrate in this work, by using a synthetic example, this micro-averaging\nstrategy is sensitive both to cluster imbalance and outliers (background\nnoise). To address these issues, we propose an alternative aggregation\nstrategy, which first averages the silhouette scores at a cluster level and\nthen (macro) averages the scores across the clusters. Based on the same\nsynthetic example, we show that the proposed macro-averaged silhouette score is\nrobust to cluster imbalance and background noise. We have conducted an\nexperimental study showing that our macro-averaged variant provides better\nestimates of the ground truth number of clusters on several cases compared to\nthe typical micro-averaged score.\n",
    "link": "http://arxiv.org/abs/2401.05831v1"
  },
  {
    "title": "Hallucination Benchmark in Medical Visual Question Answering",
    "authors": "Jinge Wu, Yunsoo Kim, Honghan Wu",
    "abstract": "  The recent success of large language and vision models on vision question\nanswering (VQA), particularly their applications in medicine (Med-VQA), has\nshown a great potential of realizing effective visual assistants for\nhealthcare. However, these models are not extensively tested on the\nhallucination phenomenon in clinical settings. Here, we created a hallucination\nbenchmark of medical images paired with question-answer sets and conducted a\ncomprehensive evaluation of the state-of-the-art models. The study provides an\nin-depth analysis of current models limitations and reveals the effectiveness\nof various prompting strategies.\n",
    "link": "http://arxiv.org/abs/2401.05827v1"
  },
  {
    "title": "Towards Goal-Oriented Agents for Evolving Problems Observed via\n  Conversation",
    "authors": "Michael Free, Andrew Langworthy, Mary Dimitropoulaki, Simon Thompson",
    "abstract": "  The objective of this work is to train a chatbot capable of solving evolving\nproblems through conversing with a user about a problem the chatbot cannot\ndirectly observe. The system consists of a virtual problem (in this case a\nsimple game), a simulated user capable of answering natural language questions\nthat can observe and perform actions on the problem, and a Deep Q-Network\n(DQN)-based chatbot architecture. The chatbot is trained with the goal of\nsolving the problem through dialogue with the simulated user using\nreinforcement learning. The contributions of this paper are as follows: a\nproposed architecture to apply a conversational DQN-based agent to evolving\nproblems, an exploration of training methods such as curriculum learning on\nmodel performance and the effect of modified reward functions in the case of\nincreasing environment complexity.\n",
    "link": "http://arxiv.org/abs/2401.05822v1"
  },
  {
    "title": "Cheetah: Bridging the Gap Between Machine Learning and Particle\n  Accelerator Physics with High-Speed, Differentiable Simulations",
    "authors": "Jan Kaiser, Chenran Xu, Annika Eichler, Andrea Santamaria Garcia",
    "abstract": "  Machine learning has emerged as a powerful solution to the modern challenges\nin accelerator physics. However, the limited availability of beam time, the\ncomputational cost of simulations, and the high-dimensionality of optimisation\nproblems pose significant challenges in generating the required data for\ntraining state-of-the-art machine learning models. In this work, we introduce\nCheetah, a PyTorch-based high-speed differentiable linear-beam dynamics code.\nCheetah enables the fast collection of large data sets by reducing computation\ntimes by multiple orders of magnitude and facilitates efficient gradient-based\noptimisation for accelerator tuning and system identification. This positions\nCheetah as a user-friendly, readily extensible tool that integrates seamlessly\nwith widely adopted machine learning tools. We showcase the utility of Cheetah\nthrough five examples, including reinforcement learning training,\ngradient-based beamline tuning, gradient-based system identification,\nphysics-informed Bayesian optimisation priors, and modular neural network\nsurrogate modelling of space charge effects. The use of such a high-speed\ndifferentiable simulation code will simplify the development of machine\nlearning-based methods for particle accelerators and fast-track their\nintegration into everyday operations of accelerator facilities.\n",
    "link": "http://arxiv.org/abs/2401.05815v1"
  },
  {
    "title": "Tuning LLMs with Contrastive Alignment Instructions for Machine\n  Translation in Unseen, Low-resource Languages",
    "authors": "Zhuoyuan Mao, Yen Yu",
    "abstract": "  This article introduces contrastive alignment instructions (AlignInstruct) to\naddress two challenges in machine translation (MT) on large language models\n(LLMs). One is the expansion of supported languages to previously unseen ones.\nThe second relates to the lack of data in low-resource languages. Model\nfine-tuning through MT instructions (MTInstruct) is a straightforward approach\nto the first challenge. However, MTInstruct is limited by weak cross-lingual\nsignals inherent in the second challenge. AlignInstruct emphasizes\ncross-lingual supervision via a cross-lingual discriminator built using\nstatistical word alignments. Our results based on fine-tuning the BLOOMZ models\n(1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can\neffectively translate unseen languages using MTInstruct; (2) AlignInstruct led\nto consistent improvements in translation quality across 48 translation\ndirections involving English; (3) Discriminator-based instructions outperformed\ntheir generative counterparts as cross-lingual instructions; (4) AlignInstruct\nimproved performance in 30 zero-shot directions.\n",
    "link": "http://arxiv.org/abs/2401.05811v1"
  },
  {
    "title": "Graph Spatiotemporal Process for Multivariate Time Series Anomaly\n  Detection with Missing Values",
    "authors": "Yu Zheng, Huan Yee Koh, Ming Jin, Lianhua Chi, Haishuai Wang, Khoa T. Phan, Yi-Ping Phoebe Chen, Shirui Pan, Wei Xiang",
    "abstract": "  The detection of anomalies in multivariate time series data is crucial for\nvarious practical applications, including smart power grids, traffic flow\nforecasting, and industrial process control. However, real-world time series\ndata is usually not well-structured, posting significant challenges to existing\napproaches: (1) The existence of missing values in multivariate time series\ndata along variable and time dimensions hinders the effective modeling of\ninterwoven spatial and temporal dependencies, resulting in important patterns\nbeing overlooked during model training; (2) Anomaly scoring with\nirregularly-sampled observations is less explored, making it difficult to use\nexisting detectors for multivariate series without fully-observed values. In\nthis work, we introduce a novel framework called GST-Pro, which utilizes a\ngraph spatiotemporal process and anomaly scorer to tackle the aforementioned\nchallenges in detecting anomalies on irregularly-sampled multivariate time\nseries. Our approach comprises two main components. First, we propose a graph\nspatiotemporal process based on neural controlled differential equations. This\nprocess enables effective modeling of multivariate time series from both\nspatial and temporal perspectives, even when the data contains missing values.\nSecond, we present a novel distribution-based anomaly scoring mechanism that\nalleviates the reliance on complete uniform observations. By analyzing the\npredictions of the graph spatiotemporal process, our approach allows anomalies\nto be easily detected. Our experimental results show that the GST-Pro method\ncan effectively detect anomalies in time series data and outperforms\nstate-of-the-art methods, regardless of whether there are missing values\npresent in the data. Our code is available: https://github.com/huankoh/GST-Pro.\n",
    "link": "http://arxiv.org/abs/2401.05800v1"
  },
  {
    "title": "Designing Heterogeneous LLM Agents for Financial Sentiment Analysis",
    "authors": "Frank Xing",
    "abstract": "  Large language models (LLMs) have drastically changed the possible ways to\ndesign intelligent systems, shifting the focuses from massive data acquisition\nand new modeling training to human alignment and strategical elicitation of the\nfull potential of existing pre-trained models. This paradigm shift, however, is\nnot fully realized in financial sentiment analysis (FSA), due to the\ndiscriminative nature of this task and a lack of prescriptive knowledge of how\nto leverage generative models in such a context. This study investigates the\neffectiveness of the new paradigm, i.e., using LLMs without fine-tuning for\nFSA. Rooted in Minsky's theory of mind and emotions, a design framework with\nheterogeneous LLM agents is proposed. The framework instantiates specialized\nagents using prior domain knowledge of the types of FSA errors and reasons on\nthe aggregated agent discussions. Comprehensive evaluation on FSA datasets show\nthat the framework yields better accuracies, especially when the discussions\nare substantial. This study contributes to the design foundations and paves new\navenues for LLMs-based FSA. Implications on business and management are also\ndiscussed.\n",
    "link": "http://arxiv.org/abs/2401.05799v1"
  },
  {
    "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language\n  Model Systems",
    "authors": "Tianyu Cui, Yanling Wang, Chuanpu Fu, Yong Xiao, Sijia Li, Xinhao Deng, Yunpeng Liu, Qinglin Zhang, Ziyi Qiu, Peiyang Li, Zhixing Tan, Junwu Xiong, Xinyu Kong, Zujie Wen, Ke Xu, Qi Li",
    "abstract": "  Large language models (LLMs) have strong capabilities in solving diverse\nnatural language processing tasks. However, the safety and security issues of\nLLM systems have become the major obstacle to their widespread application.\nMany studies have extensively investigated risks in LLM systems and developed\nthe corresponding mitigation strategies. Leading-edge enterprises such as\nOpenAI, Google, Meta, and Anthropic have also made lots of efforts on\nresponsible LLMs. Therefore, there is a growing need to organize the existing\nstudies and establish comprehensive taxonomies for the community. In this\npaper, we delve into four essential modules of an LLM system, including an\ninput module for receiving prompts, a language model trained on extensive\ncorpora, a toolchain module for development and deployment, and an output\nmodule for exporting LLM-generated content. Based on this, we propose a\ncomprehensive taxonomy, which systematically analyzes potential risks\nassociated with each module of an LLM system and discusses the corresponding\nmitigation strategies. Furthermore, we review prevalent benchmarks, aiming to\nfacilitate the risk assessment of LLM systems. We hope that this paper can help\nLLM participants embrace a systematic perspective to build their responsible\nLLM systems.\n",
    "link": "http://arxiv.org/abs/2401.05778v1"
  },
  {
    "title": "Knowledge Translation: A New Pathway for Model Compression",
    "authors": "Wujie Sun, Defang Chen, Jiawei Chen, Yan Feng, Chun Chen, Can Wang",
    "abstract": "  Deep learning has witnessed significant advancements in recent years at the\ncost of increasing training, inference, and model storage overhead. While\nexisting model compression methods strive to reduce the number of model\nparameters while maintaining high accuracy, they inevitably necessitate the\nre-training of the compressed model or impose architectural constraints. To\novercome these limitations, this paper presents a novel framework, termed\n\\textbf{K}nowledge \\textbf{T}ranslation (KT), wherein a ``translation'' model\nis trained to receive the parameters of a larger model and generate compressed\nparameters. The concept of KT draws inspiration from language translation,\nwhich effectively employs neural networks to convert different languages,\nmaintaining identical meaning. Accordingly, we explore the potential of neural\nnetworks to convert models of disparate sizes, while preserving their\nfunctionality. We propose a comprehensive framework for KT, introduce data\naugmentation strategies to enhance model performance despite restricted\ntraining data, and successfully demonstrate the feasibility of KT on the MNIST\ndataset. Code is available at \\url{https://github.com/zju-SWJ/KT}.\n",
    "link": "http://arxiv.org/abs/2401.05772v1"
  },
  {
    "title": "A Shocking Amount of the Web is Machine Translated: Insights from\n  Multi-Way Parallelism",
    "authors": "Brian Thompson, Mehak Preet Dhaliwal, Peter Frisch, Tobias Domhan, Marcello Federico",
    "abstract": "  We show that content on the web is often translated into many languages, and\nthe low quality of these multi-way translations indicates they were likely\ncreated using Machine Translation (MT). Multi-way parallel, machine generated\ncontent not only dominates the translations in lower resource languages; it\nalso constitutes a large fraction of the total web content in those languages.\nWe also find evidence of a selection bias in the type of content which is\ntranslated into many languages, consistent with low quality English content\nbeing translated en masse into many lower resource languages, via MT. Our work\nraises serious concerns about training models such as multilingual large\nlanguage models on both monolingual and bilingual data scraped from the web.\n",
    "link": "http://arxiv.org/abs/2401.05749v1"
  },
  {
    "title": "Consistent Query Answering for Existential Rules under Tuple-Deletion\n  Semantics",
    "authors": "Lorenzo Marconi, Riccardo Rosati",
    "abstract": "  We study consistent query answering over knowledge bases expressed by\nexistential rules. Specifically, we establish the data complexity of consistent\nquery answering and repair checking under tuple-deletion semantics for a\ngeneral class of disjunctive existential rules and for several subclasses\nthereof (acyclic, linear, full, guarded, and sticky). In particular, we\nidentify several cases in which the above problems are tractable or even\nfirst-order rewritable, and present new query rewriting techniques that can be\nthe basis for practical inconsistency-tolerant query answering systems.\n",
    "link": "http://arxiv.org/abs/2401.05743v1"
  },
  {
    "title": "Enhancing Contrastive Learning with Efficient Combinatorial Positive\n  Pairing",
    "authors": "Jaeill Kim, Duhun Hwang, Eunjung Lee, Jangwon Suh, Jimyeong Kim, Wonjong Rhee",
    "abstract": "  In the past few years, contrastive learning has played a central role for the\nsuccess of visual unsupervised representation learning. Around the same time,\nhigh-performance non-contrastive learning methods have been developed as well.\nWhile most of the works utilize only two views, we carefully review the\nexisting multi-view methods and propose a general multi-view strategy that can\nimprove learning speed and performance of any contrastive or non-contrastive\nmethod. We first analyze CMC's full-graph paradigm and empirically show that\nthe learning speed of $K$-views can be increased by $_{K}\\mathrm{C}_{2}$ times\nfor small learning rate and early training. Then, we upgrade CMC's full-graph\nby mixing views created by a crop-only augmentation, adopting small-size views\nas in SwAV multi-crop, and modifying the negative sampling. The resulting\nmulti-view strategy is called ECPP (Efficient Combinatorial Positive Pairing).\nWe investigate the effectiveness of ECPP by applying it to SimCLR and assessing\nthe linear evaluation performance for CIFAR-10 and ImageNet-100. For each\nbenchmark, we achieve a state-of-the-art performance. In case of ImageNet-100,\nECPP boosted SimCLR outperforms supervised learning.\n",
    "link": "http://arxiv.org/abs/2401.05730v1"
  },
  {
    "title": "R-BI: Regularized Batched Inputs enhance Incremental Decoding Framework\n  for Low-Latency Simultaneous Speech Translation",
    "authors": "Jiaxin Guo, Zhanglin Wu, Zongyao Li, Hengchao Shang, Daimeng Wei, Xiaoyu Chen, Zhiqiang Rao, Shaojun Li, Hao Yang",
    "abstract": "  Incremental Decoding is an effective framework that enables the use of an\noffline model in a simultaneous setting without modifying the original model,\nmaking it suitable for Low-Latency Simultaneous Speech Translation. However,\nthis framework may introduce errors when the system outputs from incomplete\ninput. To reduce these output errors, several strategies such as Hold-$n$,\nLA-$n$, and SP-$n$ can be employed, but the hyper-parameter $n$ needs to be\ncarefully selected for optimal performance. Moreover, these strategies are more\nsuitable for end-to-end systems than cascade systems. In our paper, we propose\na new adaptable and efficient policy named \"Regularized Batched Inputs\". Our\nmethod stands out by enhancing input diversity to mitigate output errors. We\nsuggest particular regularization techniques for both end-to-end and cascade\nsystems. We conducted experiments on IWSLT Simultaneous Speech Translation\n(SimulST) tasks, which demonstrate that our approach achieves low latency while\nmaintaining no more than 2 BLEU points loss compared to offline systems.\nFurthermore, our SimulST systems attained several new state-of-the-art results\nin various language directions.\n",
    "link": "http://arxiv.org/abs/2401.05700v1"
  },
  {
    "title": "Deep Learning Meets Mechanism Design: Key Results and Some Novel\n  Applications",
    "authors": "V. Udaya Sankar, Vishisht Srihari Rao, Y. Narahari",
    "abstract": "  Mechanism design is essentially reverse engineering of games and involves\ninducing a game among strategic agents in a way that the induced game satisfies\na set of desired properties in an equilibrium of the game. Desirable properties\nfor a mechanism include incentive compatibility, individual rationality,\nwelfare maximisation, revenue maximisation (or cost minimisation), fairness of\nallocation, etc. It is known from mechanism design theory that only certain\nstrict subsets of these properties can be simultaneously satisfied exactly by\nany given mechanism. Often, the mechanisms required by real-world applications\nmay need a subset of these properties that are theoretically impossible to be\nsimultaneously satisfied. In such cases, a prominent recent approach is to use\na deep learning based approach to learn a mechanism that approximately\nsatisfies the required properties by minimizing a suitably defined loss\nfunction. In this paper, we present, from relevant literature, technical\ndetails of using a deep learning approach for mechanism design and provide an\noverview of key results in this topic. We demonstrate the power of this\napproach for three illustrative case studies: (a) efficient energy management\nin a vehicular network (b) resource allocation in a mobile network (c)\ndesigning a volume discount procurement auction for agricultural inputs.\nSection 6 concludes the paper.\n",
    "link": "http://arxiv.org/abs/2401.05683v1"
  },
  {
    "title": "Use of Graph Neural Networks in Aiding Defensive Cyber Operations",
    "authors": "Shaswata Mitra, Trisha Chakraborty, Subash Neupane, Aritran Piplai, Sudip Mittal",
    "abstract": "  In an increasingly interconnected world, where information is the lifeblood\nof modern society, regular cyber-attacks sabotage the confidentiality,\nintegrity, and availability of digital systems and information. Additionally,\ncyber-attacks differ depending on the objective and evolve rapidly to disguise\ndefensive systems. However, a typical cyber-attack demonstrates a series of\nstages from attack initiation to final resolution, called an attack life cycle.\nThese diverse characteristics and the relentless evolution of cyber attacks\nhave led cyber defense to adopt modern approaches like Machine Learning to\nbolster defensive measures and break the attack life cycle. Among the adopted\nML approaches, Graph Neural Networks have emerged as a promising approach for\nenhancing the effectiveness of defensive measures due to their ability to\nprocess and learn from heterogeneous cyber threat data. In this paper, we look\ninto the application of GNNs in aiding to break each stage of one of the most\nrenowned attack life cycles, the Lockheed Martin Cyber Kill Chain. We address\neach phase of CKC and discuss how GNNs contribute to preparing and preventing\nan attack from a defensive standpoint. Furthermore, We also discuss open\nresearch areas and further improvement scopes.\n",
    "link": "http://arxiv.org/abs/2401.05680v1"
  },
  {
    "title": "EsaCL: Efficient Continual Learning of Sparse Models",
    "authors": "Weijieying Ren, Vasant G Honavar",
    "abstract": "  A key challenge in the continual learning setting is to efficiently learn a\nsequence of tasks without forgetting how to perform previously learned tasks.\nMany existing approaches to this problem work by either retraining the model on\nprevious tasks or by expanding the model to accommodate new tasks. However,\nthese approaches typically suffer from increased storage and computational\nrequirements, a problem that is worsened in the case of sparse models due to\nneed for expensive re-training after sparsification. To address this challenge,\nwe propose a new method for efficient continual learning of sparse models\n(EsaCL) that can automatically prune redundant parameters without adversely\nimpacting the model's predictive power, and circumvent the need of retraining.\nWe conduct a theoretical analysis of loss landscapes with parameter pruning,\nand design a directional pruning (SDP) strategy that is informed by the\nsharpness of the loss function with respect to the model parameters. SDP\nensures model with minimal loss of predictive accuracy, accelerating the\nlearning of sparse models at each stage. To accelerate model update, we\nintroduce an intelligent data selection (IDS) strategy that can identify\ncritical instances for estimating loss landscape, yielding substantially\nimproved data efficiency. The results of our experiments show that EsaCL\nachieves performance that is competitive with the state-of-the-art methods on\nthree continual learning benchmarks, while using substantially reduced memory\nand computational resources.\n",
    "link": "http://arxiv.org/abs/2401.05667v1"
  },
  {
    "title": "Towards Conversational Diagnostic AI",
    "authors": "Tao Tu, Anil Palepu, Mike Schaekermann, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Nenad Tomasev, Shekoofeh Azizi, Karan Singhal, Yong Cheng, Le Hou, Albert Webson, Kavita Kulkarni, S Sara Mahdavi, Christopher Semturs, Juraj Gottweis, Joelle Barral, Katherine Chou, Greg S Corrado, Yossi Matias, Alan Karthikesalingam, Vivek Natarajan",
    "abstract": "  At the heart of medicine lies the physician-patient dialogue, where skillful\nhistory-taking paves the way for accurate diagnosis, effective management, and\nenduring trust. Artificial Intelligence (AI) systems capable of diagnostic\ndialogue could increase accessibility, consistency, and quality of care.\nHowever, approximating clinicians' expertise is an outstanding grand challenge.\nHere, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large\nLanguage Model (LLM) based AI system optimized for diagnostic dialogue.\n  AMIE uses a novel self-play based simulated environment with automated\nfeedback mechanisms for scaling learning across diverse disease conditions,\nspecialties, and contexts. We designed a framework for evaluating\nclinically-meaningful axes of performance including history-taking, diagnostic\naccuracy, management reasoning, communication skills, and empathy. We compared\nAMIE's performance to that of primary care physicians (PCPs) in a randomized,\ndouble-blind crossover study of text-based consultations with validated patient\nactors in the style of an Objective Structured Clinical Examination (OSCE). The\nstudy included 149 case scenarios from clinical providers in Canada, the UK,\nand India, 20 PCPs for comparison with AMIE, and evaluations by specialist\nphysicians and patient actors. AMIE demonstrated greater diagnostic accuracy\nand superior performance on 28 of 32 axes according to specialist physicians\nand 24 of 26 axes according to patient actors. Our research has several\nlimitations and should be interpreted with appropriate caution. Clinicians were\nlimited to unfamiliar synchronous text-chat which permits large-scale\nLLM-patient interactions but is not representative of usual clinical practice.\nWhile further research is required before AMIE could be translated to\nreal-world settings, the results represent a milestone towards conversational\ndiagnostic AI.\n",
    "link": "http://arxiv.org/abs/2401.05654v1"
  },
  {
    "title": "DrawTalking: Building Interactive Worlds by Sketching and Speaking",
    "authors": "Karl Toby Rosenberg, Rubaiat Habib Kazi, Li-Yi Wei, Haijun Xia, Ken Perlin",
    "abstract": "  We introduce an interactive approach, DrawTalking, in which the user builds\ninteractive worlds by sketching and speaking. It emphasizes user control and\nflexibility, and gives programming-like capability without code. We implemented\nit on the iPad. An open-ended study shows the mechanics resonate and are\napplicable to many creative-exploratory use cases. We hope to inspire and\ninform research in future natural user-centered interfaces.\n",
    "link": "http://arxiv.org/abs/2401.05631v1"
  },
  {
    "title": "The Benefits of a Concise Chain of Thought on Problem-Solving in Large\n  Language Models",
    "authors": "Matthew Renze, Erhan Guven",
    "abstract": "  In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We\ncompared standard CoT and CCoT prompts to see how conciseness impacts response\nlength and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4\nwith a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced\naverage response length by 48.70% for both GPT-3.5 and GPT-4 while having a\nnegligible impact on problem-solving performance. However, on math problems,\nGPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads\nto an average per-token cost reduction of 22.67%. These results have practical\nimplications for AI systems engineers using LLMs to solve real-world problems\nwith CoT prompt-engineering techniques. In addition, these results provide more\ngeneral insight for AI researchers studying the emergent behavior of\nstep-by-step reasoning in LLMs.\n",
    "link": "http://arxiv.org/abs/2401.05618v1"
  },
  {
    "title": "Graph Q-Learning for Combinatorial Optimization",
    "authors": "Victoria M. Dax, Jiachen Li, Kevin Leahy, Mykel J. Kochenderfer",
    "abstract": "  Graph-structured data is ubiquitous throughout natural and social sciences,\nand Graph Neural Networks (GNNs) have recently been shown to be effective at\nsolving prediction and inference problems on graph data. In this paper, we\npropose and demonstrate that GNNs can be applied to solve Combinatorial\nOptimization (CO) problems. CO concerns optimizing a function over a discrete\nsolution space that is often intractably large. To learn to solve CO problems,\nwe formulate the optimization process as a sequential decision making problem,\nwhere the return is related to how close the candidate solution is to\noptimality. We use a GNN to learn a policy to iteratively build increasingly\npromising candidate solutions. We present preliminary evidence that GNNs\ntrained through Q-Learning can solve CO problems with performance approaching\nstate-of-the-art heuristic-based solvers, using only a fraction of the\nparameters and training time.\n",
    "link": "http://arxiv.org/abs/2401.05610v1"
  },
  {
    "title": "REBUS: A Robust Evaluation Benchmark of Understanding Symbols",
    "authors": "Andrew Gritsevskiy, Arjun Panickssery, Aaron Kirtland, Derik Kauffman, Hans Gundlach, Irina Gritsevskaya, Joe Cavanagh, Jonathan Chiang, Lydia La Roux, Michelle Hung",
    "abstract": "  We propose a new benchmark evaluating the performance of multimodal large\nlanguage models on rebus puzzles. The dataset covers 333 original examples of\nimage-based wordplay, cluing 13 categories such as movies, composers, major\ncities, and food. To achieve good performance on the benchmark of identifying\nthe clued word or phrase, models must combine image recognition and string\nmanipulation with hypothesis testing, multi-step reasoning, and an\nunderstanding of human cognition, making for a complex, multimodal evaluation\nof capabilities. We find that proprietary models such as GPT-4V and Gemini Pro\nsignificantly outperform all other tested models. However, even the best model\nhas a final accuracy of just 24%, highlighting the need for substantial\nimprovements in reasoning. Further, models rarely understand all parts of a\npuzzle, and are almost always incapable of retroactively explaining the correct\nanswer. Our benchmark can therefore be used to identify major shortcomings in\nthe knowledge and reasoning of multimodal large language models.\n",
    "link": "http://arxiv.org/abs/2401.05604v1"
  }
]