[
  {
    "title": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other\n  Modalities",
    "authors": "Yiyuan Zhang, Xiaohan Ding, Kaixiong Gong, Yixiao Ge, Ying Shan, Xiangyu Yue",
    "abstract": "  We propose to improve transformers of a specific modality with irrelevant\ndata from other modalities, e.g., improve an ImageNet model with audio or point\ncloud datasets. We would like to highlight that the data samples of the target\nmodality are irrelevant to the other modalities, which distinguishes our method\nfrom other works utilizing paired (e.g., CLIP) or interleaved data of different\nmodalities. We propose a methodology named Multimodal Pathway - given a target\nmodality and a transformer designed for it, we use an auxiliary transformer\ntrained with data of another modality and construct pathways to connect\ncomponents of the two models so that data of the target modality can be\nprocessed by both models. In this way, we utilize the universal\nsequence-to-sequence modeling abilities of transformers obtained from two\nmodalities. As a concrete implementation, we use a modality-specific tokenizer\nand task-specific head as usual but utilize the transformer blocks of the\nauxiliary model via a proposed method named Cross-Modal Re-parameterization,\nwhich exploits the auxiliary weights without any inference costs. On the image,\npoint cloud, video, and audio recognition tasks, we observe significant and\nconsistent performance improvements with irrelevant data from other modalities.\nThe code and models are available at https://github.com/AILab-CVC/M2PT.\n",
    "link": "http://arxiv.org/abs/2401.14405v1"
  },
  {
    "title": "Adaptive Mobile Manipulation for Articulated Objects In the Open World",
    "authors": "Haoyu Xiong, Russell Mendonca, Kenneth Shaw, Deepak Pathak",
    "abstract": "  Deploying robots in open-ended unstructured environments such as homes has\nbeen a long-standing research problem. However, robots are often studied only\nin closed-off lab settings, and prior mobile manipulation work is restricted to\npick-move-place, which is arguably just the tip of the iceberg in this area. In\nthis paper, we introduce Open-World Mobile Manipulation System, a full-stack\napproach to tackle realistic articulated object operation, e.g. real-world\ndoors, cabinets, drawers, and refrigerators in open-ended unstructured\nenvironments. The robot utilizes an adaptive learning framework to initially\nlearns from a small set of data through behavior cloning, followed by learning\nfrom online practice on novel objects that fall outside the training\ndistribution. We also develop a low-cost mobile manipulation hardware platform\ncapable of safe and autonomous online adaptation in unstructured environments\nwith a cost of around 20,000 USD. In our experiments we utilize 20 articulate\nobjects across 4 buildings in the CMU campus. With less than an hour of online\nlearning for each object, the system is able to increase success rate from 50%\nof BC pre-training to 95% using online adaptation. Video results at\nhttps://open-world-mobilemanip.github.io/\n",
    "link": "http://arxiv.org/abs/2401.14403v1"
  },
  {
    "title": "TURNA: A Turkish Encoder-Decoder Language Model for Enhanced\n  Understanding and Generation",
    "authors": "G\u00f6k\u00e7e Uludo\u011fan, Zeynep Yirmibe\u015fo\u011flu Balal, Furkan Akkurt, Melik\u015fah T\u00fcrker, Onur G\u00fcng\u00f6r, Susan \u00dcsk\u00fcdarl\u0131",
    "abstract": "  The recent advances in natural language processing have predominantly favored\nwell-resourced English-centric models, resulting in a significant gap with\nlow-resource languages. In this work, we introduce the language model TURNA,\nwhich is developed for the low-resource language Turkish and is capable of both\nnatural language understanding and generation tasks. TURNA is pretrained with\nan encoder-decoder architecture based on the unified framework UL2 with a\ndiverse corpus that we specifically curated for this purpose. We evaluated\nTURNA with three generation tasks and five understanding tasks for Turkish. The\nresults show that TURNA outperforms several multilingual models in both\nunderstanding and generation tasks, and competes with monolingual Turkish\nmodels in understanding tasks. TURNA is made available at\nhttps://huggingface.co/boun-tabi-LMG/TURNA .\n",
    "link": "http://arxiv.org/abs/2401.14373v1"
  },
  {
    "title": "Efficient Optimisation of Physical Reservoir Computers using only a\n  Delayed Input",
    "authors": "Enrico Picco, Lina Jaurigue, Kathy L\u00fcdge, Serge Massar",
    "abstract": "  We present an experimental validation of a recently proposed optimization\ntechnique for reservoir computing, using an optoelectronic setup. Reservoir\ncomputing is a robust framework for signal processing applications, and the\ndevelopment of efficient optimization approaches remains a key challenge. The\ntechnique we address leverages solely a delayed version of the input signal to\nidentify the optimal operational region of the reservoir, simplifying the\ntraditionally time-consuming task of hyperparameter tuning. We verify the\neffectiveness of this approach on different benchmark tasks and reservoir\noperating conditions.\n",
    "link": "http://arxiv.org/abs/2401.14371v1"
  },
  {
    "title": "Genie: Achieving Human Parity in Content-Grounded Datasets Generation",
    "authors": "Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv, Nathaniel Mills, Assaf Toledo, Eyal Shnarch, Leshem Choshen",
    "abstract": "  The lack of high-quality data for content-grounded generation tasks has been\nidentified as a major obstacle to advancing these tasks. To address this gap,\nwe propose Genie, a novel method for automatically generating high-quality\ncontent-grounded data. It consists of three stages: (a) Content Preparation,\n(b) Generation: creating task-specific examples from the content (e.g.,\nquestion-answer pairs or summaries). (c) Filtering mechanism aiming to ensure\nthe quality and faithfulness of the generated data. We showcase this\nmethodology by generating three large-scale synthetic data, making wishes, for\nLong-Form Question-Answering (LFQA), summarization, and information extraction.\nIn a human evaluation, our generated data was found to be natural and of high\nquality. Furthermore, we compare models trained on our data with models trained\non human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for\nSummarization. We show that our models are on par with or outperforming models\ntrained on human-generated data and consistently outperforming them in\nfaithfulness. Finally, we applied our method to create LFQA data within the\nmedical domain and compared a model trained on it with models trained on other\ndomains.\n",
    "link": "http://arxiv.org/abs/2401.14367v1"
  },
  {
    "title": "The Typing Cure: Experiences with Large Language Model Chatbots for\n  Mental Health Support",
    "authors": "Inhwa Song, Sachin R. Pendse, Neha Kumar, Munmun De Choudhury",
    "abstract": "  People experiencing severe distress increasingly use Large Language Model\n(LLM) chatbots as mental health support tools. Discussions on social media have\ndescribed how engagements were lifesaving for some, but evidence suggests that\ngeneral-purpose LLM chatbots also have notable risks that could endanger the\nwelfare of users if not designed responsibly. In this study, we investigate the\nlived experiences of people who have used LLM chatbots for mental health\nsupport. We build on interviews with 21 individuals from globally diverse\nbackgrounds to analyze how users create unique support roles for their\nchatbots, fill in gaps in everyday care, and navigate associated cultural\nlimitations when seeking support from chatbots. We ground our analysis in\npsychotherapy literature around effective support, and introduce the concept of\ntherapeutic alignment, or aligning AI with therapeutic values for mental health\ncontexts. Our study offers recommendations for how designers can approach the\nethical and effective use of LLM chatbots and other AI mental health support\ntools in mental health care.\n",
    "link": "http://arxiv.org/abs/2401.14362v1"
  },
  {
    "title": "Progressive Multi-task Anti-Noise Learning and Distilling Frameworks for\n  Fine-grained Vehicle Recognition",
    "authors": "Dichao Liu",
    "abstract": "  Fine-grained vehicle recognition (FGVR) is an essential fundamental\ntechnology for intelligent transportation systems, but very difficult because\nof its inherent intra-class variation. Most previous FGVR studies only focus on\nthe intra-class variation caused by different shooting angles, positions, etc.,\nwhile the intra-class variation caused by image noise has received little\nattention. This paper proposes a progressive multi-task anti-noise learning\n(PMAL) framework and a progressive multi-task distilling (PMD) framework to\nsolve the intra-class variation problem in FGVR due to image noise. The PMAL\nframework achieves high recognition accuracy by treating image denoising as an\nadditional task in image recognition and progressively forcing a model to learn\nnoise invariance. The PMD framework transfers the knowledge of the PMAL-trained\nmodel into the original backbone network, which produces a model with about the\nsame recognition accuracy as the PMAL-trained model, but without any additional\noverheads over the original backbone network. Combining the two frameworks, we\nobtain models that significantly exceed previous state-of-the-art methods in\nrecognition accuracy on two widely-used, standard FGVR datasets, namely\nStanford Cars, and CompCars, as well as three additional surveillance\nimage-based vehicle-type classification datasets, namely Beijing Institute of\nTechnology (BIT)-Vehicle, Vehicle Type Image Data 2 (VTID2), and Vehicle Images\nDataset for Make Model Recognition (VIDMMR), without any additional overheads\nover the original backbone networks. The source code is available at\nhttps://github.com/Dichao-Liu/Anti-noise_FGVR\n",
    "link": "http://arxiv.org/abs/2401.14336v1"
  },
  {
    "title": "Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of\n  Thoughts",
    "authors": "Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz Kwa\u015bniewski, J\u00fcrgen M\u00fcller, Lukas Gianinazzi, Ales Kubicek, Hubert Niewiadomski, Onur Mutlu, Torsten Hoefler",
    "abstract": "  The field of natural language processing (NLP) has witnessed significant\nprogress in recent years, with a notable focus on improving large language\nmodels' (LLM) performance through innovative prompting techniques. Among these,\nprompt engineering coupled with structures has emerged as a promising paradigm,\nwith designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,\nin which the overall LLM reasoning is guided by a structure such as a graph. As\nillustrated with numerous examples, this paradigm significantly enhances the\nLLM's capability to solve numerous tasks, ranging from logical or mathematical\nreasoning to planning or creative writing. To facilitate the understanding of\nthis growing field and pave the way for future developments, we devise a\ngeneral blueprint for effective and efficient LLM reasoning schemes. For this,\nwe conduct an in-depth analysis of the prompt execution pipeline, clarifying\nand clearly defining different concepts. We then build the first taxonomy of\nstructure-enhanced LLM reasoning schemes. We focus on identifying fundamental\nclasses of harnessed structures, and we analyze the representations of these\nstructures, algorithms executed with these structures, and many others. We\nrefer to these structures as reasoning topologies, because their representation\nbecomes to a degree spatial, as they are contained within the LLM context. Our\nstudy compares existing prompting schemes using the proposed taxonomy,\ndiscussing how certain design choices lead to different patterns in performance\nand cost. We also outline theoretical underpinnings, relationships between\nprompting and others parts of the LLM ecosystem such as knowledge bases, and\nthe associated research challenges. Our work will help to advance future prompt\nengineering techniques.\n",
    "link": "http://arxiv.org/abs/2401.14295v1"
  },
  {
    "title": "AST-2: Single and bi-layered 2-D acoustic soft tactile skin",
    "authors": "Vishnu Rajendran, Simon Parsons, Amir Ghalamzan E",
    "abstract": "  This paper aims to present an innovative and cost-effective design for\nAcoustic Soft Tactile (AST) Skin, with the primary goal of significantly\nenhancing the accuracy of 2-D tactile feature estimation. The existing\nchallenge lies in achieving precise tactile feature estimation, especially\nconcerning contact geometry characteristics, using cost-effective solutions. We\nhypothesise that by harnessing acoustic energy through dedicated acoustic\nchannels in 2 layers beneath the sensing surface and analysing amplitude\nmodulation, we can effectively decode interactions on the sensory surface,\nthereby improving tactile feature estimation. Our approach involves the\ndistinct separation of hardware components responsible for emitting and\nreceiving acoustic signals, resulting in a modular and highly customizable skin\ndesign. Practical tests demonstrate the effectiveness of this novel design,\nachieving remarkable precision in estimating contact normal forces (MAE &lt; 0.8\nN), 2D contact localisation (MAE &lt; 0.7 mm), and contact surface diameter (MAE &lt;\n0.3 mm). In conclusion, the AST skin, with its innovative design and modular\narchitecture, successfully addresses the challenge of tactile feature\nestimation. The presented results showcase its ability to precisely estimate\nvarious tactile features, making it a practical and cost-effective solution for\nrobotic applications.\n",
    "link": "http://arxiv.org/abs/2401.14292v1"
  },
  {
    "title": "POUR-Net: A Population-Prior-Aided Over-Under-Representation Network for\n  Low-Count PET Attenuation Map Generation",
    "authors": "Bo Zhou, Jun Hou, Tianqi Chen, Yinchi Zhou, Xiongchao Chen, Huidong Xie, Qiong Liu, Xueqi Guo, Yu-Jung Tsai, Vladimir Y. Panin, Takuya Toyonaga, James S. Duncan, Chi Liu",
    "abstract": "  Low-dose PET offers a valuable means of minimizing radiation exposure in PET\nimaging. However, the prevalent practice of employing additional CT scans for\ngenerating attenuation maps (u-map) for PET attenuation correction\nsignificantly elevates radiation doses. To address this concern and further\nmitigate radiation exposure in low-dose PET exams, we propose POUR-Net - an\ninnovative population-prior-aided over-under-representation network that aims\nfor high-quality attenuation map generation from low-dose PET. First, POUR-Net\nincorporates an over-under-representation network (OUR-Net) to facilitate\nefficient feature extraction, encompassing both low-resolution abstracted and\nfine-detail features, for assisting deep generation on the full-resolution\nlevel. Second, complementing OUR-Net, a population prior generation machine\n(PPGM) utilizing a comprehensive CT-derived u-map dataset, provides additional\nprior information to aid OUR-Net generation. The integration of OUR-Net and\nPPGM within a cascade framework enables iterative refinement of $\\mu$-map\ngeneration, resulting in the production of high-quality $\\mu$-maps.\nExperimental results underscore the effectiveness of POUR-Net, showing it as a\npromising solution for accurate CT-free low-count PET attenuation correction,\nwhich also surpasses the performance of previous baseline methods.\n",
    "link": "http://arxiv.org/abs/2401.14285v1"
  },
  {
    "title": "RomanSetu: Efficiently unlocking multilingual capabilities of Large\n  Language Models models via Romanization",
    "authors": "Jaavid Aktar Husain, Raj Dabre, Aswanth Kumar, Ratish Puduppully, Anoop Kunchukuttan",
    "abstract": "  This study addresses the challenge of extending Large Language Models (LLMs)\nto non-English languages, specifically those using non-Latin scripts. We\npropose an innovative approach that utilizes the romanized form of text as an\ninterface for LLMs, hypothesizing that its frequent informal use and shared\ntokens with English enhance cross-lingual alignment. Focusing on Hindi, we\ndemonstrate through Hindi-to-English translation and sentiment analysis tasks\nthat romanized text not only significantly improves inference efficiency due to\nits lower fertility compared to native text but also achieves competitive\nperformance with limited pre-training. Additionally, our novel multi-script\nprompting approach, which combines romanized and native texts, shows promise in\nfurther enhancing task performance. These findings suggest the potential of\nromanization in bridging the language gap for LLM applications, with future\nwork aimed at expanding this approach to more languages and tasks.\n",
    "link": "http://arxiv.org/abs/2401.14280v1"
  },
  {
    "title": "Transformers and Cortical Waves: Encoders for Pulling In Context Across\n  Time",
    "authors": "Lyle Muller, Patricia S. Churchland, Terrence J. Sejnowski",
    "abstract": "  The capabilities of transformer networks such as ChatGPT and other Large\nLanguage Models (LLMs) have captured the world's attention. The crucial\ncomputational mechanism underlying their performance relies on transforming a\ncomplete input sequence - for example, all the words in a sentence into a long\n\"encoding vector\" - that allows transformers to learn long-range temporal\ndependencies in naturalistic sequences. Specifically, \"self-attention\" applied\nto this encoding vector enhances temporal context in transformers by computing\nassociations between pairs of words in the input sequence. We suggest that\nwaves of neural activity, traveling across single cortical regions or across\nmultiple regions at the whole-brain scale, could implement a similar encoding\nprinciple. By encapsulating recent input history into a single spatial pattern\nat each moment in time, cortical waves may enable temporal context to be\nextracted from sequences of sensory inputs, the same computational principle\nused in transformers.\n",
    "link": "http://arxiv.org/abs/2401.14267v1"
  },
  {
    "title": "Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation",
    "authors": "Minglin Chen, Longguang Wang, Weihao Yuan, Yukun Wang, Zhe Sheng, Yisheng He, Zilong Dong, Liefeng Bo, Yulan Guo",
    "abstract": "  Recently, text-to-3D approaches have achieved high-fidelity 3D content\ngeneration using text description. However, the generated objects are\nstochastic and lack fine-grained control. Sketches provide a cheap approach to\nintroduce such fine-grained control. Nevertheless, it is challenging to achieve\nflexible control from these sketches due to their abstraction and ambiguity. In\nthis paper, we present a multi-view sketch-guided text-to-3D generation\nframework (namely, Sketch2NeRF) to add sketch control to 3D generation.\nSpecifically, our method leverages pretrained 2D diffusion models (e.g., Stable\nDiffusion and ControlNet) to supervise the optimization of a 3D scene\nrepresented by a neural radiance field (NeRF). We propose a novel synchronized\ngeneration and reconstruction method to effectively optimize the NeRF. In the\nexperiments, we collected two kinds of multi-view sketch datasets to evaluate\nthe proposed method. We demonstrate that our method can synthesize 3D\nconsistent contents with fine-grained sketch control while being high-fidelity\nto text prompts. Extensive results show that our method achieves\nstate-of-the-art performance in terms of sketch similarity and text alignment.\n",
    "link": "http://arxiv.org/abs/2401.14257v1"
  },
  {
    "title": "Assessing the Portability of Parameter Matrices Trained by\n  Parameter-Efficient Finetuning Methods",
    "authors": "Mohammed Sabry, Anya Belz",
    "abstract": "  As the cost of training ever larger language models has grown, so has the\ninterest in reusing previously learnt knowledge. Transfer learning methods have\nshown how reusing non-task-specific knowledge can help in subsequent\ntask-specific learning. In this paper, we investigate the inverse: porting\nwhole functional modules that encode task-specific knowledge from one model to\nanother. We designed a study comprising 1,440 training/testing runs to test the\nportability of modules trained by parameter-efficient finetuning (PEFT)\ntechniques, using sentiment analysis as an example task. We test portability in\na wide range of scenarios, involving different PEFT techniques and different\npretrained host models, among other dimensions. We compare the performance of\nported modules with that of equivalent modules trained (i) from scratch, and\n(ii) from parameters sampled from the same distribution as the ported module.\nWe find that the ported modules far outperform the two alternatives tested, but\nthat there are interesting performance differences between the four PEFT\ntechniques. We conclude that task-specific knowledge in the form of\nstructurally modular sets of parameters as produced by PEFT techniques is\nhighly portable, but that degree of success depends on type of PEFT and on\ndifferences between originating and receiving pretrained models.\n",
    "link": "http://arxiv.org/abs/2401.14228v1"
  },
  {
    "title": "Commonsense-augmented Memory Construction and Management in Long-term\n  Conversations via Context-aware Persona Refinement",
    "authors": "Hana Kim, Kai Tzu-iunn Ong, Seoyeon Kim, Dongha Lee, Jinyoung Yeo",
    "abstract": "  Memorizing and utilizing speakers' personas is a common practice for response\ngeneration in long-term conversations. Yet, human-authored datasets often\nprovide uninformative persona sentences that hinder response quality. This\npaper presents a novel framework that leverages commonsense-based persona\nexpansion to address such issues in long-term conversation. While prior work\nfocuses on not producing personas that contradict others, we focus on\ntransforming contradictory personas into sentences that contain rich speaker\ninformation, by refining them based on their contextual backgrounds with\ndesigned strategies. As the pioneer of persona expansion in multi-session\nsettings, our framework facilitates better response generation via human-like\npersona refinement. The supplementary video of our work is available at\nhttps://caffeine-15bbf.web.app/.\n",
    "link": "http://arxiv.org/abs/2401.14215v1"
  },
  {
    "title": "Exploiting Liver CT scans in Colorectal Carcinoma genomics mutation\n  classification",
    "authors": "Daniele Perlo, Luca Berton, Alessia Delpiano, Francesca Menchini, Stefano Tibaldi, Marco Grosso, Paolo Fonio",
    "abstract": "  The liver is the most involved organ by distant metastasis in colon-rectal\ncancer (CRC) patients and it comes necessary to be aware of the mutational\nstatus of the lesions to correctly design the best individual treatment. So\nfar, efforts have been made in order to develop non-invasive and real-time\nmethods that permit the analysis of the whole tumor, using new artificial\nintelligence tools to analyze the tumor's image obtained by Computed Tomography\n(CT) scan. In order to address the current medical workflow, that is biopsy\nanalysis-based, we propose the first DeepLearning-based exploration, to our\nknowledge, of such classification approach from the patient medical imaging. We\npropose i) a solid pipeline for managing undersized datasets of available CT\nscans and ii) a baseline study for genomics mutation diagnosis support for\npreemptive patient follow-up. Our method is able to identify CRC RAS mutation\nfamily from CT images with 0.73 F1 score.\n",
    "link": "http://arxiv.org/abs/2401.14206v1"
  },
  {
    "title": "TDFNet: An Efficient Audio-Visual Speech Separation Model with Top-down\n  Fusion",
    "authors": "Samuel Pegg, Kai Li, Xiaolin Hu",
    "abstract": "  Audio-visual speech separation has gained significant traction in recent\nyears due to its potential applications in various fields such as speech\nrecognition, diarization, scene analysis and assistive technologies. Designing\na lightweight audio-visual speech separation network is important for\nlow-latency applications, but existing methods often require higher\ncomputational costs and more parameters to achieve better separation\nperformance. In this paper, we present an audio-visual speech separation model\ncalled Top-Down-Fusion Net (TDFNet), a state-of-the-art (SOTA) model for\naudio-visual speech separation, which builds upon the architecture of TDANet,\nan audio-only speech separation method. TDANet serves as the architectural\nfoundation for the auditory and visual networks within TDFNet, offering an\nefficient model with fewer parameters. On the LRS2-2Mix dataset, TDFNet\nachieves a performance increase of up to 10\\% across all performance metrics\ncompared with the previous SOTA method CTCNet. Remarkably, these results are\nachieved using fewer parameters and only 28\\% of the multiply-accumulate\noperations (MACs) of CTCNet. In essence, our method presents a highly effective\nand efficient solution to the challenges of speech separation within the\naudio-visual domain, making significant strides in harnessing visual\ninformation optimally.\n",
    "link": "http://arxiv.org/abs/2401.14185v1"
  },
  {
    "title": "Copilot Refinement: Addressing Code Smells in Copilot-Generated Python\n  Code",
    "authors": "Beiqi Zhang, Peng Liang, Qiong Feng, Yujia Fu, Zengyang Li",
    "abstract": "  As one of the most popular dynamic languages, Python experiences a decrease\nin readability and maintainability when code smells are present. Recent\nadvancements in Large Language Models have sparked growing interest in\nAI-enabled tools for both code generation and refactoring. GitHub Copilot is\none such tool that has gained widespread usage. Copilot Chat, released on\nSeptember 2023, functions as an interactive tool aims at facilitating natural\nlanguage-powered coding. However, limited attention has been given to\nunderstanding code smells in Copilot-generated Python code and Copilot's\nability to fix the code smells it generates. To this end, we built a dataset\ncomprising 102 code smells in Copilot-generated Python code. Our aim is to\nfirst explore the occurrence of code smells in Copilot-generated Python code\nand then evaluate the effectiveness of Copilot in fixing these code smells\nemploying different prompts. The results show that 8 out of 10 types of Python\nsmells can be detected in Copilot-generated Python code, among which\nMultiply-Nested Container is the most common one. For these code smells,\nCopilot Chat achieves a highest fixing rate of 87.1%, showing promise in fixing\nPython code smells generated by Copilot itself. Besides, the effectiveness of\nCopilot Chat in fixing these smells can be improved with the provision of more\ndetailed prompts. However, using Copilot Chat to fix these smells might\nintroduce new code smells.\n",
    "link": "http://arxiv.org/abs/2401.14176v1"
  },
  {
    "title": "The Boundaries of Tractability in Hierarchical Task Network Planning",
    "authors": "Cornelius Brand, Robert Ganian, Fionn Mc Inerney, Simon Wietheger",
    "abstract": "  We study the complexity-theoretic boundaries of tractability for three\nclassical problems in the context of Hierarchical Task Network Planning: the\nvalidation of a provided plan, whether an executable plan exists, and whether a\ngiven state can be reached by some plan. We show that all three problems can be\nsolved in polynomial time on primitive task networks of constant partial order\nwidth (and a generalization thereof), whereas for the latter two problems this\nholds only under a provably necessary restriction to the state space. Next, we\nobtain an algorithmic meta-theorem along with corresponding lower bounds to\nidentify tight conditions under which general polynomial-time solvability\nresults can be lifted from primitive to general task networks. Finally, we\nenrich our investigation by analyzing the parameterized complexity of the three\nconsidered problems, and show that (1) fixed-parameter tractability for all\nthree problems can be achieved by replacing the partial order width with the\nvertex cover number of the network as the parameter, and (2) other classical\ngraph-theoretic parameters of the network (including treewidth, treedepth, and\nthe aforementioned partial order width) do not yield fixed-parameter\ntractability for any of the three problems.\n",
    "link": "http://arxiv.org/abs/2401.14174v1"
  },
  {
    "title": "Predicting Hypoxia in Brain Tumors from Multiparametric MRI",
    "authors": "Daniele Perlo, Georgia Kanli, Selma Boudissa, Olivier Keunen",
    "abstract": "  This research paper presents a novel approach to the prediction of hypoxia in\nbrain tumors, using multi-parametric Magnetic Resonance Imaging (MRI). Hypoxia,\na condition characterized by low oxygen levels, is a common feature of\nmalignant brain tumors associated with poor prognosis. Fluoromisonidazole\nPositron Emission Tomography (FMISO PET) is a well-established method for\ndetecting hypoxia in vivo, but it is expensive and not widely available. Our\nstudy proposes the use of MRI, a more accessible and cost-effective imaging\nmodality, to predict FMISO PET signals. We investigate deep learning models\n(DL) trained on the ACRIN 6684 dataset, a resource that contains paired MRI and\nFMISO PET images from patients with brain tumors. Our trained models\neffectively learn the complex relationships between the MRI features and the\ncorresponding FMISO PET signals, thereby enabling the prediction of hypoxia\nfrom MRI scans alone. The results show a strong correlation between the\npredicted and actual FMISO PET signals, with an overall PSNR score above 29.6\nand a SSIM score greater than 0.94, confirming MRI as a promising option for\nhypoxia prediction in brain tumors. This approach could significantly improve\nthe accessibility of hypoxia detection in clinical settings, with the potential\nfor more timely and targeted treatments.\n",
    "link": "http://arxiv.org/abs/2401.14171v1"
  },
  {
    "title": "BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on\n  Few-shot Inference via Debiased Domain Abstraction",
    "authors": "Jiangmeng Li, Fei Song, Yifan Jin, Wenwen Qiang, Changwen Zheng, Fuchun Sun, Hui Xiong",
    "abstract": "  As a novel and effective fine-tuning paradigm based on large-scale\npre-trained language models (PLMs), prompt-tuning aims to reduce the gap\nbetween downstream tasks and pre-training objectives. While prompt-tuning has\nyielded continuous advancements in various tasks, such an approach still\nremains a persistent defect: prompt-tuning methods fail to generalize to\nspecific few-shot patterns. From the perspective of distribution analyses, we\ndisclose that the intrinsic issues behind the phenomenon are the\nover-multitudinous conceptual knowledge contained in PLMs and the abridged\nknowledge for target downstream domains, which jointly result in that PLMs\nmis-locate the knowledge distributions corresponding to the target domains in\nthe universal knowledge embedding space. To this end, we intuitively explore to\napproximate the unabridged target domains of downstream tasks in a debiased\nmanner, and then abstract such domains to generate discriminative prompts,\nthereby providing the de-ambiguous guidance for PLMs. Guided by such an\nintuition, we propose a simple yet effective approach, namely BayesPrompt, to\nlearn prompts that contain the domain discriminative information against the\ninterference from domain-irrelevant knowledge. BayesPrompt primitively\nleverages known distributions to approximate the debiased factual distributions\nof target domains and further uniformly samples certain representative features\nfrom the approximated distributions to generate the ultimate prompts for PLMs.\nWe provide theoretical insights with the connection to domain adaptation.\nEmpirically, our method achieves state-of-the-art performance on benchmarks.\n",
    "link": "http://arxiv.org/abs/2401.14166v1"
  },
  {
    "title": "Alleviating Structural Distribution Shift in Graph Anomaly Detection",
    "authors": "Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, Yongdong Zhang",
    "abstract": "  Graph anomaly detection (GAD) is a challenging binary classification problem\ndue to its different structural distribution between anomalies and normal nodes\n-- abnormal nodes are a minority, therefore holding high heterophily and low\nhomophily compared to normal nodes. Furthermore, due to various time factors\nand the annotation preferences of human experts, the heterophily and homophily\ncan change across training and testing data, which is called structural\ndistribution shift (SDS) in this paper. The mainstream methods are built on\ngraph neural networks (GNNs), benefiting the classification of normals from\naggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and\nsuffering from poor generalization.\n  This work solves the problem from a feature view. We observe that the degree\nof SDS varies between anomalies and normal nodes. Hence to address the issue,\nthe key lies in resisting high heterophily for anomalies meanwhile benefiting\nthe learning of normals from homophily. We tease out the anomaly features on\nwhich we constrain to mitigate the effect of heterophilous neighbors and make\nthem invariant. We term our proposed framework as Graph Decomposition Network\n(GDN). Extensive experiments are conducted on two benchmark datasets, and the\nproposed framework achieves a remarkable performance boost in GAD, especially\nin an SDS environment where anomalies have largely different structural\ndistribution across training and testing environments. Codes are open-sourced\nin https://github.com/blacksingular/wsdm_GDN.\n",
    "link": "http://arxiv.org/abs/2401.14155v1"
  },
  {
    "title": "Agent-based Simulation with Netlogo to Evaluate AmI Scenarios",
    "authors": "J. Carbo, N. Sanchez, J. M. Molina",
    "abstract": "  In this paper an agent-based simulation is developed in order to evaluate an\nAmI scenario based on agents. Many AmI applications are implemented through\nagents but they are not compared to any other existing alternative in order to\nevaluate the relative benefits of using them. The proposal simulation\nenvironment developed in Netlogo analyse such benefits using two evaluation\ncriteria: First, measuring agent satisfaction of different types of desires\nalong the execution. Second, measuring time savings obtained through a correct\nuse of context information.\n  So, here, a previously suggested agent architecture, an ontology and a\n12-steps protocol to provide AmI services in airports, is evaluated using a\nNetLogo simulation environment. The present work uses a NetLogo model\nconsidering scalability problems of this application domain but using FIPA and\nBDI extensions to be coherent with our previous works and our previous JADE\nimplementation of them.\n  The NetLogo model presented simulates an airport with agent users passing\nthrough several zones located in a specific order in a map: passport controls,\ncheck-in counters of airline companies, boarding gates, different types of\nshopping. Although initial data in simulations are generated randomly, and the\nmodel is just an approximation of real-world airports, the definition of this\ncase of use of Ambient Intelligence through NetLogo agents opens an interesting\nway to evaluate the benefits of using Ambient Intelligence, which is a\nsignificant contribution to the final development of them.\n",
    "link": "http://arxiv.org/abs/2401.14153v1"
  },
  {
    "title": "True Knowledge Comes from Practice: Aligning LLMs with Embodied\n  Environments via Reinforcement Learning",
    "authors": "Weihao Tan, Wentao Zhang, Shanqi Liu, Longtao Zheng, Xinrun Wang, Bo An",
    "abstract": "  Despite the impressive performance across numerous tasks, large language\nmodels (LLMs) often fail in solving simple decision-making tasks due to the\nmisalignment of the knowledge in LLMs with environments. On the contrary,\nreinforcement learning (RL) agents learn policies from scratch, which makes\nthem always align with environments but difficult to incorporate prior\nknowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a\nnovel general online framework that deploys LLMs as decision-making agents to\nefficiently interact and align with embodied environments via RL without\nrequiring any prepared datasets or prior knowledge of the environments.\nFirstly, we query the joint probabilities of each valid action with LLMs to\nform behavior policies. Then, to enhance the stability and robustness of the\npolicies, we propose two normalization methods and summarize four prompt design\nprinciples. Finally, we design a novel parameter-efficient training\narchitecture where the actor and critic share one frozen LLM equipped with\nlow-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to\nevaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency\nand performance compared to the conventional RL method, PPO, and prompt tuning\nmethod, SayCan, in both classical decision-making environment, Overcooked, and\nsimulated household environment, VirtualHome. ii) Benefiting from LLMs'\nopen-vocabulary feature, TWOSOME shows superior generalization ability to\nunseen tasks. iii) Under our framework, there is no significant loss of the\nLLMs' original ability during online PPO finetuning.\n",
    "link": "http://arxiv.org/abs/2401.14151v1"
  },
  {
    "title": "Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept\n  Intervention, and Conditional Interpretations",
    "authors": "Xinyue Xu, Yi Qin, Lu Mi, Hao Wang, Xiaomeng Li",
    "abstract": "  Existing methods, such as concept bottleneck models (CBMs), have been\nsuccessful in providing concept-based interpretations for black-box deep\nlearning models. They typically work by predicting concepts given the input and\nthen predicting the final class label given the predicted concepts. However,\n(1) they often fail to capture the high-order, nonlinear interaction between\nconcepts, e.g., correcting a predicted concept (e.g., \"yellow breast\") does not\nhelp correct highly correlated concepts (e.g., \"yellow belly\"), leading to\nsuboptimal final accuracy; (2) they cannot naturally quantify the complex\nconditional dependencies between different concepts and class labels (e.g., for\nan image with the class label \"Kentucky Warbler\" and a concept \"black bill\",\nwhat is the probability that the model correctly predicts another concept\n\"black crown\"), therefore failing to provide deeper insight into how a\nblack-box model works. In response to these limitations, we propose\nEnergy-based Concept Bottleneck Models (ECBMs). Our ECBMs use a set of neural\nnetworks to define the joint energy of candidate (input, concept, class)\ntuples. With such a unified interface, prediction, concept correction, and\nconditional dependency quantification are then represented as conditional\nprobabilities, which are generated by composing different energy functions. Our\nECBMs address both limitations of existing CBMs, providing higher accuracy and\nricher concept interpretations. Empirical results show that our approach\noutperforms the state-of-the-art on real-world datasets.\n",
    "link": "http://arxiv.org/abs/2401.14142v1"
  },
  {
    "title": "FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric\n  Algorithm-System Co-Design",
    "authors": "Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon Song",
    "abstract": "  Six-bit quantization (FP6) can effectively reduce the size of large language\nmodels (LLMs) and preserve the model quality consistently across varied\napplications. However, existing systems do not provide Tensor Core support for\nFP6 quantization and struggle to achieve practical performance improvements\nduring LLM inference. It is challenging to support FP6 quantization on GPUs due\nto (1) unfriendly memory access of model weights with irregular bit-width and\n(2) high runtime overhead of weight de-quantization. To address these problems,\nwe propose TC-FPx, the first full-stack GPU kernel design scheme with unified\nTensor Core support of float-point weights for various quantization bit-width.\nWe integrate TC-FPx kernel into an existing inference system, providing new\nend-to-end support (called FP6-LLM) for quantized LLM inference, where better\ntrade-offs between inference cost and model quality are achieved. Experiments\nshow that FP6-LLM enables the inference of LLaMA-70b using only a single GPU,\nachieving 1.69x-2.65x higher normalized inference throughput than the FP16\nbaseline. The source code will be publicly available soon.\n",
    "link": "http://arxiv.org/abs/2401.14112v1"
  },
  {
    "title": "Towards Cheaper Inference in Deep Networks with Lower Bit-Width\n  Accumulators",
    "authors": "Yaniv Blumenfeld, Itay Hubara, Daniel Soudry",
    "abstract": "  The majority of the research on the quantization of Deep Neural Networks\n(DNNs) is focused on reducing the precision of tensors visible by high-level\nframeworks (e.g., weights, activations, and gradients). However, current\nhardware still relies on high-accuracy core operations. Most significant is the\noperation of accumulating products. This high-precision accumulation operation\nis gradually becoming the main computational bottleneck. This is because, so\nfar, the usage of low-precision accumulators led to a significant degradation\nin performance. In this work, we present a simple method to train and fine-tune\nhigh-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits\naccumulators, with no significant degradation in accuracy. Lastly, we show that\nas we decrease the accumulation precision further, using fine-grained gradient\napproximations can improve the DNN accuracy.\n",
    "link": "http://arxiv.org/abs/2401.14110v1"
  },
  {
    "title": "CompactifAI: Extreme Compression of Large Language Models using\n  Quantum-Inspired Tensor Networks",
    "authors": "Andrei Tomut, Saeed S. Jahromi, Sukhbinder Singh, Faysal Ishtiaq, Cesar Mu\u00f1oz, Prabdeep Singh Bajaj, Ali Elborady, Gianni del Bimbo, Mehrazin Alizadeh, David Montero, Pablo Martin-Ramiro, Muhammad Ibrahim, Oussama Tahiri Alaoui, John Malcolm, Samuel Mugel, Roman Orus",
    "abstract": "  Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly\nin generative Artificial Intelligence (AI), but their immense size poses\nsignificant challenges, such as huge training and inference costs, substantial\nenergy demands, and limitations for on-site deployment. Traditional compression\nmethods such as pruning, distillation, and low-rank approximation focus on\nreducing the effective number of neurons in the network, while quantization\nfocuses on reducing the numerical precision of individual weights to reduce the\nmodel size while keeping the number of neurons fixed. While these compression\nmethods have been relatively successful in practice, there's no compelling\nreason to believe that truncating the number of neurons is an optimal strategy.\nIn this context, this paper introduces CompactifAI, an innovative LLM\ncompression approach using quantum-inspired Tensor Networks that focuses on the\nmodel's correlation space instead, allowing for a more controlled, refined and\ninterpretable model compression. Our method is versatile and can be implemented\nwith - or on top of - other compression techniques. As a benchmark, we\ndemonstrate that CompactifAI alone enables compression of the LlaMA-2 7B model\nto only $30\\%$ of its original size while recovering over $90\\%$ of the\noriginal accuracy after a brief distributed retraining.\n",
    "link": "http://arxiv.org/abs/2401.14109v1"
  },
  {
    "title": "GQHAN: A Grover-inspired Quantum Hard Attention Network",
    "authors": "Ren-Xin Zhao, Jinjing Shi, Xuelong Li",
    "abstract": "  Numerous current Quantum Machine Learning (QML) models exhibit an inadequacy\nin discerning the significance of quantum data, resulting in diminished\nefficacy when handling extensive quantum datasets. Hard Attention Mechanism\n(HAM), anticipated to efficiently tackle the above QML bottlenecks, encounters\nthe substantial challenge of non-differentiability, consequently constraining\nits extensive applicability. In response to the dilemma of HAM and QML, a\nGrover-inspired Quantum Hard Attention Mechanism (GQHAM) consisting of a\nFlexible Oracle (FO) and an Adaptive Diffusion Operator (ADO) is proposed.\nNotably, the FO is designed to surmount the non-differentiable issue by\nexecuting the activation or masking of Discrete Primitives (DPs) with Flexible\nControl (FC) to weave various discrete destinies. Based on this, such discrete\nchoice can be visualized with a specially defined Quantum Hard Attention Score\n(QHAS). Furthermore, a trainable ADO is devised to boost the generality and\nflexibility of GQHAM. At last, a Grover-inspired Quantum Hard Attention Network\n(GQHAN) based on QGHAM is constructed on PennyLane platform for Fashion MNIST\nbinary classification. Experimental findings demonstrate that GQHAN adeptly\nsurmounts the non-differentiability hurdle, surpassing the efficacy of extant\nquantum soft self-attention mechanisms in accuracies and learning ability. In\nnoise experiments, GQHAN is robuster to bit-flip noise in accuracy and\namplitude damping noise in learning performance. Predictably, the proposal of\nGQHAN enriches the Quantum Attention Mechanism (QAM), lays the foundation for\nfuture quantum computers to process large-scale data, and promotes the\ndevelopment of quantum computer vision.\n",
    "link": "http://arxiv.org/abs/2401.14089v1"
  },
  {
    "title": "Generating Likely Counterfactuals Using Sum-Product Networks",
    "authors": "Jiri Nemecek, Tomas Pevny, Jakub Marecek",
    "abstract": "  Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI\nsystems need to be explained. These decisions are often explainable only post\nhoc, where counterfactual explanations are popular. The question of what\nconstitutes the best counterfactual explanation must consider multiple aspects,\nwhere \"distance from the sample\" is the most common. We argue that this\nrequirement frequently leads to explanations that are unlikely and, therefore,\nof limited value. Here, we present a system that provides high-likelihood\nexplanations. We show that the search for the most likely explanations\nsatisfying many common desiderata for counterfactual explanations can be\nmodeled using mixed-integer optimization (MIO). In the process, we propose an\nMIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the\nlikelihood of a counterfactual, which can be of independent interest. A\nnumerical comparison against several methods for generating counterfactual\nexplanations is provided.\n",
    "link": "http://arxiv.org/abs/2401.14086v1"
  },
  {
    "title": "From Requirements to Architecture: An AI-Based Journey to\n  Semi-Automatically Generate Software Architectures",
    "authors": "Tobias Eisenreich, Sandro Speth, Stefan Wagner",
    "abstract": "  Designing domain models and software architectures represents a significant\nchallenge in software development, as the resulting architectures play a vital\nrole in fulfilling the system's quality of service. Due to time pressure,\narchitects often model only one architecture based on their known limited\ndomain understanding, patterns, and experience instead of thoroughly analyzing\nthe domain and evaluating multiple candidates, selecting the best fitting.\nExisting approaches try to generate domain models based on requirements, but\nstill require time-consuming manual effort to achieve good results. Therefore,\nin this vision paper, we propose a method to generate software architecture\ncandidates semi-automatically based on requirements using artificial\nintelligence techniques. We further envision an automatic evaluation and\ntrade-off analysis of the generated architecture candidates using, e.g., the\narchitecture trade-off analysis method combined with large language models and\nquantitative analyses. To evaluate this approach, we aim to analyze the quality\nof the generated architecture models and the efficiency and effectiveness of\nour proposed process by conducting qualitative studies.\n",
    "link": "http://arxiv.org/abs/2401.14079v1"
  },
  {
    "title": "Ta'keed: The First Generative Fact-Checking System for Arabic Claims",
    "authors": "Saud Althabiti, Mohammad Ammar Alsalka, Eric Atwell",
    "abstract": "  This paper introduces Ta'keed, an explainable Arabic automatic fact-checking\nsystem. While existing research often focuses on classifying claims as \"True\"\nor \"False,\" there is a limited exploration of generating explanations for claim\ncredibility, particularly in Arabic. Ta'keed addresses this gap by assessing\nclaim truthfulness based on retrieved snippets, utilizing two main components:\ninformation retrieval and LLM-based claim verification. We compiled the\nArFactEx, a testing gold-labelled dataset with manually justified references,\nto evaluate the system. The initial model achieved a promising F1 score of 0.72\nin the classification task. Meanwhile, the system's generated explanations are\ncompared with gold-standard explanations syntactically and semantically. The\nstudy recommends evaluating using semantic similarities, resulting in an\naverage cosine similarity score of 0.76. Additionally, we explored the impact\nof varying snippet quantities on claim classification accuracy, revealing a\npotential correlation, with the model using the top seven hits outperforming\nothers with an F1 score of 0.77.\n",
    "link": "http://arxiv.org/abs/2401.14067v1"
  },
  {
    "title": "CreativeSynth: Creative Blending and Synthesis of Visual Arts based on\n  Multimodal Diffusion",
    "authors": "Nisha Huang, Weiming Dong, Yuxin Zhang, Fan Tang, Ronghui Li, Chongyang Ma, Xiu Li, Changsheng Xu",
    "abstract": "  Large-scale text-to-image generative models have made impressive strides,\nshowcasing their ability to synthesize a vast array of high-quality images.\nHowever, adapting these models for artistic image editing presents two\nsignificant challenges. Firstly, users struggle to craft textual prompts that\nmeticulously detail visual elements of the input image. Secondly, prevalent\nmodels, when effecting modifications in specific zones, frequently disrupt the\noverall artistic style, complicating the attainment of cohesive and\naesthetically unified artworks. To surmount these obstacles, we build the\ninnovative unified framework CreativeSynth, which is based on a diffusion model\nwith the ability to coordinate multimodal inputs and multitask in the field of\nartistic image generation. By integrating multimodal features with customized\nattention mechanisms, CreativeSynth facilitates the importation of real-world\nsemantic content into the domain of art through inversion and real-time style\ntransfer. This allows for the precise manipulation of image style and content\nwhile maintaining the integrity of the original model parameters. Rigorous\nqualitative and quantitative evaluations underscore that CreativeSynth excels\nin enhancing artistic images' fidelity and preserves their innate aesthetic\nessence. By bridging the gap between generative models and artistic finesse,\nCreativeSynth becomes a custom digital palette.\n",
    "link": "http://arxiv.org/abs/2401.14066v1"
  },
  {
    "title": "Left/Right Brain, human motor control and the implications for robotics",
    "authors": "Jarrad Rinaldo, Levin Kuhlmann, Jason Friedman, Gideon Kowadlo",
    "abstract": "  Neural Network movement controllers promise a variety of advantages over\nconventional control methods however they are not widely adopted due to their\ninability to produce reliably precise movements. This research explores a\nbilateral neural network architecture as a control system for motor tasks. We\naimed to achieve hemispheric specialisation similar to what is observed in\nhumans across different tasks; the dominant system (usually the right hand,\nleft hemisphere) excels at tasks involving coordination and efficiency of\nmovement, and the non-dominant system performs better at tasks requiring\npositional stability. Specialisation was achieved by training the hemispheres\nwith different loss functions tailored toward the expected behaviour of the\nrespective hemispheres. We compared bilateral models with and without\nspecialised hemispheres, with and without inter-hemispheric connectivity\n(representing the biological Corpus Callosum), and unilateral models with and\nwithout specialisation. The models were trained and tested on two tasks common\nin the human motor control literature: the random reach task, suited to the\ndominant system, a model with better coordination, and the hold position task,\nsuited to the non-dominant system, a model with more stable movement. Each\nsystem out-performed the non-favoured system in its preferred task. For both\ntasks, a bilateral model outperforms the 'non-preferred' hand, and is as good\nor better than the 'preferred' hand. The Corpus Callosum tends to improve\nperformance, but not always for the specialised models.\n",
    "link": "http://arxiv.org/abs/2401.14057v1"
  },
  {
    "title": "Towards Goal-oriented Large Language Model Prompting: A Survey",
    "authors": "Haochen Li, Jonathan Leung, Zhiqi Shen",
    "abstract": "  Large Language Models (LLMs) have shown prominent performance in various\ndownstream tasks in which prompt engineering plays a pivotal role in optimizing\nLLMs' performance. This paper, not as an overview of current prompt engineering\nmethods, aims to highlight the limitation of designing prompts while holding an\nanthropomorphic assumption that expects LLMs to think like humans. From our\nreview of 35 representative studies, we demonstrate that a goal-oriented prompt\nformulation, which guides LLMs to follow established human logical thinking,\nsignificantly improves the performance of LLMs. Furthermore, We introduce a\nnovel taxonomy that categorizes goal-oriented prompting methods into five\ninterconnected stages and we demonstrate the broad applicability of our\nframework by summarizing ten applicable tasks. With four future directions\nproposed, we hope to further emphasize and promote goal-oriented prompt\nengineering.\n",
    "link": "http://arxiv.org/abs/2401.14043v1"
  },
  {
    "title": "GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D\n  Reconstruction Dataset Using Gaussian Splatting",
    "authors": "Butian Xiong, Zhuo Li, Zhen Li",
    "abstract": "  We introduce a novel large-scale scene reconstruction benchmark using the\nnewly developed 3D representation approach, Gaussian Splatting, on our\nexpansive U-Scene dataset. U-Scene encompasses over one and a half square\nkilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground\ntruth. For data acquisition, we employed the Matrix 300 drone equipped with the\nhigh-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This\ndataset, offers a unique blend of urban and academic environments for advanced\nspatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with\nGaussian Splatting includes a detailed analysis across various novel\nviewpoints. We also juxtapose these results with those derived from our\naccurate point cloud dataset, highlighting significant differences that\nunderscore the importance of combine multi-modal information\n",
    "link": "http://arxiv.org/abs/2401.14032v1"
  },
  {
    "title": "Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation\n  for Generative AI",
    "authors": "Elron Bandel, Yotam Perlitz, Elad Venezian, Roni Friedman-Melamed, Ofir Arviv, Matan Orbach, Shachar Don-Yehyia, Dafna Sheinwald, Ariel Gera, Leshem Choshen, Michal Shmueli-Scheuer, Yoav Katz",
    "abstract": "  In the dynamic landscape of generative NLP, traditional text processing\npipelines limit research flexibility and reproducibility, as they are tailored\nto specific dataset, task, and model combinations. The escalating complexity,\ninvolving system prompts, model-specific formats, instructions, and more, calls\nfor a shift to a structured, modular, and customizable solution. Addressing\nthis need, we present Unitxt, an innovative library for customizable textual\ndata preparation and evaluation tailored to generative language models. Unitxt\nnatively integrates with common libraries like HuggingFace and LM-eval-harness\nand deconstructs processing flows into modular components, enabling easy\ncustomization and sharing between practitioners. These components encompass\nmodel-specific formats, task prompts, and many other comprehensive dataset\nprocessing definitions. The Unitxt-Catalog centralizes these components,\nfostering collaboration and exploration in modern textual data workflows.\nBeyond being a tool, Unitxt is a community-driven platform, empowering users to\nbuild, share, and advance their pipelines collaboratively. Join the Unitxt\ncommunity at https://github.com/IBM/unitxt!\n",
    "link": "http://arxiv.org/abs/2401.14019v1"
  },
  {
    "title": "CMMU: A Benchmark for Chinese Multi-modal Multi-type Question\n  Understanding and Reasoning",
    "authors": "Zheqi He, Xinya Wu, Pengfei Zhou, Richeng Xuan, Guang Liu, Xi Yang, Qiannan Zhu, Hua Huang",
    "abstract": "  Multi-modal large language models(MLLMs) have achieved remarkable progress\nand demonstrated powerful knowledge comprehension and reasoning abilities.\nHowever, the mastery of domain-specific knowledge, which is essential for\nevaluating the intelligence of MLLMs, continues to be a challenge. Current\nmulti-modal benchmarks for domain-specific knowledge concentrate on\nmultiple-choice questions and are predominantly available in English, which\nimposes limitations on the comprehensiveness of the evaluation. To this end, we\nintroduce CMMU, a novel benchmark for multi-modal and multi-type question\nunderstanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7\nsubjects, covering knowledge from primary to high school. The questions can be\ncategorized into 3 types: multiple-choice, multiple-response, and\nfill-in-the-blank, bringing greater challenges to MLLMs. In addition, we\npropose a rigorous evaluation strategy called ShiftCheck for assessing\nmultiple-choice questions. The strategy aims to reduce position bias, minimize\nthe influence of randomness on correctness, and perform a quantitative analysis\nof position bias. We evaluate seven open-source MLLMs along with GPT4-V,\nGemini-Pro, and Qwen-VL-Plus. The results demonstrate that CMMU poses a\nsignificant challenge to the recent MLLMs.\n",
    "link": "http://arxiv.org/abs/2401.14011v1"
  },
  {
    "title": "ConstraintChecker: A Plugin for Large Language Models to Reason on\n  Commonsense Knowledge Bases",
    "authors": "Quyet V. Do, Tianqing Fang, Shizhe Diao, Zhaowei Wang, Yangqiu Song",
    "abstract": "  Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has\nbeen explored as a way to acquire new commonsense knowledge based on reference\nknowledge in the original CSKBs and external prior knowledge. Despite the\nadvancement of Large Language Models (LLM) and prompt engineering techniques in\nvarious reasoning tasks, they still struggle to deal with CSKB reasoning. One\nof the problems is that it is hard for them to acquire explicit relational\nconstraints in CSKBs from only in-context exemplars, due to a lack of symbolic\nreasoning capabilities (Bengio et al., 2021). To this end, we proposed\n**ConstraintChecker**, a plugin over prompting techniques to provide and check\nexplicit constraints. When considering a new knowledge instance,\nConstraintChecker employs a rule-based module to produce a list of constraints,\nthen it uses a zero-shot learning module to check whether this knowledge\ninstance satisfies all constraints. The acquired constraint-checking result is\nthen aggregated with the output of the main prompting technique to produce the\nfinal output. Experimental results on CSKB Reasoning benchmarks demonstrate the\neffectiveness of our method by bringing consistent improvements over all\nprompting methods. Codes and data are available at\n\\url{https://github.com/HKUST-KnowComp/ConstraintChecker}.\n",
    "link": "http://arxiv.org/abs/2401.14003v1"
  },
  {
    "title": "Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent\n  Self-Evolution",
    "authors": "Cheng Qian, Shihao Liang, Yujia Qin, Yining Ye, Xin Cong, Yankai Lin, Yesai Wu, Zhiyuan Liu, Maosong Sun",
    "abstract": "  This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy\nfor enhancing the adaptability and flexibility of AI agents through inter-task\nself-evolution. Unlike existing methods focused on intra-task learning, ICE\npromotes the transfer of knowledge between tasks for genuine self-evolution,\nsimilar to human experience learning. The strategy dynamically investigates\nplanning and execution trajectories, consolidates them into simplified\nworkflows and pipelines, and exploits them for improved task execution. Our\nexperiments on the XAgent framework demonstrate ICE's effectiveness, reducing\nAPI calls by as much as 80% and significantly decreasing the demand for the\nmodel's capability. Specifically, when combined with GPT-3.5, ICE's performance\nmatches that of raw GPT-4 across various agent tasks. We argue that this\nself-evolution approach represents a paradigm shift in agent design,\ncontributing to a more robust AI community and ecosystem, and moving a step\ncloser to full autonomy.\n",
    "link": "http://arxiv.org/abs/2401.13996v1"
  },
  {
    "title": "Cross-Domain Few-Shot Learning via Adaptive Transformer Networks",
    "authors": "Naeem Paeedeh, Mahardhika Pratama, Muhammad Anwar Ma'sum, Wolfgang Mayer, Zehong Cao, Ryszard Kowlczyk",
    "abstract": "  Most few-shot learning works rely on the same domain assumption between the\nbase and the target tasks, hindering their practical applications. This paper\nproposes an adaptive transformer network (ADAPTER), a simple but effective\nsolution for cross-domain few-shot learning where there exist large domain\nshifts between the base task and the target task. ADAPTER is built upon the\nidea of bidirectional cross-attention to learn transferable features between\nthe two domains. The proposed architecture is trained with DINO to produce\ndiverse, and less biased features to avoid the supervision collapse problem.\nFurthermore, the label smoothing approach is proposed to improve the\nconsistency and reliability of the predictions by also considering the\npredicted labels of the close samples in the embedding space. The performance\nof ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it\noutperforms prior arts with significant margins.\n",
    "link": "http://arxiv.org/abs/2401.13987v1"
  },
  {
    "title": "Towards Consistent Natural-Language Explanations via\n  Explanation-Consistency Finetuning",
    "authors": "Yanda Chen, Chandan Singh, Xiaodong Liu, Simiao Zuo, Bin Yu, He He, Jianfeng Gao",
    "abstract": "  Large language models (LLMs) often generate convincing, fluent explanations.\nHowever, different from humans, they often generate inconsistent explanations\non different inputs. For example, an LLM may generate the explanation \"all\nbirds can fly\" when answering the question \"Can sparrows fly?\" but meanwhile\nanswer \"no\" to the related question \"Can penguins fly?\". Explanations should be\nconsistent across related examples so that they allow a human to simulate the\nLLM's decision process on multiple examples. We propose explanation-consistency\nfinetuning (EC-finetuning), a method that adapts LLMs to generate more\nconsistent natural-language explanations on related examples. EC-finetuning\ninvolves finetuning LLMs on synthetic data that is carefully constructed to\ncontain consistent explanations. Across a variety of question-answering\ndatasets in various domains, EC-finetuning yields a 10.0% relative explanation\nconsistency improvement on four finetuning datasets, and generalizes to seven\nout-of-distribution datasets not seen during finetuning (+4.5% relative). Code\nis available at https://github.com/yandachen/explanation-consistency-finetuning .\n",
    "link": "http://arxiv.org/abs/2401.13986v1"
  },
  {
    "title": "Leeroo Orchestrator: Elevating LLMs Performance Through Model\n  Integration",
    "authors": "Alireza Mohammadshahi, Ali Shaikh, Majid Yazdani",
    "abstract": "  In this paper, we propose an architecture to harness the collective knowledge\nof multiple trained LLMs to create a new state-of-the-art. At the core of this\nframework is a LLM-based orchestrator that is adept at picking the right\nunderlying LLM experts for optimal task execution. Inspired by self-play in\nreinforcement learning, we created a loop of query generation, orchestration,\nand evaluation to generate training data for the orchestrator. Our evaluation\nfocused on the MMLU benchmark, employing models with 7B, 13B, and 34B\nparameters available on Hugging Face. The results demonstrate new\nstate-of-the-art open-source models: Our Leeroo orchestrator achieves\nperformance on par with the Mixtral model while incurring only two-thirds of\nits cost. Moreover, increasing the allowed cost surpasses Mixtral's accuracy by\nover 5% at the same cost level, reaching an accuracy of 75.9%. Further\nenhancements were observed when integrating GPT4 into the underlying model\npool. The Leeroo orchestrator nearly matches GPT4's performance at half the\ncost and even exceeds GPT4's results with a 25% cost reduction. These findings\nillustrate the potential of our architecture in creating state-of-the-art and\ncost-effective LLMs by optimizing the synergy between multiple LLMs to achieve\nsuperior performance outcomes.\n",
    "link": "http://arxiv.org/abs/2401.13979v1"
  },
  {
    "title": "Learning to Manipulate Artistic Images",
    "authors": "Wei Guo, Yuqi Zhang, De Ma, Qian Zheng",
    "abstract": "  Recent advancement in computer vision has significantly lowered the barriers\nto artistic creation. Exemplar-based image translation methods have attracted\nmuch attention due to flexibility and controllability. However, these methods\nhold assumptions regarding semantics or require semantic information as the\ninput, while accurate semantics is not easy to obtain in artistic images.\nBesides, these methods suffer from cross-domain artifacts due to training data\nprior and generate imprecise structure due to feature compression in the\nspatial domain. In this paper, we propose an arbitrary Style Image Manipulation\nNetwork (SIM-Net), which leverages semantic-free information as guidance and a\nregion transportation strategy in a self-supervised manner for image\ngeneration. Our method balances computational efficiency and high resolution to\na certain extent. Moreover, our method facilitates zero-shot style image\nmanipulation. Both qualitative and quantitative experiments demonstrate the\nsuperiority of our method over state-of-the-art methods.Code is available at\nhttps://github.com/SnailForce/SIM-Net.\n",
    "link": "http://arxiv.org/abs/2401.13976v1"
  },
  {
    "title": "BootPIG: Bootstrapping Zero-shot Personalized Image Generation\n  Capabilities in Pretrained Diffusion Models",
    "authors": "Senthil Purushwalkam, Akash Gokul, Shafiq Joty, Nikhil Naik",
    "abstract": "  Recent text-to-image generation models have demonstrated incredible success\nin generating images that faithfully follow input prompts. However, the\nrequirement of using words to describe a desired concept provides limited\ncontrol over the appearance of the generated concepts. In this work, we address\nthis shortcoming by proposing an approach to enable personalization\ncapabilities in existing text-to-image diffusion models. We propose a novel\narchitecture (BootPIG) that allows a user to provide reference images of an\nobject in order to guide the appearance of a concept in the generated images.\n  The proposed BootPIG architecture makes minimal modifications to a pretrained\ntext-to-image diffusion model and utilizes a separate UNet model to steer the\ngenerations toward the desired appearance. We introduce a training procedure\nthat allows us to bootstrap personalization capabilities in the BootPIG\narchitecture using data generated from pretrained text-to-image models, LLM\nchat agents, and image segmentation models. In contrast to existing methods\nthat require several days of pretraining, the BootPIG architecture can be\ntrained in approximately 1 hour. Experiments on the DreamBooth dataset\ndemonstrate that BootPIG outperforms existing zero-shot methods while being\ncomparable with test-time finetuning approaches. Through a user study, we\nvalidate the preference for BootPIG generations over existing methods both in\nmaintaining fidelity to the reference object's appearance and aligning with\ntextual prompts.\n",
    "link": "http://arxiv.org/abs/2401.13974v1"
  },
  {
    "title": "Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks",
    "authors": "Muhammad Anwar Ma'sum, MD Rasel Sarkar, Mahardhika Pratama, Savitha Ramasamy, Sreenatha Anavatti, Lin Liu,  Habibullah, Ryszard Kowalczyk",
    "abstract": "  A reliable long-term time-series forecaster is highly demanded in practice\nbut comes across many challenges such as low computational and memory\nfootprints as well as robustness against dynamic learning environments. This\npaper proposes Meta-Transformer Networks (MANTRA) to deal with the dynamic\nlong-term time-series forecasting tasks. MANTRA relies on the concept of fast\nand slow learners where a collection of fast learners learns different aspects\nof data distributions while adapting quickly to changes. A slow learner tailors\nsuitable representations to fast learners. Fast adaptations to dynamic\nenvironments are achieved using the universal representation transformer layers\nproducing task-adapted representations with a small number of parameters. Our\nexperiments using four datasets with different prediction lengths demonstrate\nthe advantage of our approach with at least $3\\%$ improvements over the\nbaseline algorithms for both multivariate and univariate settings. Source codes\nof MANTRA are publicly available in\n\\url{https://github.com/anwarmaxsum/MANTRA}.\n",
    "link": "http://arxiv.org/abs/2401.13968v1"
  },
  {
    "title": "General Automatic Solution Generation of Social Problems",
    "authors": "Tong Niu, Haoyu Huang, Yu Du, Weihao Zhang, Luping Shi, Rong Zhao",
    "abstract": "  Given the escalating intricacy and multifaceted nature of contemporary social\nsystems, manually generating solutions to address pertinent social issues has\nbecome a formidable task. In response to this challenge, the rapid development\nof artificial intelligence has spurred the exploration of computational\nmethodologies aimed at automatically generating solutions. However, current\nmethods for auto-generation of solutions mainly concentrate on local social\nregulations that pertain to specific scenarios. Here, we report an automatic\nsocial operating system (ASOS) designed for general social solution generation,\nwhich is built upon agent-based models, enabling both global and local analyses\nand regulations of social problems across spatial and temporal dimensions. ASOS\nadopts a hypergraph with extensible social semantics for a comprehensive and\nstructured representation of social dynamics. It also incorporates a\ngeneralized protocol for standardized hypergraph operations and a symbolic\nhybrid framework that delivers interpretable solutions, yielding a balance\nbetween regulatory efficacy and function viability. To demonstrate the\neffectiveness of ASOS, we apply it to the domain of averting extreme events\nwithin international oil futures markets. By generating a new trading role\nsupplemented by new mechanisms, ASOS can adeptly discern precarious market\nconditions and make front-running interventions for non-profit purposes. This\nstudy demonstrates that ASOS provides an efficient and systematic approach for\ngenerating solutions for enhancing our society.\n",
    "link": "http://arxiv.org/abs/2401.13945v1"
  },
  {
    "title": "A New Paradigm for Counterfactual Reasoning in Fairness and Recourse",
    "authors": "Lucius E. J. Bynum, Joshua R. Loftus, Julia Stoyanovich",
    "abstract": "  Counterfactuals and counterfactual reasoning underpin numerous techniques for\nauditing and understanding artificial intelligence (AI) systems. The\ntraditional paradigm for counterfactual reasoning in this literature is the\ninterventional counterfactual, where hypothetical interventions are imagined\nand simulated. For this reason, the starting point for causal reasoning about\nlegal protections and demographic data in AI is an imagined intervention on a\nlegally-protected characteristic, such as ethnicity, race, gender, disability,\nage, etc. We ask, for example, what would have happened had your race been\ndifferent? An inherent limitation of this paradigm is that some demographic\ninterventions -- like interventions on race -- may not translate into the\nformalisms of interventional counterfactuals. In this work, we explore a new\nparadigm based instead on the backtracking counterfactual, where rather than\nimagine hypothetical interventions on legally-protected characteristics, we\nimagine alternate initial conditions while holding these characteristics fixed.\nWe ask instead, what would explain a counterfactual outcome for you as you\nactually are or could be? This alternate framework allows us to address many of\nthe same social concerns, but to do so while asking fundamentally different\nquestions that do not rely on demographic interventions.\n",
    "link": "http://arxiv.org/abs/2401.13935v1"
  },
  {
    "title": "LocMoE: A Low-overhead MoE for Large Language Model Training",
    "authors": "Jing Li, Zhijie Sun, Xuan He, Li Zeng, Yi Lin, Entong Li, Binfan Zheng, Rongqian Zhao, Xin Chen",
    "abstract": "  The Mixtures-of-Experts (MoE) model is a widespread distributed and\nintegrated learning method for large language models (LLM), which is favored\ndue to its ability to sparsify and expand models efficiently. However, the\nperformance of MoE is limited by load imbalance and high latency of All-To-All\ncommunication, along with relatively redundant computation owing to large\nexpert capacity. Load imbalance may result from existing routing policies that\nconsistently tend to select certain experts. The frequent inter-node\ncommunication in the All-To-All procedure also significantly prolongs the\ntraining time. To alleviate the above performance problems, we propose a novel\nrouting strategy that combines load balance and locality by converting partial\ninter-node communication to that of intra-node. Notably, we elucidate that\nthere is a minimum threshold for expert capacity, calculated through the\nmaximal angular deviation between the gating weights of the experts and the\nassigned tokens. We port these modifications on the PanGu-Sigma model based on\nthe MindSpore framework with multi-level routing and conduct experiments on\nAscend clusters. The experiment results demonstrate that the proposed LocMoE\nreduces training time per epoch by 12.68% to 22.24% compared to classical\nrouters, such as hash router and switch router, without impacting the model\naccuracy.\n",
    "link": "http://arxiv.org/abs/2401.13920v1"
  },
  {
    "title": "WebVoyager: Building an End-to-End Web Agent with Large Multimodal\n  Models",
    "authors": "Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, Dong Yu",
    "abstract": "  The advancement of large language models (LLMs) leads to a new era marked by\nthe development of autonomous applications in the real world, which drives\ninnovation in the creation of advanced web-based agents. Existing web agents\ntypically only handle one input modality and are evaluated only in simplified\nweb simulators or static web snapshots, greatly limiting their applicability in\nreal-world scenarios. To bridge this gap, we introduce WebVoyager, an\ninnovative Large Multimodal Model (LMM) powered web agent that can complete\nuser instructions end-to-end by interacting with real-world websites. Moreover,\nwe propose a new evaluation protocol for web agents to address the challenges\nof automatic evaluation of open-ended web agent tasks, leveraging the robust\nmultimodal comprehension capabilities of GPT-4V. We create a new benchmark by\ngathering real-world tasks from 15 widely used websites to evaluate our agents.\nWe show that WebVoyager achieves a 55.7% task success rate, significantly\nsurpassing the performance of both GPT-4 (All Tools) and the WebVoyager\n(text-only) setups, underscoring the exceptional capability of WebVoyager in\npractical applications. We found that our proposed automatic evaluation\nachieves 85.3% agreement with human judgment, paving the way for further\ndevelopment of web agents in a real-world setting.\n",
    "link": "http://arxiv.org/abs/2401.13919v1"
  }
]