[
  {
    "title": "Synthesizing Moving People with 3D Control",
    "authors": "Boyi Li, Jathushan Rajasegaran, Yossi Gandelsman, Alexei A. Efros, Jitendra Malik",
    "abstract": "  In this paper, we present a diffusion model-based framework for animating\npeople from a single image for a given target 3D motion sequence. Our approach\nhas two core components: a) learning priors about invisible parts of the human\nbody and clothing, and b) rendering novel body poses with proper clothing and\ntexture. For the first part, we learn an in-filling diffusion model to\nhallucinate unseen parts of a person given a single image. We train this model\non texture map space, which makes it more sample-efficient since it is\ninvariant to pose and viewpoint. Second, we develop a diffusion-based rendering\npipeline, which is controlled by 3D human poses. This produces realistic\nrenderings of novel poses of the person, including clothing, hair, and\nplausible in-filling of unseen regions. This disentangled approach allows our\nmethod to generate a sequence of images that are faithful to the target motion\nin the 3D pose and, to the input image in terms of visual similarity. In\naddition to that, the 3D control allows various synthetic camera trajectories\nto render a person. Our experiments show that our method is resilient in\ngenerating prolonged motions and varied challenging and complex poses compared\nto prior methods. Please check our website for more details:\nhttps://boyiliee.github.io/3DHM.github.io/.\n",
    "link": "http://arxiv.org/abs/2401.10889v1"
  },
  {
    "title": "SCENES: Subpixel Correspondence Estimation With Epipolar Supervision",
    "authors": "Dominik A. Kloepfer, Jo\u00e3o F. Henriques, Dylan Campbell",
    "abstract": "  Extracting point correspondences from two or more views of a scene is a\nfundamental computer vision problem with particular importance for relative\ncamera pose estimation and structure-from-motion. Existing local feature\nmatching approaches, trained with correspondence supervision on large-scale\ndatasets, obtain highly-accurate matches on the test sets. However, they do not\ngeneralise well to new datasets with different characteristics to those they\nwere trained on, unlike classic feature extractors. Instead, they require\nfinetuning, which assumes that ground-truth correspondences or ground-truth\ncamera poses and 3D structure are available. We relax this assumption by\nremoving the requirement of 3D structure, e.g., depth maps or point clouds, and\nonly require camera pose information, which can be obtained from odometry. We\ndo so by replacing correspondence losses with epipolar losses, which encourage\nputative matches to lie on the associated epipolar line. While weaker than\ncorrespondence supervision, we observe that this cue is sufficient for\nfinetuning existing models on new data. We then further relax the assumption of\nknown camera poses by using pose estimates in a novel bootstrapping approach.\nWe evaluate on highly challenging datasets, including an indoor drone dataset\nand an outdoor smartphone camera dataset, and obtain state-of-the-art results\nwithout strong supervision.\n",
    "link": "http://arxiv.org/abs/2401.10886v1"
  },
  {
    "title": "Reinforcement learning for question answering in programming domain\n  using public community scoring as a human feedback",
    "authors": "Alexey Gorbatovski, Sergey Kovalchuk",
    "abstract": "  In this study, we investigate the enhancement of the GPT Neo 125M performance\nin Community Question Answering (CQA) with a focus on programming, through the\nintegration of Reinforcement Learning from Human Feedback (RLHF) and the\nutilization of scores from Stack Overflow. Two distinct reward model training\nstrategies are employed for fine-tuning with Proximal Policy Optimization\n(PPO). Notably, the improvements in performance achieved through this method\nare comparable to those of GPT Neo 2.7B parameter variant. Additionally, an\nauxiliary scoring mechanism is introduced, which demonstrates the limitations\nof conventional linguistic metrics in evaluating responses in the programming\ndomain. Through accurate analysis, this paper looks at the divergence between\ntraditional linguistic metrics and our human-preferences-based reward model,\nunderscoring the imperative for domain-specific evaluation methods. By\nelucidating the complexities involved in applying RLHF to programming CQA and\naccentuating the significance of context-aware evaluation, this study\ncontributes to the ongoing efforts in refining Large Language Models through\nfocused human feedback.\n",
    "link": "http://arxiv.org/abs/2401.10882v1"
  },
  {
    "title": "Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs\n  Without Fine-Tuning",
    "authors": "Adib Hasan, Ileana Rugina, Alex Wang",
    "abstract": "  Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type\nof attack that can coax these models into generating harmful and illegal\ncontent. In this paper, we show that pruning up to 20% of LLM parameters\nmarkedly increases their resistance to such attacks without additional training\nand without sacrificing their performance in standard benchmarks. Intriguingly,\nwe discovered that the enhanced safety observed post-pruning correlates to the\ninitial safety training level of the model, hinting that the effect of pruning\ncould be more general and may hold for other LLM behaviors beyond safety.\nAdditionally, we introduce a curated dataset of 225 harmful tasks across five\ncategories, inserted into ten different Jailbreaking prompts, showing that\npruning aids LLMs in concentrating attention on task-relevant tokens in\njailbreaking prompts. Lastly, our experiments reveal that the prominent chat\nmodels, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high\nsusceptibility to jailbreaking attacks, with some categories achieving nearly\n70-100% success rate. These insights underline the potential of pruning as a\ngeneralizable approach for improving LLM safety, reliability, and potentially\nother desired behaviors.\n",
    "link": "http://arxiv.org/abs/2401.10862v1"
  },
  {
    "title": "Advancements in eHealth Data Analytics through Natural Language\n  Processing and Deep Learning",
    "authors": "Elena-Simona Apostol, Ciprian-Octavian Truic\u0103",
    "abstract": "  The healthcare environment is commonly referred to as \"information-rich\" but\nalso \"knowledge poor\". Healthcare systems collect huge amounts of data from\nvarious sources: lab reports, medical letters, logs of medical tools or\nprograms, medical prescriptions, etc. These massive sets of data can provide\ngreat knowledge and information that can improve the medical services, and\noverall the healthcare domain, such as disease prediction by analyzing the\npatient's symptoms or disease prevention, by facilitating the discovery of\nbehavioral factors for diseases. Unfortunately, only a relatively small volume\nof the textual eHealth data is processed and interpreted, an important factor\nbeing the difficulty in efficiently performing Big Data operations. In the\nmedical field, detecting domain-specific multi-word terms is a crucial task as\nthey can define an entire concept with a few words. A term can be defined as a\nlinguistic structure or a concept, and it is composed of one or more words with\na specific meaning to a domain. All the terms of a domain create its\nterminology. This chapter offers a critical study of the current, most\nperformant solutions for analyzing unstructured (image and textual) eHealth\ndata. This study also provides a comparison of the current Natural Language\nProcessing and Deep Learning techniques in the eHealth context. Finally, we\nexamine and discuss some of the current issues, and we define a set of research\ndirections in this area.\n",
    "link": "http://arxiv.org/abs/2401.10850v1"
  },
  {
    "title": "Source-Free and Image-Only Unsupervised Domain Adaptation for Category\n  Level Object Pose Estimation",
    "authors": "Prakhar Kaushik, Aayush Mishra, Adam Kortylewski, Alan Yuille",
    "abstract": "  We consider the problem of source-free unsupervised category-level pose\nestimation from only RGB images to a target domain without any access to source\ndomain data or 3D annotations during adaptation. Collecting and annotating\nreal-world 3D data and corresponding images is laborious, expensive, yet\nunavoidable process, since even 3D pose domain adaptation methods require 3D\ndata in the target domain. We introduce 3DUDA, a method capable of adapting to\na nuisance-ridden target domain without 3D or depth data. Our key insight stems\nfrom the observation that specific object subparts remain stable across\nout-of-domain (OOD) scenarios, enabling strategic utilization of these\ninvariant subcomponents for effective model updates. We represent object\ncategories as simple cuboid meshes, and harness a generative model of neural\nfeature activations modeled at each mesh vertex learnt using differential\nrendering. We focus on individual locally robust mesh vertex features and\niteratively update them based on their proximity to corresponding features in\nthe target domain even when the global pose is not correct. Our model is then\ntrained in an EM fashion, alternating between updating the vertex features and\nthe feature extractor. We show that our method simulates fine-tuning on a\nglobal pseudo-labeled dataset under mild assumptions, which converges to the\ntarget domain asymptotically. Through extensive empirical validation, including\na complex extreme UDA setup which combines real nuisances, synthetic noise, and\nocclusion, we demonstrate the potency of our simple approach in addressing the\ndomain shift challenge and significantly improving pose estimation accuracy.\n",
    "link": "http://arxiv.org/abs/2401.10848v1"
  },
  {
    "title": "Using LLMs to discover emerging coded antisemitic hate-speech emergence\n  in extremist social media",
    "authors": "Dhanush Kikkisetti, Raza Ul Mustafa, Wendy Melillo, Roberto Corizzo, Zois Boukouvalas, Jeff Gill, Nathalie Japkowicz",
    "abstract": "  Online hate speech proliferation has created a difficult problem for social\nmedia platforms. A particular challenge relates to the use of coded language by\ngroups interested in both creating a sense of belonging for its users and\nevading detection. Coded language evolves quickly and its use varies over time.\nThis paper proposes a methodology for detecting emerging coded hate-laden\nterminology. The methodology is tested in the context of online antisemitic\ndiscourse. The approach considers posts scraped from social media platforms,\noften used by extremist users. The posts are scraped using seed expressions\nrelated to previously known discourse of hatred towards Jews. The method begins\nby identifying the expressions most representative of each post and calculating\ntheir frequency in the whole corpus. It filters out grammatically incoherent\nexpressions as well as previously encountered ones so as to focus on emergent\nwell-formed terminology. This is followed by an assessment of semantic\nsimilarity to known antisemitic terminology using a fine-tuned large language\nmodel, and subsequent filtering out of the expressions that are too distant\nfrom known expressions of hatred. Emergent antisemitic expressions containing\nterms clearly relating to Jewish topics are then removed to return only coded\nexpressions of hatred.\n",
    "link": "http://arxiv.org/abs/2401.10841v1"
  },
  {
    "title": "Understanding Video Transformers via Universal Concept Discovery",
    "authors": "Matthew Kowal, Achal Dave, Rares Ambrus, Adrien Gaidon, Konstantinos G. Derpanis, Pavel Tokmakov",
    "abstract": "  This paper studies the problem of concept-based interpretability of\ntransformer representations for videos. Concretely, we seek to explain the\ndecision-making process of video transformers based on high-level,\nspatiotemporal concepts that are automatically discovered. Prior research on\nconcept-based interpretability has concentrated solely on image-level tasks.\nComparatively, video models deal with the added temporal dimension, increasing\ncomplexity and posing challenges in identifying dynamic concepts over time. In\nthis work, we systematically address these challenges by introducing the first\nVideo Transformer Concept Discovery (VTCD) algorithm. To this end, we propose\nan efficient approach for unsupervised identification of units of video\ntransformer representations - concepts, and ranking their importance to the\noutput of a model. The resulting concepts are highly interpretable, revealing\nspatio-temporal reasoning mechanisms and object-centric representations in\nunstructured video models. Performing this analysis jointly over a diverse set\nof supervised and self-supervised representations, we discover that some of\nthese mechanism are universal in video transformers. Finally, we demonstrate\nthat VTCDcan be used to improve model performance for fine-grained tasks.\n",
    "link": "http://arxiv.org/abs/2401.10831v1"
  },
  {
    "title": "Optimisation in Neurosymbolic Learning Systems",
    "authors": "Emile van Krieken",
    "abstract": "  Neurosymbolic AI aims to integrate deep learning with symbolic AI. This\nintegration has many promises, such as decreasing the amount of data required\nto train a neural network, improving the explainability and interpretability of\nanswers given by models and verifying the correctness of trained systems. We\nstudy neurosymbolic learning, where we have both data and background knowledge\nexpressed using symbolic languages. How do we connect the symbolic and neural\ncomponents to communicate this knowledge? One option is fuzzy reasoning, which\nstudies degrees of truth. For example, being tall is not a binary concept.\nInstead, probabilistic reasoning studies the probability that something is true\nor will happen. Our first research question studies how different forms of\nfuzzy reasoning combine with learning. We find surprising results like a\nconnection to the Raven paradox stating we confirm \"ravens are black\" when we\nobserve a green apple. In this study, we did not use the background knowledge\nwhen we deployed our models after training. In our second research question, we\nstudied how to use background knowledge in deployed models. We developed a new\nneural network layer based on fuzzy reasoning. Probabilistic reasoning is a\nnatural fit for neural networks, which we usually train to be probabilistic.\nHowever, they are expensive to compute and do not scale well to large tasks. In\nour third research question, we study how to connect probabilistic reasoning\nwith neural networks by sampling to estimate averages, while in the final\nresearch question, we study scaling probabilistic neurosymbolic learning to\nmuch larger problems than before. Our insight is to train a neural network with\nsynthetic data to predict the result of probabilistic reasoning.\n",
    "link": "http://arxiv.org/abs/2401.10819v1"
  },
  {
    "title": "Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve\n  Health Outcomes",
    "authors": "Jodi Chiam, Aloysius Lim, Cheryl Nott, Nicholas Mark, Ankur Teredesai, Sunil Shinde",
    "abstract": "  The ability to shape health behaviors of large populations automatically,\nacross wearable types and disease conditions at scale has tremendous potential\nto improve global health outcomes. We designed and implemented an AI driven\nplatform for digital algorithmic nudging, enabled by a Graph-Neural Network\n(GNN) based Recommendation System, and granular health behavior data from\nwearable fitness devices. Here we describe the efficacy results of this\nplatform with its capabilities of personalized and contextual nudging to\n$n=84,764$ individuals over a 12-week period in Singapore. We statistically\nvalidated that participants in the target group who received such AI optimized\ndaily nudges increased daily physical activity like step count by 6.17% ($p =\n3.09\\times10^{-4}$) and weekly minutes of Moderate to Vigorous Physical\nActivity (MVPA) by 7.61% ($p = 1.16\\times10^{-2}$), compared to matched\nparticipants in control group who did not receive any nudges. Further, such\nnudges were very well received, with a 13.1% of nudges sent being opened (open\nrate), and 11.7% of the opened nudges rated useful compared to 1.9% rated as\nnot useful thereby demonstrating significant improvement in population level\nengagement metrics.\n",
    "link": "http://arxiv.org/abs/2401.10816v1"
  },
  {
    "title": "Learning to Visually Connect Actions and their Effects",
    "authors": "Eric Peh, Paritosh Parmar, Basura Fernando",
    "abstract": "  In this work, we introduce the novel concept of visually Connecting Actions\nand Their Effects (CATE) in video understanding. CATE can have applications in\nareas like task planning and learning from demonstration. We propose different\nCATE-based task formulations, such as action selection and action\nspecification, where video understanding models connect actions and effects at\nsemantic and fine-grained levels. We observe that different formulations\nproduce representations capturing intuitive action properties. We also design\nvarious baseline models for action selection and action specification. Despite\nthe intuitive nature of the task, we observe that models struggle, and humans\noutperform them by a large margin. The study aims to establish a foundation for\nfuture efforts, showcasing the flexibility and versatility of connecting\nactions and effects in video understanding, with the hope of inspiring advanced\nformulations and models.\n",
    "link": "http://arxiv.org/abs/2401.10805v1"
  },
  {
    "title": "Metric Dynamic Equilibrium Logic",
    "authors": "Arvid Becker, Pedro Cabalar, Mart\u00edn Di\u00e9guez, Luis Fari\u00f1as, Torsten Schaub, Anna Schuhmann",
    "abstract": "  In temporal extensions of Answer Set Programming (ASP) based on linear-time,\nthe behavior of dynamic systems is captured by sequences of states. While this\nrepresentation reflects their relative order, it abstracts away the specific\ntimes associated with each state. In many applications, however, timing\nconstraints are important like, for instance, when planning and scheduling go\nhand in hand. We address this by developing a metric extension of linear-time\nDynamic Equilibrium Logic, in which dynamic operators are constrained by\nintervals over integers. The resulting Metric Dynamic Equilibrium Logic\nprovides the foundation of an ASP-based approach for specifying qualitative and\nquantitative dynamic constraints. As such, it constitutes the most general\namong a whole spectrum of temporal extensions of Equilibrium Logic. In detail,\nwe show that it encompasses Temporal, Dynamic, Metric, and regular Equilibrium\nLogic, as well as its classic counterparts once the law of the excluded middle\nis added.\n",
    "link": "http://arxiv.org/abs/2401.10781v1"
  },
  {
    "title": "Interactions with Prompt Problems: A New Way to Teach Programming with\n  Large Language Models",
    "authors": "James Prather, Paul Denny, Juho Leinonen, David H. Smith IV, Brent N. Reeves, Stephen MacNeil, Brett A. Becker, Andrew Luxton-Reilly, Thezyrie Amarouche, Bailey Kimmel",
    "abstract": "  Large Language Models (LLMs) have upended decades of pedagogy in computing\neducation. Students previously learned to code through \\textit{writing} many\nsmall problems with less emphasis on code reading and comprehension. Recent\nresearch has shown that free code generation tools powered by LLMs can solve\nintroductory programming problems presented in natural language with ease. In\nthis paper, we propose a new way to teach programming with Prompt Problems.\nStudents receive a problem visually, indicating how input should be transformed\nto output, and must translate that to a prompt for an LLM to decipher. The\nproblem is considered correct when the code that is generated by the student\nprompt can pass all test cases. In this paper we present the design of this\ntool, discuss student interactions with it as they learn, and provide insights\ninto this new class of programming problems as well as the design tools that\nintegrate LLMs.\n",
    "link": "http://arxiv.org/abs/2401.10759v1"
  },
  {
    "title": "BoolGebra: Attributed Graph-learning for Boolean Algebraic Manipulation",
    "authors": "Yingjie Li, Anthony Agnesina, Yanqing Zhang, Haoxing Ren, Cunxi Yu",
    "abstract": "  Boolean algebraic manipulation is at the core of logic synthesis in\nElectronic Design Automation (EDA) design flow. Existing methods struggle to\nfully exploit optimization opportunities, and often suffer from an explosive\nsearch space and limited scalability efficiency. This work presents BoolGebra,\na novel attributed graph-learning approach for Boolean algebraic manipulation\nthat aims to improve fundamental logic synthesis. BoolGebra incorporates Graph\nNeural Networks (GNNs) and takes initial feature embeddings from both\nstructural and functional information as inputs. A fully connected neural\nnetwork is employed as the predictor for direct optimization result\npredictions, significantly reducing the search space and efficiently locating\nthe optimization space. The experiments involve training the BoolGebra model\nw.r.t design-specific and cross-design inferences using the trained model,\nwhere BoolGebra demonstrates generalizability for cross-design inference and\nits potential to scale from small, simple training datasets to large, complex\ninference datasets. Finally, BoolGebra is integrated with existing synthesis\ntool ABC to perform end-to-end logic minimization evaluation w.r.t SOTA\nbaselines.\n",
    "link": "http://arxiv.org/abs/2401.10753v1"
  },
  {
    "title": "EFO: the Emotion Frame Ontology",
    "authors": "Stefano De Giorgis, Aldo Gangemi",
    "abstract": "  Emotions are a subject of intense debate in various disciplines. Despite the\nproliferation of theories and definitions, there is still no consensus on what\nemotions are, and how to model the different concepts involved when we talk\nabout - or categorize - them. In this paper, we propose an OWL frame-based\nontology of emotions: the Emotion Frames Ontology (EFO). EFO treats emotions as\nsemantic frames, with a set of semantic roles that capture the different\naspects of emotional experience. EFO follows pattern-based ontology design, and\nis aligned to the DOLCE foundational ontology. EFO is used to model multiple\nemotion theories, which can be cross-linked as modules in an Emotion Ontology\nNetwork. In this paper, we exemplify it by modeling Ekman's Basic Emotions (BE)\nTheory as an EFO-BE module, and demonstrate how to perform automated inferences\non the representation of emotion situations. EFO-BE has been evaluated by\nlexicalizing the BE emotion frames from within the Framester knowledge graph,\nand implementing a graph-based emotion detector from text. In addition, an EFO\nintegration of multimodal datasets, including emotional speech and emotional\nface expressions, has been performed to enable further inquiry into crossmodal\nemotion semantics.\n",
    "link": "http://arxiv.org/abs/2401.10751v1"
  },
  {
    "title": "A Systematic Evaluation of Euclidean Alignment with Deep Learning for\n  EEG Decoding",
    "authors": "Bruna Junqueira, Bruno Aristimunha, Sylvain Chevallier, Raphael Y. de Camargo",
    "abstract": "  Electroencephalography (EEG) signals are frequently used for various\nBrain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have\nshown promising results, they are hindered by the substantial data\nrequirements. By leveraging data from multiple subjects, transfer learning\nenables more effective training of DL models. A technique that is gaining\npopularity is Euclidean Alignment (EA) due to its ease of use, low\ncomputational complexity, and compatibility with Deep Learning models. However,\nfew studies evaluate its impact on the training performance of shared and\nindividual DL models. In this work, we systematically evaluate the effect of EA\ncombined with DL for decoding BCI signals. We used EA to train shared models\nwith data from multiple subjects and evaluated its transferability to new\nsubjects. Our experimental results show that it improves decoding in the target\nsubject by 4.33% and decreases convergence time by more than 70%. We also\ntrained individual models for each subject to use as a majority-voting ensemble\nclassifier. In this scenario, using EA improved the 3-model ensemble accuracy\nby 3.7%. However, when compared to the shared model with EA, the ensemble\naccuracy was 3.62% lower.\n",
    "link": "http://arxiv.org/abs/2401.10746v1"
  },
  {
    "title": "FinLLMs: A Framework for Financial Reasoning Dataset Generation with\n  Large Language Models",
    "authors": "Ziqiang Yuan, Kaiyuan Wang, Shoutai Zhu, Ye Yuan, Jingya Zhou, Yanlin Zhu, Wenqi Wei",
    "abstract": "  Large Language models (LLMs) usually rely on extensive training datasets. In\nthe financial domain, creating numerical reasoning datasets that include a mix\nof tables and long text often involves substantial manual annotation expenses.\nTo address the limited data resources and reduce the annotation cost, we\nintroduce FinLLMs, a method for generating financial question-answering data\nbased on common financial formulas using Large Language Models. First, we\ncompile a list of common financial formulas and construct a graph based on the\nvariables these formulas employ. We then augment the formula set by combining\nthose that share identical variables as new elements. Specifically, we explore\nformulas obtained by manual annotation and merge those formulas with shared\nvariables by traversing the constructed graph. Finally, utilizing GPT-3.5, we\ngenerate financial question-answering data that encompasses both tabular\ninformation and long textual content, building on the collected formula set.\nOur experiments demonstrate that synthetic data generated by FinLLMs\neffectively enhances the performance of several large-scale numerical reasoning\nmodels in the financial domain, outperforming two established benchmark\nfinancial question-answering datasets.\n",
    "link": "http://arxiv.org/abs/2401.10744v1"
  },
  {
    "title": "Dynamic Q&amp;A of Clinical Documents with Large Language Models",
    "authors": "Ran Elgedawy, Sudarshan Srinivasan, Ioana Danciu",
    "abstract": "  Electronic health records (EHRs) house crucial patient data in clinical\nnotes. As these notes grow in volume and complexity, manual extraction becomes\nchallenging. This work introduces a natural language interface using large\nlanguage models (LLMs) for dynamic question-answering on clinical notes. Our\nchatbot, powered by Langchain and transformer-based LLMs, allows users to query\nin natural language, receiving relevant answers from clinical notes.\nExperiments, utilizing various embedding models and advanced LLMs, show Wizard\nVicuna's superior accuracy, albeit with high compute demands. Model\noptimization, including weight quantization, improves latency by approximately\n48 times. Promising results indicate potential, yet challenges such as model\nhallucinations and limited diverse medical case evaluations remain. Addressing\nthese gaps is crucial for unlocking the value in clinical notes and advancing\nAI-driven clinical decision-making.\n",
    "link": "http://arxiv.org/abs/2401.10733v1"
  },
  {
    "title": "Proceedings 14th International Conference on Automated Deduction in\n  Geometry",
    "authors": "Pedro Quaresma, Zolt\u00e1n Kov\u00e1cs",
    "abstract": "  ADG is a forum to exchange ideas and views, to present research results and\nprogress, and to demonstrate software tools at the intersection between\ngeometry and automated deduction. The conference is held every two years. The\nprevious editions of ADG were held in Hagenberg in 2021 (online, postponed from\n2020 due to COVID-19), Nanning in 2018, Strasbourg in 2016, Coimbra in 2014,\nEdinburgh in 2012, Munich in 2010, Shanghai in 2008, Pontevedra in 2006,\nGainesville in 2004, Hagenberg in 2002, Zurich in 2000, Beijing in 1998, and\nToulouse in 1996.\n  The 14th edition, ADG 2023, was held in Belgrade, Serbia, in September 20-22,\n2023. This edition of ADG had an additional special focus topic, Deduction in\nEducation.\n  Invited Speakers: Julien Narboux, University of Strasbourg, France\n\"Formalisation, arithmetization and automatisation of geometry\"; Filip Mari\\'c,\nUniversity of Belgrade, Serbia, \"Automatization, formalization and\nvisualization of hyperbolic geometry\"; Zlatan Magajna, University of Ljubljana,\nSlovenia, \"Workshop OK Geometry\"\n",
    "link": "http://arxiv.org/abs/2401.10725v1"
  },
  {
    "title": "Q&amp;A Prompts: Discovering Rich Visual Clues through Mining\n  Question-Answer Prompts for VQA requiring Diverse World Knowledge",
    "authors": "Haibi Wang, Weifeng Ge",
    "abstract": "  With the breakthrough of multi-modal large language models, answering complex\nvisual questions that demand advanced reasoning abilities and world knowledge\nhas become a much more important testbed for developing AI models than ever.\nHowever, equipping AI models with robust cross-modality reasoning ability\nremains challenging since the cognition scheme of humans has not been\nunderstood systematically. In this paper, we believe that if we can collect\nvisual clues in the given image as much as possible, we will recognize the\nimage more accurately, understand the question better, recall relevant\nknowledge more easily, and finally reason out the answer. We discover these\nrich visual clues by mining question-answer pairs in images and sending them\ninto multi-modal large language models as prompts. We call the proposed method\nQ&amp;A Prompts. Specifically, we first use the image-answer pairs and the\ncorresponding questions in the training set as inputs and outputs to train a\nvisual question generation model. Then, we use an image tagging model to\nidentify various instances and send packaged image-tag pairs into the visual\nquestion generation model to generate relevant questions with the extracted\nimage tags as answers. Finally, we encode these generated question-answer pairs\nas prompts with a visual-aware prompting module and send them into pre-trained\nmulti-modal large language models to reason out the final answers. Experimental\nresults show that, compared with state-of-the-art methods, our Q&amp;A Prompts\nachieves substantial improvements on the challenging visual question answering\ndatasets requiring reasoning over diverse world knowledge, such as OK-VQA and\nA-OKVQA.\n",
    "link": "http://arxiv.org/abs/2401.10712v1"
  },
  {
    "title": "Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal\n  Models for Video Question Answering",
    "authors": "Haibo Wang, Chenghang Lai, Yixuan Sun, Weifeng Ge",
    "abstract": "  Video Question Answering (VideoQA) aims to answer natural language questions\nbased on the information observed in videos. Despite the recent success of\nLarge Multimodal Models (LMMs) in image-language understanding and reasoning,\nthey deal with VideoQA insufficiently by simply taking uniformly sampled frames\nas visual inputs, which ignores question-relevant visual clues. Moreover, there\nare no human annotations for question-critical timestamps in existing VideoQA\ndatasets. In light of this, we propose a novel weakly supervised framework to\nenforce the LMMs to reason out the answers with question-critical moments as\nvisual inputs. Specifically, we fuse the question and answer pairs as event\ndescriptions to find multiple keyframes as target moments, which will be\npseudo-labels. With these pseudo-labels as additionally weak supervision, we\ndevise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG\nlearns multiple Gaussian functions to characterize the temporal structure of\nthe video, and sample question-critical frames as positive moments to be the\nvisual inputs of LMMs. Extensive experiments on several VideoQA benchmarks\nverify the effectiveness of our framework, and we achieve substantial\nimprovements compared to previous state-of-the-art methods.\n",
    "link": "http://arxiv.org/abs/2401.10711v1"
  },
  {
    "title": "Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion\n  Model",
    "authors": "Yinan Zheng, Jianxiong Li, Dongjie Yu, Yujie Yang, Shengbo Eben Li, Xianyuan Zhan, Jingjing Liu",
    "abstract": "  Safe offline RL is a promising way to bypass risky online interactions\ntowards safe policy learning. Most existing methods only enforce soft\nconstraints, i.e., constraining safety violations in expectation below\nthresholds predetermined. This can lead to potentially unsafe outcomes, thus\nunacceptable in safety-critical scenarios. An alternative is to enforce the\nhard constraint of zero violation. However, this can be challenging in offline\nsetting, as it needs to strike the right balance among three highly intricate\nand correlated aspects: safety constraint satisfaction, reward maximization,\nand behavior regularization imposed by offline datasets. Interestingly, we\ndiscover that via reachability analysis of safe-control theory, the hard safety\nconstraint can be equivalently translated to identifying the largest feasible\nregion given the offline dataset. This seamlessly converts the original trilogy\nproblem to a feasibility-dependent objective, i.e., maximizing reward value\nwithin the feasible region while minimizing safety risks in the infeasible\nregion. Inspired by these, we propose FISOR (FeasIbility-guided Safe Offline\nRL), which allows safety constraint adherence, reward maximization, and offline\npolicy learning to be realized via three decoupled processes, while offering\nstrong safety performance and stability. In FISOR, the optimal policy for the\ntranslated optimization problem can be derived in a special form of weighted\nbehavior cloning. Thus, we propose a novel energy-guided diffusion model that\ndoes not require training a complicated time-dependent classifier to extract\nthe policy, greatly simplifying the training. We compare FISOR against\nbaselines on DSRL benchmark for safe offline RL. Evaluation results show that\nFISOR is the only method that can guarantee safety satisfaction in all tasks,\nwhile achieving top returns in most tasks.\n",
    "link": "http://arxiv.org/abs/2401.10700v1"
  },
  {
    "title": "Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and\n  unfairness in dyadic regression models",
    "authors": "Jorge Paz-Ruza, Amparo Alonso-Betanzos, Bertha Guijarro-Berdi\u00f1as, Brais Cancela, Carlos Eiras-Franco",
    "abstract": "  Dyadic regression models, which predict real-valued outcomes for pairs of\nentities, are fundamental in many domains (e.g. predicting the rating of a user\nto a product in Recommender Systems) and promising and under exploration in\nmany others (e.g. approximating the adequate dosage of a drug for a patient in\npersonalized pharmacology). In this work, we demonstrate that non-uniformity in\nthe observed value distributions of individual entities leads to severely\nbiased predictions in state-of-the-art models, skewing predictions towards the\naverage of observed past values for the entity and providing worse-than-random\npredictive power in eccentric yet equally important cases. We show that the\nusage of global error metrics like Root Mean Squared Error (RMSE) and Mean\nAbsolute Error (MAE) is insufficient to capture this phenomenon, which we name\neccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as\na new complementary metric that can quantify it in all studied models and\ndatasets. We also prove the adequateness of EAUC by using naive de-biasing\ncorrections to demonstrate that a lower model bias correlates with a lower EAUC\nand vice-versa. This work contributes a bias-aware evaluation of dyadic\nregression models to avoid potential unfairness and risks in critical\nreal-world applications of such systems.\n",
    "link": "http://arxiv.org/abs/2401.10690v1"
  },
  {
    "title": "Towards End-to-End GPS Localization with Neural Pseudorange Correction",
    "authors": "Xu Weng, KV Ling, Haochen Liu, Kun Cao",
    "abstract": "  Pseudorange errors are the root cause of localization inaccuracy in GPS.\nPrevious data-driven methods regress and eliminate pseudorange errors using\nhandcrafted intermediate labels. Unlike them, we propose an end-to-end GPS\nlocalization framework, E2E-PrNet, to train a neural network for pseudorange\ncorrection (PrNet) directly using the final task loss calculated with the\nground truth of GPS receiver states. The gradients of the loss with respect to\nlearnable parameters are backpropagated through a differentiable nonlinear\nleast squares optimizer to PrNet. The feasibility is verified with GPS data\ncollected by Android phones, showing that E2E-PrNet outperforms the\nstate-of-the-art end-to-end GPS localization methods.\n",
    "link": "http://arxiv.org/abs/2401.10685v1"
  },
  {
    "title": "A Simple Framework to Accelerate Multilingual Language Model for\n  Monolingual Text Generation",
    "authors": "Jimin Hong, Gibbeum Lee, Jaewoong Cho",
    "abstract": "  Recent advancements in large language models have facilitated the execution\nof complex language tasks, not only in English but also in non-English\nlanguages. However, the tokenizers of most language models, such as Llama,\ntrained on English-centric corpora, tend to excessively fragment tokens in\nnon-English languages. This issue is especially pronounced in non-roman\nalphabetic languages, which are often divided at a character or even Unicode\nlevel, leading to slower text generation. To address this, our study introduces\na novel framework designed to expedite text generation in these languages. This\nframework predicts larger linguistic units than those of conventional\nmultilingual tokenizers and is specifically tailored to the target language,\nthereby reducing the number of decoding steps required. Our empirical results\ndemonstrate that the proposed framework increases the generation speed by a\nfactor of 1.9 compared to standard decoding while maintaining the performance\nof a pre-trained multilingual model on monolingual tasks.\n",
    "link": "http://arxiv.org/abs/2401.10660v1"
  },
  {
    "title": "A Comprehensive Survey on Deep-Learning-based Vehicle Re-Identification:\n  Models, Data Sets and Challenges",
    "authors": "Ali Amiri, Aydin Kaya, Ali Seydi Keceli",
    "abstract": "  Vehicle re-identification (ReID) endeavors to associate vehicle images\ncollected from a distributed network of cameras spanning diverse traffic\nenvironments. This task assumes paramount importance within the spectrum of\nvehicle-centric technologies, playing a pivotal role in deploying Intelligent\nTransportation Systems (ITS) and advancing smart city initiatives. Rapid\nadvancements in deep learning have significantly propelled the evolution of\nvehicle ReID technologies in recent years. Consequently, undertaking a\ncomprehensive survey of methodologies centered on deep learning for vehicle\nre-identification has become imperative and inescapable. This paper extensively\nexplores deep learning techniques applied to vehicle ReID. It outlines the\ncategorization of these methods, encompassing supervised and unsupervised\napproaches, delves into existing research within these categories, introduces\ndatasets and evaluation criteria, and delineates forthcoming challenges and\npotential research directions. This comprehensive assessment examines the\nlandscape of deep learning in vehicle ReID and establishes a foundation and\nstarting point for future works. It aims to serve as a complete reference by\nhighlighting challenges and emerging trends, fostering advancements and\napplications in vehicle ReID utilizing deep learning models.\n",
    "link": "http://arxiv.org/abs/2401.10643v1"
  },
  {
    "title": "Fast Butterfly-Core Community Search For Large Labeled Graphs",
    "authors": "JiaYi Du, Yinghao Wu, Wei Ai, Tao Meng, CanHao Xie, KeQin Li",
    "abstract": "  Community Search (CS) aims to identify densely interconnected subgraphs\ncorresponding to query vertices within a graph. However, existing heterogeneous\ngraph-based community search methods need help identifying cross-group\ncommunities and suffer from efficiency issues, making them unsuitable for large\ngraphs. This paper presents a fast community search model based on the\nButterfly-Core Community (BCC) structure for heterogeneous graphs. The Random\nWalk with Restart (RWR) algorithm and butterfly degree comprehensively evaluate\nthe importance of vertices within communities, allowing leader vertices to be\nrapidly updated to maintain cross-group cohesion. Moreover, we devised a more\nefficient method for updating vertex distances, which minimizes vertex visits\nand enhances operational efficiency. Extensive experiments on several\nreal-world temporal graphs demonstrate the effectiveness and efficiency of this\nsolution.\n",
    "link": "http://arxiv.org/abs/2401.10642v1"
  },
  {
    "title": "An Effective Index for Truss-based Community Search on Large Directed\n  Graphs",
    "authors": "Wei Ai, CanHao Xie, Tao Meng, Yinghao Wu, KeQin Li",
    "abstract": "  Community search is a derivative of community detection that enables online\nand personalized discovery of communities and has found extensive applications\nin massive real-world networks. Recently, there needs to be more focus on the\ncommunity search issue within directed graphs, even though substantial research\nhas been carried out on undirected graphs. The recently proposed D-truss model\nhas achieved good results in the quality of retrieved communities. However,\nexisting D-truss-based work cannot perform efficient community searches on\nlarge graphs because it consumes too many computing resources to retrieve the\nmaximal D-truss. To overcome this issue, we introduce an innovative merge\nrelation known as D-truss-connected to capture the inherent density and\ncohesiveness of edges within D-truss. This relation allows us to partition all\nthe edges in the original graph into a series of D-truss-connected classes.\nThen, we construct a concise and compact index, ConDTruss, based on\nD-truss-connected. Using ConDTruss, the efficiency of maximum D-truss retrieval\nwill be greatly improved, making it a theoretically optimal approach.\nExperimental evaluations conducted on large directed graph certificate the\neffectiveness of our proposed method.\n",
    "link": "http://arxiv.org/abs/2401.10641v1"
  },
  {
    "title": "A comprehensive study on fidelity metrics for XAI",
    "authors": "Miquel Mir\u00f3-Nicolau, Antoni Jaume-i-Cap\u00f3, Gabriel Moy\u00e0-Alcover",
    "abstract": "  The use of eXplainable Artificial Intelligence (XAI) systems has introduced a\nset of challenges that need resolution. Herein, we focus on how to correctly\nselect an XAI method, an open questions within the field. The inherent\ndifficulty of this task is due to the lack of a ground truth. Several authors\nhave proposed metrics to approximate the fidelity of different XAI methods.\nThese metrics lack verification and have concerning disagreements. In this\nstudy, we proposed a novel methodology to verify fidelity metrics, using a\nwell-known transparent model, namely a decision tree. This model allowed us to\nobtain explanations with perfect fidelity. Our proposal constitutes the first\nobjective benchmark for these metrics, facilitating a comparison of existing\nproposals, and surpassing existing methods. We applied our benchmark to assess\nthe existing fidelity metrics in two different experiments, each using public\ndatasets comprising 52,000 images. The images from these datasets had a size a\n128 by 128 pixels and were synthetic data that simplified the training process.\nAll metric values, indicated a lack of fidelity, with the best one showing a 30\n\\% deviation from the expected values for perfect explanation. Our\nexperimentation led us to conclude that the current fidelity metrics are not\nreliable enough to be used in real scenarios. From this finding, we deemed it\nnecessary to development new metrics, to avoid the detected problems, and we\nrecommend the usage of our proposal as a benchmark within the scientific\ncommunity to address these limitations.\n",
    "link": "http://arxiv.org/abs/2401.10640v1"
  },
  {
    "title": "ZnTrack -- Data as Code",
    "authors": "Fabian Zills, Moritz Sch\u00e4fer, Samuel Tovey, Johannes K\u00e4stner, Christian Holm",
    "abstract": "  The past decade has seen tremendous breakthroughs in computation and there is\nno indication that this will slow any time soon. Machine learning, large-scale\ncomputing resources, and increased industry focus have resulted in rising\ninvestments in computer-driven solutions for data management, simulations, and\nmodel generation. However, with this growth in computation has come an even\nlarger expansion of data and with it, complexity in data storage, sharing, and\ntracking. In this work, we introduce ZnTrack, a Python-driven data versioning\ntool. ZnTrack builds upon established version control systems to provide a\nuser-friendly and easy-to-use interface for tracking parameters in experiments,\ndesigning workflows, and storing and sharing data. From this ability to reduce\nlarge datasets to a simple Python script emerges the concept of Data as Code, a\ncore component of the work presented here and an undoubtedly important concept\nas the age of computation continues to evolve. ZnTrack offers an open-source,\nFAIR data compatible Python package to enable users to harness these concepts\nof the future.\n",
    "link": "http://arxiv.org/abs/2401.10603v1"
  },
  {
    "title": "Rethinking the Soft Conflict Pseudo Boolean Constraint on MaxSAT Local\n  Search Solvers",
    "authors": "Jiongzhi Zheng, Zhuo Chen, Chu-Min Li, Kun He",
    "abstract": "  MaxSAT is an optimization version of the famous NP-complete Satisfiability\nproblem (SAT). Algorithms for MaxSAT mainly include complete solvers and local\nsearch incomplete solvers. In many complete solvers, once a better solution is\nfound, a Soft conflict Pseudo Boolean (SPB) constraint will be generated to\nenforce the algorithm to find better solutions. In many local search\nalgorithms, clause weighting is a key technique for effectively guiding the\nsearch directions. In this paper, we propose to transfer the SPB constraint\ninto the clause weighting system of the local search method, leading the\nalgorithm to better solutions. We further propose an adaptive clause weighting\nstrategy that breaks the tradition of using constant values to adjust clause\nweights. Based on the above methods, we propose a new local search algorithm\ncalled SPB-MaxSAT that provides new perspectives for clause weighting on MaxSAT\nlocal search solvers. Extensive experiments demonstrate the excellent\nperformance of the proposed methods.\n",
    "link": "http://arxiv.org/abs/2401.10589v1"
  },
  {
    "title": "PuriDefense: Randomized Local Implicit Adversarial Purification for\n  Defending Black-box Query-based Attacks",
    "authors": "Ping Guo, Zhiyuan Yang, Xi Lin, Qingchuan Zhao, Qingfu Zhang",
    "abstract": "  Black-box query-based attacks constitute significant threats to Machine\nLearning as a Service (MLaaS) systems since they can generate adversarial\nexamples without accessing the target model's architecture and parameters.\nTraditional defense mechanisms, such as adversarial training, gradient masking,\nand input transformations, either impose substantial computational costs or\ncompromise the test accuracy of non-adversarial inputs. To address these\nchallenges, we propose an efficient defense mechanism, PuriDefense, that\nemploys random patch-wise purifications with an ensemble of lightweight\npurification models at a low level of inference cost. These models leverage the\nlocal implicit function and rebuild the natural image manifold. Our theoretical\nanalysis suggests that this approach slows down the convergence of query-based\nattacks by incorporating randomness into purifications. Extensive experiments\non CIFAR-10 and ImageNet validate the effectiveness of our proposed\npurifier-based defense mechanism, demonstrating significant improvements in\nrobustness against query-based attacks.\n",
    "link": "http://arxiv.org/abs/2401.10586v1"
  },
  {
    "title": "CivRealm: A Learning and Reasoning Odyssey in Civilization for\n  Decision-Making Agents",
    "authors": "Siyuan Qi, Shuo Chen, Yexin Li, Xiangyu Kong, Junqi Wang, Bangcheng Yang, Pring Wong, Yifan Zhong, Xiaoyuan Zhang, Zhaowei Zhang, Nian Liu, Wei Wang, Yaodong Yang, Song-Chun Zhu",
    "abstract": "  The generalization of decision-making agents encompasses two fundamental\nelements: learning from past experiences and reasoning in novel contexts.\nHowever, the predominant emphasis in most interactive environments is on\nlearning, often at the expense of complexity in reasoning. In this paper, we\nintroduce CivRealm, an environment inspired by the Civilization game.\nCivilization's profound alignment with human history and society necessitates\nsophisticated learning, while its ever-changing situations demand strong\nreasoning to generalize. Particularly, CivRealm sets up an\nimperfect-information general-sum game with a changing number of players; it\npresents a plethora of complex features, challenging the agent to deal with\nopen-ended stochastic environments that require diplomacy and negotiation\nskills. Within CivRealm, we provide interfaces for two typical agent types:\ntensor-based agents that focus on learning, and language-based agents that\nemphasize reasoning. To catalyze further research, we present initial results\nfor both paradigms. The canonical RL-based agents exhibit reasonable\nperformance in mini-games, whereas both RL- and LLM-based agents struggle to\nmake substantial progress in the full game. Overall, CivRealm stands as a\nunique learning and reasoning challenge for decision-making agents. The code is\navailable at https://github.com/bigai-ai/civrealm.\n",
    "link": "http://arxiv.org/abs/2401.10568v1"
  },
  {
    "title": "OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy",
    "authors": "Haowen Wang, Tao Sun, Kaixiang Ji, Jian Wang, Cong Fan, Jinjie Gu",
    "abstract": "  We advance the field of Parameter-Efficient Fine-Tuning (PEFT) with our novel\nmulti-adapter method, OrchMoE, which capitalizes on modular skill architecture\nfor enhanced forward transfer in neural networks. Unlike prior models that\ndepend on explicit task identification inputs, OrchMoE automatically discerns\ntask categories, streamlining the learning process. This is achieved through an\nintegrated mechanism comprising an Automatic Task Classification module and a\nTask-Skill Allocation module, which collectively deduce task-specific\nclassifications and tailor skill allocation matrices. Our extensive evaluations\non the 'Super Natural Instructions' dataset, featuring 1,600 diverse\ninstructional tasks, indicate that OrchMoE substantially outperforms comparable\nmulti-adapter baselines in terms of both performance and sample utilization\nefficiency, all while operating within the same parameter constraints. These\nfindings suggest that OrchMoE offers a significant leap forward in multi-task\nlearning efficiency.\n",
    "link": "http://arxiv.org/abs/2401.10559v1"
  },
  {
    "title": "AAT: Adapting Audio Transformer for Various Acoustics Recognition Tasks",
    "authors": "Yun Liang, Hai Lin, Shaojian Qiu, Yihang Zhang",
    "abstract": "  Recently, Transformers have been introduced into the field of acoustics\nrecognition. They are pre-trained on large-scale datasets using methods such as\nsupervised learning and semi-supervised learning, demonstrating robust\ngenerality--It fine-tunes easily to downstream tasks and shows more robust\nperformance. However, the predominant fine-tuning method currently used is\nstill full fine-tuning, which involves updating all parameters during training.\nThis not only incurs significant memory usage and time costs but also\ncompromises the model's generality. Other fine-tuning methods either struggle\nto address this issue or fail to achieve matching performance. Therefore, we\nconducted a comprehensive analysis of existing fine-tuning methods and proposed\nan efficient fine-tuning approach based on Adapter tuning, namely AAT. The core\nidea is to freeze the audio Transformer model and insert extra learnable\nAdapters, efficiently acquiring downstream task knowledge without compromising\nthe model's original generality. Extensive experiments have shown that our\nmethod achieves performance comparable to or even superior to full fine-tuning\nwhile optimizing only 7.118% of the parameters. It also demonstrates\nsuperiority over other fine-tuning methods.\n",
    "link": "http://arxiv.org/abs/2401.10544v1"
  },
  {
    "title": "Mementos: A Comprehensive Benchmark for Multimodal Large Language Model\n  Reasoning over Image Sequences",
    "authors": "Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, Huaxiu Yao, Furong Huang",
    "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated proficiency in\nhandling a variety of visual-language tasks. However, current MLLM benchmarks\nare predominantly designed to evaluate reasoning based on static information\nabout a single image, and the ability of modern MLLMs to extrapolate from image\nsequences, which is essential for understanding our ever-changing world, has\nbeen less investigated. To address this challenge, this paper introduces\nMementos, a new benchmark designed to assess MLLMs' sequential image reasoning\nabilities. Mementos features 4,761 diverse image sequences with varying\nlengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning\nperformance. Through a careful evaluation of nine recent MLLMs on Mementos,\nincluding GPT-4V and Gemini, we find that they struggle to accurately describe\ndynamic information about given image sequences, often leading to\nhallucinations/misrepresentations of objects and their corresponding behaviors.\nOur quantitative analysis and case studies identify three key factors impacting\nMLLMs' sequential image reasoning: the correlation between object and\nbehavioral hallucinations, the influence of cooccurring behaviors, and the\ncompounding impact of behavioral hallucinations. Our dataset is available at\nhttps://github.com/umd-huang-lab/Mementos.\n",
    "link": "http://arxiv.org/abs/2401.10529v1"
  },
  {
    "title": "Cross-lingual Editing in Multilingual Language Models",
    "authors": "Himanshu Beniwal, Kowsik Nandagopan D, Mayank Singh",
    "abstract": "  The training of large language models (LLMs) necessitates substantial data\nand computational resources, and updating outdated LLMs entails significant\nefforts and resources. While numerous model editing techniques (METs) have\nemerged to efficiently update model outputs without retraining, their\neffectiveness in multilingual LLMs, where knowledge is stored in diverse\nlanguages, remains an underexplored research area. This research paper\nintroduces the cross-lingual model editing (\\textbf{XME}) paradigm, wherein a\nfact is edited in one language, and the subsequent update propagation is\nobserved across other languages. To investigate the XME paradigm, we conducted\nexperiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts:\n\\textit{Latin} (English, French, and Spanish) and \\textit{Indic} (Hindi,\nGujarati, and Bengali). The results reveal notable performance limitations of\nstate-of-the-art METs under the XME setting, mainly when the languages involved\nbelong to two distinct script families. These findings highlight the need for\nfurther research and development of XME techniques to address these challenges.\nFor more comprehensive information, the dataset used in this research and the\nassociated code are publicly available at the following\nURL\\url{https://github.com/lingo-iitgn/XME}.\n",
    "link": "http://arxiv.org/abs/2401.10521v1"
  },
  {
    "title": "Episodic Reinforcement Learning with Expanded State-reward Space",
    "authors": "Dayang Liang, Yaru Zhang, Yunlong Liu",
    "abstract": "  Empowered by deep neural networks, deep reinforcement learning (DRL) has\ndemonstrated tremendous empirical successes in various domains, including\ngames, health care, and autonomous driving. Despite these advancements, DRL is\nstill identified as data-inefficient as effective policies demand vast numbers\nof environmental samples. Recently, episodic control (EC)-based model-free DRL\nmethods enable sample efficiency by recalling past experiences from episodic\nmemory. However, existing EC-based methods suffer from the limitation of\npotential misalignment between the state and reward spaces for neglecting the\nutilization of (past) retrieval states with extensive information, which\nprobably causes inaccurate value estimation and degraded policy performance. To\ntackle this issue, we introduce an efficient EC-based DRL framework with\nexpanded state-reward space, where the expanded states used as the input and\nthe expanded rewards used in the training both contain historical and current\ninformation. To be specific, we reuse the historical states retrieved by EC as\npart of the input states and integrate the retrieved MC-returns into the\nimmediate reward in each interactive transition. As a result, our method is\nable to simultaneously achieve the full utilization of retrieval information\nand the better evaluation of state values by a Temporal Difference (TD) loss.\nEmpirical results on challenging Box2d and Mujoco tasks demonstrate the\nsuperiority of our method over a recent sibling method and common baselines.\nFurther, we also verify our method's effectiveness in alleviating Q-value\noverestimation by additional experiments of Q-value comparison.\n",
    "link": "http://arxiv.org/abs/2401.10516v1"
  },
  {
    "title": "A match made in consistency heaven: when large language models meet\n  evolutionary algorithms",
    "authors": "Wang Chao, Jiaxuan Zhao, Licheng Jiao, Lingling Li, Fang Liu, Shuyuan Yang",
    "abstract": "  Pre-trained large language models (LLMs) have powerful capabilities for\ngenerating creative natural text. Evolutionary algorithms (EAs) can discover\ndiverse solutions to complex real-world problems. Motivated by the common\ncollective and directionality of text sequence generation and evolution, this\npaper illustrates the strong consistency of LLMs and EAs, which includes\nmultiple one-to-one key characteristics: token embedding and genotype-phenotype\nmapping, position encoding and fitness shaping, position embedding and\nselection, attention and crossover, feed-forward neural network and mutation,\nmodel training and parameter update, and multi-task learning and\nmulti-objective optimization. Based on this consistency perspective, existing\ncoupling studies are analyzed, including evolutionary fine-tuning and\nLLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap\nfor future research in coupling LLMs and EAs, while highlighting key challenges\nalong the way. The consistency not only reveals the evolution mechanism behind\nLLMs but also facilitates the development of evolved artificial agents that\napproach or surpass biological organisms.\n",
    "link": "http://arxiv.org/abs/2401.10510v1"
  },
  {
    "title": "FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial\n  Analysis",
    "authors": "Chao Zhang, Yuren Mao, Yijiang Fan, Yu Mi, Yunjun Gao, Lu Chen, Dongfang Lou, Jinshu Lin",
    "abstract": "  Text-to-SQL, which provides zero-code interface for operating relational\ndatabases, has gained much attention in financial analysis; because, financial\nprofessionals may not well-skilled in SQL programming. However, until now,\nthere is no practical Text-to-SQL benchmark dataset for financial analysis, and\nexisting Text-to-SQL methods have not considered the unique characteristics of\ndatabases in financial applications, such as commonly existing wide tables. To\naddress these issues, we collect a practical Text-to-SQL benchmark dataset and\npropose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL\nframework for financial analysis. The benchmark dataset, BULL, is collected\nfrom the practical financial analysis business of Hundsun Technologies Inc.,\nincluding databases for fund, stock, and macro economy. Besides, the proposed\nLLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for\nfinancial Text-to-SQL from the perspectives of prompt construction,\nparameter-efficient fine-tuning and output calibration. Extensive experimental\nresults on BULL demonstrate that FinSQL achieves the state-of-the-art\nText-to-SQL performance at a small cost; furthermore, FinSQL can bring up to\n36.64% performance improvement in scenarios requiring few-shot cross-database\nmodel transfer.\n",
    "link": "http://arxiv.org/abs/2401.10506v1"
  },
  {
    "title": "Causal Layering via Conditional Entropy",
    "authors": "Itai Feigenbaum, Devansh Arpit, Huan Wang, Shelby Heinecke, Juan Carlos Niebles, Weiran Yao, Caiming Xiong, Silvio Savarese",
    "abstract": "  Causal discovery aims to recover information about an unobserved causal graph\nfrom the observable data it generates. Layerings are orderings of the variables\nwhich place causes before effects. In this paper, we provide ways to recover\nlayerings of a graph by accessing the data via a conditional entropy oracle,\nwhen distributions are discrete. Our algorithms work by repeatedly removing\nsources or sinks from the graph. Under appropriate assumptions and\nconditioning, we can separate the sources or sinks from the remainder of the\nnodes by comparing their conditional entropy to the unconditional entropy of\ntheir noise. Our algorithms are provably correct and run in worst-case\nquadratic time. The main assumptions are faithfulness and injective noise, and\neither known noise entropies or weakly monotonically increasing noise entropies\nalong directed paths. In addition, we require one of either a very mild\nextension of faithfulness, or strictly monotonically increasing noise\nentropies, or expanding noise injectivity to include an additional single\nargument in the structural functions.\n",
    "link": "http://arxiv.org/abs/2401.10495v1"
  },
  {
    "title": "Enhancing Scalability in Recommender Systems through Lottery Ticket\n  Hypothesis and Knowledge Distillation-based Neural Network Pruning",
    "authors": "Rajaram R, Manoj Bharadhwaj, Vasan VS, Nargis Pervin",
    "abstract": "  This study introduces an innovative approach aimed at the efficient pruning\nof neural networks, with a particular focus on their deployment on edge\ndevices. Our method involves the integration of the Lottery Ticket Hypothesis\n(LTH) with the Knowledge Distillation (KD) framework, resulting in the\nformulation of three distinct pruning models. These models have been developed\nto address scalability issue in recommender systems, whereby the complexities\nof deep learning models have hindered their practical deployment. With\njudicious application of the pruning techniques, we effectively curtail the\npower consumption and model dimensions without compromising on accuracy.\nEmpirical evaluation has been performed using two real world datasets from\ndiverse domains against two baselines. Gratifyingly, our approaches yielded a\nGPU computation-power reduction of up to 66.67%. Notably, our study contributes\nto the field of recommendation system by pioneering the application of LTH and\nKD.\n",
    "link": "http://arxiv.org/abs/2401.10484v1"
  },
  {
    "title": "Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step\n  Reasoning",
    "authors": "Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin Sun, Heda Wang, Kan Li",
    "abstract": "  Self-consistency (SC) has been a widely used decoding strategy for\nchain-of-thought reasoning. Despite bringing significant performance\nimprovements across a variety of multi-step reasoning tasks, it is a high-cost\nmethod that requires multiple sampling with the preset size. In this paper, we\npropose a simple and scalable sampling process, \\textbf{E}arly-Stopping\n\\textbf{S}elf-\\textbf{C}onsistency (ESC), to greatly reduce the cost of SC\nwithout sacrificing performance. On this basis, one control scheme for ESC is\nfurther derivated to dynamically choose the performance-cost balance for\ndifferent tasks and models. To demonstrate ESC's effectiveness, we conducted\nextensive experiments on three popular categories of reasoning tasks:\narithmetic, commonsense and symbolic reasoning over language models with\nvarying scales. The empirical results show that ESC reduces the average number\nof sampling of chain-of-thought reasoning by a significant margin on six\nbenchmarks, including MATH (-33.8%), GSM8K (-80.1%), StrategyQA (-76.8%),\nCommonsenseQA (-78.5%), Coin Flip (-84.2%) and Last Letters (-67.4%), while\nattaining comparable performances.\n",
    "link": "http://arxiv.org/abs/2401.10480v1"
  },
  {
    "title": "LDReg: Local Dimensionality Regularized Self-Supervised Learning",
    "authors": "Hanxun Huang, Ricardo J. G. B. Campello, Sarah Monazam Erfani, Xingjun Ma, Michael E. Houle, James Bailey",
    "abstract": "  Representations learned via self-supervised learning (SSL) can be susceptible\nto dimensional collapse, where the learned representation subspace is of\nextremely low dimensionality and thus fails to represent the full data\ndistribution and modalities. Dimensional collapse also known as the\n\"underfilling\" phenomenon is one of the major causes of degraded performance on\ndownstream tasks. Previous work has investigated the dimensional collapse\nproblem of SSL at a global level. In this paper, we demonstrate that\nrepresentations can span over high dimensional space globally, but collapse\nlocally. To address this, we propose a method called $\\textit{local\ndimensionality regularization (LDReg)}$. Our formulation is based on the\nderivation of the Fisher-Rao metric to compare and optimize local distance\ndistributions at an asymptotically small radius for each data point. By\nincreasing the local intrinsic dimensionality, we demonstrate through a range\nof experiments that LDReg improves the representation quality of SSL. The\nresults also show that LDReg can regularize dimensionality at both local and\nglobal levels.\n",
    "link": "http://arxiv.org/abs/2401.10474v1"
  },
  {
    "title": "DeepEdit: Knowledge Editing as Decoding with Constraints",
    "authors": "Yiwei Wang, Muhao Chen, Nanyun Peng, Kai-Wei Chang",
    "abstract": "  We develop a new perspective of knowledge editing for large language models\n(LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search\nbased Progressive Decoding for Knowledge Editing), a neuro-symbolic method that\nimproves knowledge editing with better coherence of reasoning, relevance to the\nquestion, and awareness of updated knowledge. DeepEdit can be flexibly applied\nto all black-box LLMs: it does not require any access to the model parameters,\nrepresentations, or output vocabulary distributions. DeepEdit progressively\nproduces the high-quality reasoning steps towards effective knowledge editing.\nIt utilizes a depth-first search to revise the LLMs' output, which improves the\noutput's informativeness to the input question and awareness of the updated\nknowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more\nsuccinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit\nyields significant gains on MQuaKE, a challenging multi-hop question-answering\ndataset with knowledge editing. We release the source code at\nhttps://github.com/wangywUST/DeepEdit.\n",
    "link": "http://arxiv.org/abs/2401.10471v1"
  },
  {
    "title": "Learning Backdoors for Mixed Integer Programs with Contrastive Learning",
    "authors": "Junyang Cai, Taoan Huang, Bistra Dilkina",
    "abstract": "  Many real-world problems can be efficiently modeled as Mixed Integer Programs\n(MIPs) and solved with the Branch-and-Bound method. Prior work has shown the\nexistence of MIP backdoors, small sets of variables such that prioritizing\nbranching on them when possible leads to faster running times. However, finding\nhigh-quality backdoors that improve running times remains an open question.\nPrevious work learns to estimate the relative solver speed of randomly sampled\nbackdoors through ranking and then decide whether to use it. In this paper, we\nutilize the Monte-Carlo tree search method to collect backdoors for training,\nrather than relying on random sampling, and adapt a contrastive learning\nframework to train a Graph Attention Network model to predict backdoors. Our\nmethod, evaluated on four common MIP problem domains, demonstrates performance\nimprovements over both Gurobi and previous models.\n",
    "link": "http://arxiv.org/abs/2401.10467v1"
  },
  {
    "title": "Critical Data Size of Language Models from a Grokking Perspective",
    "authors": "Xuekai Zhu, Yao Fu, Bowen Zhou, Zhouhan Lin",
    "abstract": "  We explore the critical data size in language models, a threshold that marks\na fundamental shift from quick memorization to slow generalization. We\nformalize the phase transition under the grokking configuration into the Data\nEfficiency Hypothesis and identify data insufficiency, sufficiency, and surplus\nregimes in language models training dynamics. We develop a grokking\nconfiguration to reproduce grokking on simplistic language models stably by\nrescaling initialization and weight decay. We show that generalization occurs\nonly when language models reach a critical size. We analyze grokking across\nsample-wise and model-wise, verifying the proposed data efficiency hypothesis.\nOur experiments reveal smoother phase transitions occurring at the critical\ndataset size for language datasets. As the model size increases, this critical\npoint also becomes larger, indicating that larger models require more data. Our\nresults deepen the understanding of language model training, offering a novel\nperspective on the role of data in the learning mechanism of language models.\n",
    "link": "http://arxiv.org/abs/2401.10463v1"
  },
  {
    "title": "Investigating Training Strategies and Model Robustness of Low-Rank\n  Adaptation for Language Modeling in Speech Recognition",
    "authors": "Yu Yu, Chao-Han Huck Yang, Tuan Dinh, Sungho Ryu, Jari Kolehmainen, Roger Ren, Denis Filimonov, Prashanth G. Shivakumar, Ankur Gandhe, Ariya Rastow, Jia Xu, Ivan Bulyko, Andreas Stolcke",
    "abstract": "  The use of low-rank adaptation (LoRA) with frozen pretrained language models\n(PLMs) has become increasing popular as a mainstream, resource-efficient\nmodeling approach for memory-constrained hardware. In this study, we first\nexplore how to enhance model performance by introducing various LoRA training\nstrategies, achieving relative word error rate reductions of 3.50\\% on the\npublic Librispeech dataset and of 3.67\\% on an internal dataset in the\nmessaging domain. To further characterize the stability of LoRA-based\nsecond-pass speech recognition models, we examine robustness against input\nperturbations. These perturbations are rooted in homophone replacements and a\nnovel metric called N-best Perturbation-based Rescoring Robustness (NPRR), both\ndesigned to measure the relative degradation in the performance of rescoring\nmodels. Our experimental results indicate that while advanced variants of LoRA,\nsuch as dynamic rank-allocated LoRA, lead to performance degradation in\n$1$-best perturbation, they alleviate the degradation in $N$-best perturbation.\nThis finding is in comparison to fully-tuned models and vanilla LoRA tuning\nbaselines, suggesting that a comprehensive selection is needed when using\nLoRA-based adaptation for compute-cost savings and robust language modeling.\n",
    "link": "http://arxiv.org/abs/2401.10447v1"
  },
  {
    "title": "Large Language Models are Efficient Learners of Noise-Robust Speech\n  Recognition",
    "authors": "Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Chao Zhang, Pin-Yu Chen, EnSiong Chng",
    "abstract": "  Recent advances in large language models (LLMs) have promoted generative\nerror correction (GER) for automatic speech recognition (ASR), which leverages\nthe rich linguistic knowledge and powerful reasoning ability of LLMs to improve\nrecognition results. The latest work proposes a GER benchmark with HyPoradise\ndataset to learn the mapping from ASR N-best hypotheses to ground-truth\ntranscription by efficient LLM finetuning, which shows great effectiveness but\nlacks specificity on noise-robust ASR. In this work, we extend the benchmark to\nnoisy conditions and investigate if we can teach LLMs to perform denoising for\nGER just like what robust ASR do}, where one solution is introducing noise\ninformation as a conditioner into LLM. However, directly incorporating noise\nembeddings from audio encoder could harm the LLM tuning due to cross-modality\ngap. To this end, we propose to extract a language-space noise embedding from\nthe N-best list to represent the noise conditions of source speech, which can\npromote the denoising process in GER. Furthermore, in order to enhance its\nrepresentation ability of audio noise, we design a knowledge distillation (KD)\napproach via mutual information estimation to distill the real noise\ninformation in audio embeddings to our language embedding. Experiments on\nvarious latest LLMs demonstrate our approach achieves a new breakthrough with\nup to 53.9% correction improvement in terms of word error rate while with\nlimited training data. Analysis shows that our language-space noise embedding\ncan well represent the noise conditions of source speech, under which\noff-the-shelf LLMs show strong ability of language-space denoising.\n",
    "link": "http://arxiv.org/abs/2401.10446v1"
  },
  {
    "title": "Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?",
    "authors": "Ron Sun",
    "abstract": "  The paper discusses what is needed to address the limitations of current\nLLM-centered AI systems. The paper argues that incorporating insights from\nhuman cognition and psychology, as embodied by a computational cognitive\narchitecture, can help develop systems that are more capable, more reliable,\nand more human-like. It emphasizes the importance of the dual-process\narchitecture and the hybrid neuro-symbolic approach in addressing the\nlimitations of current LLMs. In the opposite direction, the paper also\nhighlights the need for an overhaul of computational cognitive architectures to\nbetter reflect advances in AI and computing technology. Overall, the paper\nadvocates for a multidisciplinary, mutually beneficial approach towards\ndeveloping better models both for AI and for understanding the human mind.\n",
    "link": "http://arxiv.org/abs/2401.10444v1"
  }
]