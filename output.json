[
  {
    "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
    "authors": " DeepSeek-AI,  :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou",
    "abstract": "  The rapid development of open-source large language models (LLMs) has been\ntruly remarkable. However, the scaling law described in previous literature\npresents varying conclusions, which casts a dark cloud over scaling LLMs. We\ndelve into the study of scaling laws and present our distinctive findings that\nfacilitate scaling of large scale models in two commonly used open-source\nconfigurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek\nLLM, a project dedicated to advancing open-source language models with a\nlong-term perspective. To support the pre-training phase, we have developed a\ndataset that currently consists of 2 trillion tokens and is continuously\nexpanding. We further conduct supervised fine-tuning (SFT) and Direct\nPreference Optimization (DPO) on DeepSeek LLM Base models, resulting in the\ncreation of DeepSeek Chat models. Our evaluation results demonstrate that\nDeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in\nthe domains of code, mathematics, and reasoning. Furthermore, open-ended\nevaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance\ncompared to GPT-3.5.\n",
    "link": "http://arxiv.org/abs/2401.02954v1"
  },
  {
    "title": "Graph2Tac: Learning Hierarchical Representations of Math Concepts in\n  Theorem proving",
    "authors": "Jason Rute, Miroslav Ol\u0161\u00e1k, Lasse Blaauwbroek, Fidel Ivan Schaposnik Massolo, Jelle Piepenbrock, Vasily Pestun",
    "abstract": "  Concepts abound in mathematics and its applications. They vary greatly\nbetween subject areas, and new ones are introduced in each mathematical paper\nor application. A formal theory builds a hierarchy of definitions, theorems and\nproofs that reference each other. When an AI agent is proving a new theorem,\nmost of the mathematical concepts and lemmas relevant to that theorem may have\nnever been seen during training. This is especially true in the Coq proof\nassistant, which has a diverse library of Coq projects, each with its own\ndefinitions, lemmas, and even custom tactic procedures used to prove those\nlemmas. It is essential for agents to incorporate such new information into\ntheir knowledge base on the fly. We work towards this goal by utilizing a new,\nlarge-scale, graph-based dataset for machine learning in Coq. We leverage a\nfaithful graph-representation of Coq terms that induces a directed graph of\ndependencies between definitions to create a novel graph neural network,\nGraph2Tac (G2T), that takes into account not only the current goal, but also\nthe entire hierarchy of definitions that led to the current goal. G2T is an\nonline model that is deeply integrated into the users' workflow and can adapt\nin real time to new Coq projects and their definitions. It complements well\nwith other online models that learn in real time from new proof scripts. Our\nnovel definition embedding task, which is trained to compute representations of\nmathematical concepts not seen during training, boosts the performance of the\nneural network to rival state-of-the-art k-nearest neighbor predictors.\n",
    "link": "http://arxiv.org/abs/2401.02949v1"
  },
  {
    "title": "Analytically-Driven Resource Management for Cloud-Native Microservices",
    "authors": "Yanqi Zhang, Zhuangzhuang Zhou, Sameh Elnikety, Christina Delimitrou",
    "abstract": "  Resource management for cloud-native microservices has attracted a lot of\nrecent attention. Previous work has shown that machine learning (ML)-driven\napproaches outperform traditional techniques, such as autoscaling, in terms of\nboth SLA maintenance and resource efficiency. However, ML-driven approaches\nalso face challenges including lengthy data collection processes and limited\nscalability. We present Ursa, a lightweight resource management system for\ncloud-native microservices that addresses these challenges. Ursa uses an\nanalytical model that decomposes the end-to-end SLA into per-service SLA, and\nmaps per-service SLA to individual resource allocations per microservice tier.\nTo speed up the exploration process and avoid prolonged SLA violations, Ursa\nexplores each microservice individually, and swiftly stops exploration if\nlatency exceeds its SLA.\n  We evaluate Ursa on a set of representative and end-to-end microservice\ntopologies, including a social network, media service and video processing\npipeline, each consisting of multiple classes and priorities of requests with\ndifferent SLAs, and compare it against two representative ML-driven systems,\nSinan and Firm. Compared to these ML-driven approaches, Ursa provides\nsignificant advantages: It shortens the data collection process by more than\n128x, and its control plane is 43x faster than ML-driven approaches. At the\nsame time, Ursa does not sacrifice resource efficiency or SLAs. During online\ndeployment, Ursa reduces the SLA violation rate by 9.0% up to 49.9%, and\nreduces CPU allocation by up to 86.2% compared to ML-driven approaches.\n",
    "link": "http://arxiv.org/abs/2401.02920v1"
  },
  {
    "title": "H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network\n  Framework for Discovery of Multi-Modal Physiological Responses",
    "authors": "Haidong Gu, Nathan Gaw, Yinan Wang, Chancellor Johnstone, Christine Beauchene, Sophia Yuditskaya, Hrishikesh Rao, Chun-An Chou",
    "abstract": "  Discovering human cognitive and emotional states using multi-modal\nphysiological signals draws attention across various research applications.\nPhysiological responses of the human body are influenced by human cognition and\ncommonly used to analyze cognitive states. From a network science perspective,\nthe interactions of these heterogeneous physiological modalities in a graph\nstructure may provide insightful information to support prediction of cognitive\nstates. However, there is no clue to derive exact connectivity between\nheterogeneous modalities and there exists a hierarchical structure of\nsub-modalities. Existing graph neural networks are designed to learn on\nnon-hierarchical homogeneous graphs with pre-defined graph structures; they\nfailed to learn from hierarchical, multi-modal physiological data without a\npre-defined graph structure. To this end, we propose a hierarchical\nheterogeneous graph generative network (H2G2-Net) that automatically learns a\ngraph structure without domain knowledge, as well as a powerful representation\non the hierarchical heterogeneous graph in an end-to-end fashion. We validate\nthe proposed method on the CogPilot dataset that consists of multi-modal\nphysiological signals. Extensive experiments demonstrate that our proposed\nmethod outperforms the state-of-the-art GNNs by 5%-20% in prediction accuracy.\n",
    "link": "http://arxiv.org/abs/2401.02905v1"
  },
  {
    "title": "MsDC-DEQ-Net: Deep Equilibrium Model (DEQ) with Multi-scale Dilated\n  Convolution for Image Compressive Sensing (CS)",
    "authors": "Youhao Yu, Richard M. Dansereau",
    "abstract": "  Compressive sensing (CS) is a technique that enables the recovery of sparse\nsignals using fewer measurements than traditional sampling methods. To address\nthe computational challenges of CS reconstruction, our objective is to develop\nan interpretable and concise neural network model for reconstructing natural\nimages using CS. We achieve this by mapping one step of the iterative shrinkage\nthresholding algorithm (ISTA) to a deep network block, representing one\niteration of ISTA. To enhance learning ability and incorporate structural\ndiversity, we integrate aggregated residual transformations (ResNeXt) and\nsqueeze-and-excitation (SE) mechanisms into the ISTA block. This block serves\nas a deep equilibrium layer, connected to a semi-tensor product network\n(STP-Net) for convenient sampling and providing an initial reconstruction. The\nresulting model, called MsDC-DEQ-Net, exhibits competitive performance compared\nto state-of-the-art network-based methods. It significantly reduces storage\nrequirements compared to deep unrolling methods, using only one iteration block\ninstead of multiple iterations. Unlike deep unrolling models, MsDC-DEQ-Net can\nbe iteratively used, gradually improving reconstruction accuracy while\nconsidering computation trade-offs. Additionally, the model benefits from\nmulti-scale dilated convolutions, further enhancing performance.\n",
    "link": "http://arxiv.org/abs/2401.02884v1"
  },
  {
    "title": "Optimal Chaining of Vehicle Plans with Time Windows",
    "authors": "David Fiedler, Fabio V. Difonzo, Jan Mrkos",
    "abstract": "  For solving problems from the domain of vehicle routing with time windows, we\noften need to connect vehicle plans into sequences spanning a longer time\nhorizon or, in other words, we need to perform a plan chaining. Recently, a\nnetwork-based solution has been proposed to solve the fleet-sizing problem. The\nmethod, however, does not consider the time flexibility of the plans, an\nessential property of all vehicle routing problems with time windows. Instead,\nplans have fixed times and cannot be delayed. This work presents a new problem\nformulation that considers delays in line with the given time windows and a\nmethod that can be used to solve it. Moreover, we prove that the method is\noptimal, and we analyze its complexity. Finally, we list some practical\napplications and perform a demonstration for one of them: the method for\nsolving the static Dial-a-ride problem. The demonstration results show that for\na significant number of instances, the proposed method provides a better\nsolution than the other two heuristic baseline methods we have evaluated, while\nnot having the largest computational time requirements.\n",
    "link": "http://arxiv.org/abs/2401.02873v1"
  },
  {
    "title": "AFSPP: Agent Framework for Shaping Preference and Personality with Large\n  Language Models",
    "authors": "Zihong He, Changwang Zhang",
    "abstract": "  The evolution of Large Language Models (LLMs) has introduced a new paradigm\nfor investigating human behavior emulation. Recent research has employed\nLLM-based Agents to create a sociological research environment, in which agents\nexhibit behavior based on the unfiltered characteristics of large language\nmodels. However, these studies overlook the iterative development within a\nhuman-like setting - Human preferences and personalities are complex, shaped by\nvarious factors and subject to ongoing change as a result of environmental and\nsubjective influences. In light of this observation, we propose Agent Framework\nfor Shaping Preference and Personality (AFSPP), exploring the multifaceted\nimpact of social networks and subjective consciousness on LLM-based Agents'\npreference and personality formation. With AFSPP, we have, for the first time,\nsuccessfully replicated several key findings from human personality\nexperiments. And other AFSPP-based experimental results indicate that plan\nmaking, sensory perceptions and social networking with subjective information,\nwield the most pronounced influence on preference shaping. AFSPP can\nsignificantly enhance the efficiency and scope of psychological experiments,\nwhile yielding valuable insights for Trustworthy Artificial Intelligence\nresearch for strategies to prevent undesirable preference and personality\ndevelopment.\n",
    "link": "http://arxiv.org/abs/2401.02870v1"
  },
  {
    "title": "Framework for Variable-lag Motif Following Relation Inference In Time\n  Series using Matrix Profile analysis",
    "authors": "Naaek Chinpattanakarn, Chainarong Amornbunchornvej",
    "abstract": "  Knowing who follows whom and what patterns they are following are crucial\nsteps to understand collective behaviors (e.g. a group of human, a school of\nfish, or a stock market). Time series is one of resources that can be used to\nget insight regarding following relations. However, the concept of following\npatterns or motifs and the solution to find them in time series are not\nobvious. In this work, we formalize a concept of following motifs between two\ntime series and present a framework to infer following patterns between two\ntime series. The framework utilizes one of efficient and scalable methods to\nretrieve motifs from time series called the Matrix Profile Method. We compare\nour proposed framework with several baselines. The framework performs better\nthan baselines in the simulation datasets. In the dataset of sound recording,\nthe framework is able to retrieve the following motifs within a pair of time\nseries that two singers sing following each other. In the cryptocurrency\ndataset, the framework is capable of capturing the following motifs within a\npair of time series from two digital currencies, which implies that the values\nof one currency follow the values of another currency patterns. Our framework\ncan be utilized in any field of time series to get insight regarding following\npatterns between time series.\n",
    "link": "http://arxiv.org/abs/2401.02860v1"
  },
  {
    "title": "Generative Large Language Models are autonomous practitioners of\n  evidence-based medicine",
    "authors": "Akhil Vaid, Joshua Lampert, Juhee Lee, Ashwin Sawant, Donald Apakama, Ankit Sakhuja, Ali Soroush, Denise Lee, Isotta Landi, Nicole Bussola, Ismail Nabeel, Robbie Freeman, Patricia Kovatch, Brendan Carr, Benjamin Glicksberg, Edgar Argulian, Stamatios Lerakis, Monica Kraft, Alexander Charney, Girish Nadkarni",
    "abstract": "  Background: Evidence-based medicine (EBM) is fundamental to modern clinical\npractice, requiring clinicians to continually update their knowledge and apply\nthe best clinical evidence in patient care. The practice of EBM faces\nchallenges due to rapid advancements in medical research, leading to\ninformation overload for clinicians. The integration of artificial intelligence\n(AI), specifically Generative Large Language Models (LLMs), offers a promising\nsolution towards managing this complexity.\n  Methods: This study involved the curation of real-world clinical cases across\nvarious specialties, converting them into .json files for analysis. LLMs,\nincluding proprietary models like ChatGPT 3.5 and 4, Gemini Pro, and\nopen-source models like LLaMA v2 and Mixtral-8x7B, were employed. These models\nwere equipped with tools to retrieve information from case files and make\nclinical decisions similar to how clinicians must operate in the real world.\nModel performance was evaluated based on correctness of final answer, judicious\nuse of tools, conformity to guidelines, and resistance to hallucinations.\n  Results: GPT-4 was most capable of autonomous operation in a clinical\nsetting, being generally more effective in ordering relevant investigations and\nconforming to clinical guidelines. Limitations were observed in terms of model\nability to handle complex guidelines and diagnostic nuances. Retrieval\nAugmented Generation made recommendations more tailored to patients and\nhealthcare systems.\n  Conclusions: LLMs can be made to function as autonomous practitioners of\nevidence-based medicine. Their ability to utilize tooling can be harnessed to\ninteract with the infrastructure of a real-world healthcare system and perform\nthe tasks of patient management in a guideline directed manner. Prompt\nengineering may help to further enhance this potential and transform healthcare\nfor the clinician and the patient.\n",
    "link": "http://arxiv.org/abs/2401.02851v1"
  },
  {
    "title": "Thousands of AI Authors on the Future of AI",
    "authors": "Katja Grace, Harlan Stewart, Julia Fabienne Sandk\u00fchler, Stephen Thomas, Ben Weinstein-Raun, Jan Brauner",
    "abstract": "  In the largest survey of its kind, 2,778 researchers who had published in\ntop-tier artificial intelligence (AI) venues gave predictions on the pace of AI\nprogress and the nature and impacts of advanced AI systems The aggregate\nforecasts give at least a 50% chance of AI systems achieving several milestones\nby 2028, including autonomously constructing a payment processing site from\nscratch, creating a song indistinguishable from a new song by a popular\nmusician, and autonomously downloading and fine-tuning a large language model.\nIf science continues undisrupted, the chance of unaided machines outperforming\nhumans in every possible task was estimated at 10% by 2027, and 50% by 2047.\nThe latter estimate is 13 years earlier than that reached in a similar survey\nwe conducted only one year earlier [Grace et al., 2022]. However, the chance of\nall human occupations becoming fully automatable was forecast to reach 10% by\n2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).\n  Most respondents expressed substantial uncertainty about the long-term value\nof AI progress: While 68.3% thought good outcomes from superhuman AI are more\nlikely than bad, of these net optimists 48% gave at least a 5% chance of\nextremely bad outcomes such as human extinction, and 59% of net pessimists gave\n5% or more to extremely good outcomes. Between 38% and 51% of respondents gave\nat least a 10% chance to advanced AI leading to outcomes as bad as human\nextinction. More than half suggested that \"substantial\" or \"extreme\" concern is\nwarranted about six different AI-related scenarios, including misinformation,\nauthoritarian control, and inequality. There was disagreement about whether\nfaster or slower AI progress would be better for the future of humanity.\nHowever, there was broad agreement that research aimed at minimizing potential\nrisks from AI systems ought to be prioritized more.\n",
    "link": "http://arxiv.org/abs/2401.02843v1"
  },
  {
    "title": "Pheme: Efficient and Conversational Speech Generation",
    "authors": "Pawe\u0142 Budzianowski, Taras Sereda, Tomasz Cichy, Ivan Vuli\u0107",
    "abstract": "  In recent years, speech generation has seen remarkable progress, now\nachieving one-shot generation capability that is often virtually\nindistinguishable from real human voice. Integrating such advancements in\nspeech generation with large language models might revolutionize a wide range\nof applications. However, certain applications, such as assistive\nconversational systems, require natural and conversational speech generation\ntools that also operate efficiently in real time. Current state-of-the-art\nmodels like VALL-E and SoundStorm, powered by hierarchical neural audio codecs,\nrequire large neural components and extensive training data to work well. In\ncontrast, MQTTS aims to build more compact conversational TTS models while\ncapitalizing on smaller-scale real-life conversational speech data. However,\nits autoregressive nature yields high inference latency and thus limits its\nreal-time usage. In order to mitigate the current limitations of the\nstate-of-the-art TTS models while capitalizing on their strengths, in this work\nwe introduce the Pheme model series that 1) offers compact yet high-performing\nmodels, 2) allows for parallel speech generation of 3) natural conversational\nspeech, and 4) it can be trained efficiently on smaller-scale conversational\ndata, cutting data demands by more than 10x but still matching the quality of\nthe autoregressive TTS models. We also show that through simple teacher-student\ndistillation we can meet significant improvements in voice quality for\nsingle-speaker setups on top of pretrained Pheme checkpoints, relying solely on\nsynthetic speech generated by much larger teacher models. Audio samples and\npretrained models are available online.\n",
    "link": "http://arxiv.org/abs/2401.02839v1"
  },
  {
    "title": "CrisisViT: A Robust Vision Transformer for Crisis Image Classification",
    "authors": "Zijun Long, Richard McCreadie, Muhammad Imran",
    "abstract": "  In times of emergency, crisis response agencies need to quickly and\naccurately assess the situation on the ground in order to deploy relevant\nservices and resources. However, authorities often have to make decisions based\non limited information, as data on affected regions can be scarce until local\nresponse services can provide first-hand reports. Fortunately, the widespread\navailability of smartphones with high-quality cameras has made citizen\njournalism through social media a valuable source of information for crisis\nresponders. However, analyzing the large volume of images posted by citizens\nrequires more time and effort than is typically available. To address this\nissue, this paper proposes the use of state-of-the-art deep neural models for\nautomatic image classification/tagging, specifically by adapting\ntransformer-based architectures for crisis image classification (CrisisViT). We\nleverage the new Incidents1M crisis image dataset to develop a range of new\ntransformer-based image classification models. Through experimentation over the\nstandard Crisis image benchmark dataset, we demonstrate that the CrisisViT\nmodels significantly outperform previous approaches in emergency type, image\nrelevance, humanitarian category, and damage severity classification.\nAdditionally, we show that the new Incidents1M dataset can further augment the\nCrisisViT models resulting in an additional 1.25% absolute accuracy gain.\n",
    "link": "http://arxiv.org/abs/2401.02838v1"
  },
  {
    "title": "Physics-Informed Neural Networks for High-Frequency and Multi-Scale\n  Problems using Transfer Learning",
    "authors": "Abdul Hannan Mustajab, Hao Lyu, Zarghaam Rizvi, Frank Wuttke",
    "abstract": "  Physics-informed neural network (PINN) is a data-driven solver for partial\nand ordinary differential equations(ODEs/PDEs). It provides a unified framework\nto address both forward and inverse problems. However, the complexity of the\nobjective function often leads to training failures. This issue is particularly\nprominent when solving high-frequency and multi-scale problems. We proposed\nusing transfer learning to boost the robustness and convergence of training\nPINN, starting training from low-frequency problems and gradually approaching\nhigh-frequency problems. Through two case studies, we discovered that transfer\nlearning can effectively train PINN to approximate solutions from low-frequency\nproblems to high-frequency problems without increasing network parameters.\nFurthermore, it requires fewer data points and less training time. We\nelaborately described our training strategy, including optimizer selection, and\nsuggested guidelines for using transfer learning to train neural networks for\nsolving more complex problems.\n",
    "link": "http://arxiv.org/abs/2401.02810v1"
  },
  {
    "title": "PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language\n  Models for Medical Visual Question Answering",
    "authors": "Jinlong He, Pengfei Li, Gang Liu, Zixu Zhao, Shenjun Zhong",
    "abstract": "  Multimodal large language models (MLLMs) represent an evolutionary expansion\nin the capabilities of traditional large language models, enabling them to\ntackle challenges that surpass the scope of purely text-based applications. It\nleverages the knowledge previously encoded within these language models,\nthereby enhancing their applicability and functionality in the reign of\nmultimodal contexts. Recent works investigate the adaptation of MLLMs to\npredict free-form answers as a generative task to solve medical visual question\nanswering (Med-VQA) tasks. In this paper, we propose a parameter efficient\nframework for fine-tuning MLLM specifically tailored to Med-VQA applications,\nand empirically validate it on a public benchmark dataset. To accurately\nmeasure the performance, we employ human evaluation and the results reveal that\nour model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v\nmodel by a significant margin of 26% absolute accuracy on closed-ended\nquestions. The code will be available here: https://github.com/jinlHe/PeFoMed.\n",
    "link": "http://arxiv.org/abs/2401.02797v1"
  },
  {
    "title": "From LLM to Conversational Agent: A Memory Enhanced Architecture with\n  Fine-Tuning of Large Language Models",
    "authors": "Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, Ming Cui",
    "abstract": "  This paper introduces RAISE (Reasoning and Acting through Scratchpad and\nExamples), an advanced architecture enhancing the integration of Large Language\nModels (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of\nthe ReAct framework, incorporates a dual-component memory system, mirroring\nhuman short-term and long-term memory, to maintain context and continuity in\nconversations. It entails a comprehensive agent construction scenario,\nincluding phases like Conversation Selection, Scene Extraction, CoT Completion,\nand Scene Augmentation, leading to the LLMs Training phase. This approach\nappears to enhance agent controllability and adaptability in complex,\nmulti-turn dialogues. Our preliminary evaluations in a real estate sales\ncontext suggest that RAISE has some advantages over traditional agents,\nindicating its potential for broader applications. This work contributes to the\nAI field by providing a robust framework for developing more context-aware and\nversatile conversational agents.\n",
    "link": "http://arxiv.org/abs/2401.02777v1"
  },
  {
    "title": "Tackling Electrode Shift In Gesture Recognition with HD-EMG Electrode\n  Subsets",
    "authors": "Joao Pereira, Dimitrios Chalatsis, Balint Hodossy, Dario Farina",
    "abstract": "  sEMG pattern recognition algorithms have been explored extensively in\ndecoding movement intent, yet are known to be vulnerable to changing recording\nconditions, exhibiting significant drops in performance across subjects, and\neven across sessions. Multi-channel surface EMG, also referred to as\nhigh-density sEMG (HD-sEMG) systems, have been used to improve performance with\nthe information collected through the use of additional electrodes. However, a\nlack of robustness is ever present due to limited datasets and the difficulties\nin addressing sources of variability, such as electrode placement. In this\nstudy, we propose training on a collection of input channel subsets and\naugmenting our training distribution with data from different electrode\nlocations, simultaneously targeting electrode shift and reducing input\ndimensionality. Our method increases robustness against electrode shift and\nresults in significantly higher intersession performance across subjects and\nclassification algorithms.\n",
    "link": "http://arxiv.org/abs/2401.02773v1"
  },
  {
    "title": "Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding",
    "authors": "Yuu Jinnai, Kaito Ariu",
    "abstract": "  Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to\nbeam search decoding for a wide range of text generation tasks. However, MBR\nrequires a huge amount of time for inference to compute the MBR objective,\nwhich makes the method infeasible in many situations where response time is\ncritical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently\nbeen proposed to reduce the inference time in machine translation tasks.\nAlthough it is shown to significantly reduce the amount of computation, it\nrequires hyperparameter tuning using a development set to be effective. To this\nend, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a\nhyperparameter-free method to run MBR decoding approximately. AMBR is derived\nfrom the observation that the problem of computing the sample-based MBR\nobjective is the medoid identification problem. AMBR uses the Correlated\nSequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best\napproximation algorithm to date for the medoid identification problem, to\ncompute the sample-based MBR objective. We evaluate AMBR on machine\ntranslation, text summarization, and image captioning tasks. The results show\nthat AMBR achieves on par with CBP, with CBP selecting hyperparameters through\nan Oracle for each given computation budget.\n",
    "link": "http://arxiv.org/abs/2401.02749v1"
  },
  {
    "title": "MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron\n  Captioning",
    "authors": "Alfirsa Damasyifa Fauzulhaq, Wahyu Parwitayasa, Joseph Ananda Sugihdharma, M. Fadli Ridhani, Novanto Yudistira",
    "abstract": "  Neuron labeling is an approach to visualize the behaviour and respond of a\ncertain neuron to a certain pattern that activates the neuron. Neuron labeling\nextract information about the features captured by certain neurons in a deep\nneural network, one of which uses the encoder-decoder image captioning\napproach. The encoder used can be a pretrained CNN-based model and the decoder\nis an RNN-based model for text generation. Previous work, namely MILAN (Mutual\nInformation-guided Linguistic Annotation of Neuron), has tried to visualize the\nneuron behaviour using modified Show, Attend, and Tell (SAT) model in the\nencoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show\ngreat result on short sequence neuron captioning, but it does not show great\nresult on long sequence neuron captioning, so in this work, we would like to\nimprove the performance of MILAN even more by utilizing different kind of\nattention mechanism and additionally adding several attention result into one,\nin order to combine all the advantages from several attention mechanism. Using\nour compound dataset, we obtained higher BLEU and F1-Score on our proposed\nmodel, achieving 17.742 and 0.4811 respectively. At some point where the model\nconverges at the peak, our model obtained BLEU of 21.2262 and BERTScore\nF1-Score of 0.4870.\n",
    "link": "http://arxiv.org/abs/2401.02744v1"
  },
  {
    "title": "Fairness-Aware Job Scheduling for Multi-Job Federated Learning",
    "authors": "Yuxin Shi, Han Yu",
    "abstract": "  Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to\ncollaboratively train machine learning models without disclosing sensitive\nprivate data. Existing FL research mostly focuses on the monopoly scenario in\nwhich a single FL server selects a subset of FL clients to update their local\nmodels in each round of training. In practice, there can be multiple FL servers\nsimultaneously trying to select clients from the same pool. In this paper, we\npropose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS)\napproach to bridge this gap. Based on Lyapunov optimization, it ensures fair\nallocation of high-demand FL client datasets to FL jobs in need of them, by\njointly considering the current demand and the job payment bids, in order to\nprevent prolonged waiting. Extensive experiments comparing FairFedJS against\nfour state-of-the-art approaches on two datasets demonstrate its significant\nadvantages. It outperforms the best baseline by 31.9% and 1.0% on average in\nterms of scheduling fairness and convergence time, respectively, while\nachieving comparable test accuracy.\n",
    "link": "http://arxiv.org/abs/2401.02740v1"
  },
  {
    "title": "Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts\n  for Instruction Tuning on General Tasks",
    "authors": "Haoyuan Wu, Haisheng Zheng, Bei Yu",
    "abstract": "  Large Language Models (LLMs) have demonstrated considerable proficiency in\ngeneral natural language processing (NLP) tasks. Instruction tuning, a\nsuccessful paradigm, enhances the ability of LLMs to follow natural language\ninstructions and exhibit robust generalization across a wide range of tasks.\nHowever, these models often encounter performance limitations across multiple\ntasks due to constrained model capacity. Expanding this capacity during the\ninstruction tuning phase poses significant challenges. To address this issue,\nwe introduce a novel approach, Parameter-Efficient Sparsity Crafting (PESC),\nwhich transitions dense models to sparse models using a Mixture of Experts\n(MoE) architecture. PESC integrates adapters into the MoE layers of sparse\nmodels, differentiating experts without altering the individual weights within\nthese layers. This method significantly reduces computational costs and GPU\nmemory requirements, facilitating model capacity expansion through a minimal\nincrease in parameters via the inserted adapters. Our empirical evaluation\ndemonstrates the effectiveness of the PESC method. Using PESC during\ninstruction tuning, our sparse models, dubbed Camelidae outperform all other\nopensource sparse models and exhibit superior general capabilities compared to\nGPT3.5.\n",
    "link": "http://arxiv.org/abs/2401.02731v1"
  },
  {
    "title": "Enhancing targeted transferability via feature space fine-tuning",
    "authors": "Hui Zeng, Biwei Chen, Anjie Peng",
    "abstract": "  Adversarial examples (AEs) have been extensively studied due to their\npotential for privacy protection and inspiring robust neural networks. However,\nmaking a targeted AE transferable across unknown models remains challenging. In\nthis paper, to alleviate the overfitting dilemma common in an AE crafted by\nexisting simple iterative attacks, we propose fine-tuning it in the feature\nspace. Specifically, starting with an AE generated by a baseline attack, we\nencourage the features that contribute to the target class and discourage the\nfeatures that contribute to the original class in a middle layer of the source\nmodel. Extensive experiments demonstrate that only a few iterations of\nfine-tuning can boost existing attacks in terms of targeted transferability\nnontrivially and universally. Our results also verify that the simple iterative\nattacks can yield comparable or even better transferability than the\nresource-intensive methods, which rely on training target-specific classifiers\nor generators with additional data. The code is available at:\ngithub.com/zengh5/TA_feature_FT.\n",
    "link": "http://arxiv.org/abs/2401.02727v1"
  },
  {
    "title": "Une ontologie pour les syst{\u00e8}mes multi-agents ambiants dans les\n  villes intelligentes",
    "authors": "Nathan Aky, Denis Payet, Sylvain Giroux, R\u00e9my Courdier",
    "abstract": "  Towns and cities are currently equipping themselves with a host of connected\ndevices, with a view to transforming themselves into ''smart cities''. To\nmanage this mass of connected objects, autonomous software entities, known as\nagents, can be attached to them to cooperate and use these devices to offer\npersonalized services. However, this object infrastructure needs to be\nsemantically structured in order to be exploited. This is why the proposal of\nthis article is an ontology, formatted in OWL, describing the object\ninfrastructures, their links with the organization of the multi-agent system\nand the services to be delivered according to the users of the system. The\nontology is applied to smart mobility for people with reduced mobility, and\ncould be adapted to other smart city axes.\n",
    "link": "http://arxiv.org/abs/2401.02726v1"
  },
  {
    "title": "Learning Image Demoireing from Unpaired Real Data",
    "authors": "Yunshan Zhong, Yuyao Zhou, Yuxin Zhang, Fei Chao, Rongrong Ji",
    "abstract": "  This paper focuses on addressing the issue of image demoireing. Unlike the\nlarge volume of existing studies that rely on learning from paired real data,\nwe attempt to learn a demoireing model from unpaired real data, i.e., moire\nimages associated with irrelevant clean images. The proposed method, referred\nto as Unpaired Demoireing (UnDeM), synthesizes pseudo moire images from\nunpaired datasets, generating pairs with clean images for training demoireing\nmodels. To achieve this, we divide real moire images into patches and group\nthem in compliance with their moire complexity. We introduce a novel moire\ngeneration framework to synthesize moire images with diverse moire features,\nresembling real moire patches, and details akin to real moire-free images.\nAdditionally, we introduce an adaptive denoise method to eliminate the\nlow-quality pseudo moire images that adversely impact the learning of\ndemoireing models. We conduct extensive experiments on the commonly-used FHDMi\nand UHDM datasets. Results manifest that our UnDeM performs better than\nexisting methods when using existing demoireing models such as MBCNN and\nESDNet-L. Code: https://github.com/zysxmu/UnDeM\n",
    "link": "http://arxiv.org/abs/2401.02719v1"
  },
  {
    "title": "Complementary Information Mutual Learning for Multimodality Medical\n  Image Segmentation",
    "authors": "Chuyun Shen, Wenhao Li, Haoqing Chen, Xiaoling Wang, Fengping Zhu, Yuxin Li, Xiangfeng Wang, Bo Jin",
    "abstract": "  Radiologists must utilize multiple modal images for tumor segmentation and\ndiagnosis due to the limitations of medical imaging and the diversity of tumor\nsignals. This leads to the development of multimodal learning in segmentation.\nHowever, the redundancy among modalities creates challenges for existing\nsubtraction-based joint learning methods, such as misjudging the importance of\nmodalities, ignoring specific modal information, and increasing cognitive load.\nThese thorny issues ultimately decrease segmentation accuracy and increase the\nrisk of overfitting. This paper presents the complementary information mutual\nlearning (CIML) framework, which can mathematically model and address the\nnegative impact of inter-modal redundant information. CIML adopts the idea of\naddition and removes inter-modal redundant information through inductive\nbias-driven task decomposition and message passing-based redundancy filtering.\nCIML first decomposes the multimodal segmentation task into multiple subtasks\nbased on expert prior knowledge, minimizing the information dependence between\nmodalities. Furthermore, CIML introduces a scheme in which each modality can\nextract information from other modalities additively through message passing.\nTo achieve non-redundancy of extracted information, the redundant filtering is\ntransformed into complementary information learning inspired by the variational\ninformation bottleneck. The complementary information learning procedure can be\nefficiently solved by variational inference and cross-modal spatial attention.\nNumerical results from the verification task and standard benchmarks indicate\nthat CIML efficiently removes redundant information between modalities,\noutperforming SOTA methods regarding validation accuracy and segmentation\neffect.\n",
    "link": "http://arxiv.org/abs/2401.02717v1"
  },
  {
    "title": "Graph-level Protein Representation Learning by Structure Knowledge\n  Refinement",
    "authors": "Ge Wang, Zelin Zang, Jiangbin Zheng, Jun Xia, Stan Z. Li",
    "abstract": "  This paper focuses on learning representation on the whole graph level in an\nunsupervised manner. Learning graph-level representation plays an important\nrole in a variety of real-world issues such as molecule property prediction,\nprotein structure feature extraction, and social network analysis. The\nmainstream method is utilizing contrastive learning to facilitate graph feature\nextraction, known as Graph Contrastive Learning (GCL). GCL, although effective,\nsuffers from some complications in contrastive learning, such as the effect of\nfalse negative pairs. Moreover, augmentation strategies in GCL are weakly\nadaptive to diverse graph datasets. Motivated by these problems, we propose a\nnovel framework called Structure Knowledge Refinement (SKR) which uses data\nstructure to determine the probability of whether a pair is positive or\nnegative. Meanwhile, we propose an augmentation strategy that naturally\npreserves the semantic meaning of the original data and is compatible with our\nSKR framework. Furthermore, we illustrate the effectiveness of our SKR\nframework through intuition and experiments. The experimental results on the\ntasks of graph-level classification demonstrate that our SKR framework is\nsuperior to most state-of-the-art baselines.\n",
    "link": "http://arxiv.org/abs/2401.02713v1"
  },
  {
    "title": "Synergistic Formulaic Alpha Generation for Quantitative Trading based on\n  Reinforcement Learning",
    "authors": "Hong-Gi Shin, Sukhyun Jeong, Eui-Yeon Kim, Sungho Hong, Young-Jin Cho, Yong-Hoon Choi",
    "abstract": "  Mining of formulaic alpha factors refers to the process of discovering and\ndeveloping specific factors or indicators (referred to as alpha factors) for\nquantitative trading in stock market. To efficiently discover alpha factors in\nvast search space, reinforcement learning (RL) is commonly employed. This paper\nproposes a method to enhance existing alpha factor mining approaches by\nexpanding a search space and utilizing pretrained formulaic alpha set as\ninitial seed values to generate synergistic formulaic alpha. We employ\ninformation coefficient (IC) and rank information coefficient (Rank IC) as\nperformance evaluation metrics for the model. Using CSI300 market data, we\nconducted real investment simulations and observed significant performance\nimprovement compared to existing techniques.\n",
    "link": "http://arxiv.org/abs/2401.02710v1"
  },
  {
    "title": "German Text Embedding Clustering Benchmark",
    "authors": "Silvan Wehrli, Bert Arnrich, Christopher Irrgang",
    "abstract": "  This work introduces a benchmark assessing the performance of clustering\nGerman text embeddings in different domains. This benchmark is driven by the\nincreasing use of clustering neural text embeddings in tasks that require the\ngrouping of texts (such as topic modeling) and the need for German resources in\nexisting benchmarks. We provide an initial analysis for a range of pre-trained\nmono- and multilingual models evaluated on the outcome of different clustering\nalgorithms. Results include strong performing mono- and multilingual models.\nReducing the dimensions of embeddings can further improve clustering.\nAdditionally, we conduct experiments with continued pre-training for German\nBERT models to estimate the benefits of this additional training. Our\nexperiments suggest that significant performance improvements are possible for\nshort text. All code and datasets are publicly available.\n",
    "link": "http://arxiv.org/abs/2401.02709v1"
  },
  {
    "title": "TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis",
    "authors": "Liwen Zhang, Lianzhen Zhong, Fan Yang, Di Dong, Hui Hui, Jie Tian",
    "abstract": "  A core challenge in survival analysis is to model the distribution of\ncensored time-to-event data, where the event of interest may be a death,\nfailure, or occurrence of a specific event. Previous studies have showed that\nranking and maximum likelihood estimation (MLE)loss functions are widely-used\nfor survival analysis. However, ranking loss only focus on the ranking of\nsurvival time and does not consider potential effect of samples for exact\nsurvival time values. Furthermore, the MLE is unbounded and easily subject to\noutliers (e.g., censored data), which may cause poor performance of modeling.\nTo handle the complexities of learning process and exploit valuable survival\ntime values, we propose a time-adaptive coordinate loss function, TripleSurv,\nto achieve adaptive adjustments by introducing the differences in the survival\ntime between sample pairs into the ranking, which can encourage the model to\nquantitatively rank relative risk of pairs, ultimately enhancing the accuracy\nof predictions. Most importantly, the TripleSurv is proficient in quantifying\nthe relative risk between samples by ranking ordering of pairs, and consider\nthe time interval as a trade-off to calibrate the robustness of model over\nsample distribution. Our TripleSurv is evaluated on three real-world survival\ndatasets and a public synthetic dataset. The results show that our method\noutperforms the state-of-the-art methods and exhibits good model performance\nand robustness on modeling various sophisticated data distributions with\ndifferent censor rates. Our code will be available upon acceptance.\n",
    "link": "http://arxiv.org/abs/2401.02708v1"
  },
  {
    "title": "XUAT-Copilot: Multi-Agent Collaborative System for Automated User\n  Acceptance Testing with Large Language Model",
    "authors": "Zhitao Wang, Wei Wang, Zirao Li, Long Wang, Can Yi, Xinjie Xu, Luyang Cao, Hanjing Su, Shouzhi Chen, Jun Zhou",
    "abstract": "  In past years, we have been dedicated to automating user acceptance testing\n(UAT) process of WeChat Pay, one of the most influential mobile payment\napplications in China. A system titled XUAT has been developed for this\npurpose. However, there is still a human-labor-intensive stage, i.e, test\nscripts generation, in the current system. Therefore, in this paper, we\nconcentrate on methods of boosting the automation level of the current system,\nparticularly the stage of test scripts generation. With recent notable\nsuccesses, large language models (LLMs) demonstrate significant potential in\nattaining human-like intelligence and there has been a growing research area\nthat employs LLMs as autonomous agents to obtain human-like decision-making\ncapabilities. Inspired by these works, we propose an LLM-powered multi-agent\ncollaborative system, named XUAT-Copilot, for automated UAT. The proposed\nsystem mainly consists of three LLM-based agents responsible for action\nplanning, state checking and parameter selecting, respectively, and two\nadditional modules for state sensing and case rewriting. The agents interact\nwith testing device, make human-like decision and generate action command in a\ncollaborative way. The proposed multi-agent system achieves a close\neffectiveness to human testers in our experimental studies and gains a\nsignificant improvement of Pass@1 accuracy compared with single-agent\narchitecture. More importantly, the proposed system has launched in the formal\ntesting environment of WeChat Pay mobile app, which saves a considerable amount\nof manpower in the daily development work.\n",
    "link": "http://arxiv.org/abs/2401.02705v1"
  },
  {
    "title": "Verifying Relational Explanations: A Probabilistic Approach",
    "authors": "Abisha Thapa Magar, Anup Shakya, Somdeb Sarkhel, Deepak Venugopal",
    "abstract": "  Explanations on relational data are hard to verify since the explanation\nstructures are more complex (e.g. graphs). To verify interpretable explanations\n(e.g. explanations of predictions made in images, text, etc.), typically human\nsubjects are used since it does not necessarily require a lot of expertise.\nHowever, to verify the quality of a relational explanation requires expertise\nand is hard to scale-up. GNNExplainer is arguably one of the most popular\nexplanation methods for Graph Neural Networks. In this paper, we develop an\napproach where we assess the uncertainty in explanations generated by\nGNNExplainer. Specifically, we ask the explainer to generate explanations for\nseveral counterfactual examples. We generate these examples as symmetric\napproximations of the relational structure in the original data. From these\nexplanations, we learn a factor graph model to quantify uncertainty in an\nexplanation. Our results on several datasets show that our approach can help\nverify explanations from GNNExplainer by reliably estimating the uncertainty of\na relation specified in the explanation.\n",
    "link": "http://arxiv.org/abs/2401.02703v1"
  },
  {
    "title": "Geometric-Facilitated Denoising Diffusion Model for 3D Molecule\n  Generation",
    "authors": "Can Xu, Haosen Wang, Weigang Wang, Pengfei Zheng, Hongyang Chen",
    "abstract": "  Denoising diffusion models have shown great potential in multiple research\nareas. Existing diffusion-based generative methods on de novo 3D molecule\ngeneration face two major challenges. Since majority heavy atoms in molecules\nallow connections to multiple atoms through single bonds, solely using\npair-wise distance to model molecule geometries is insufficient. Therefore, the\nfirst one involves proposing an effective neural network as the denoising\nkernel that is capable to capture complex multi-body interatomic relationships\nand learn high-quality features. Due to the discrete nature of graphs,\nmainstream diffusion-based methods for molecules heavily rely on predefined\nrules and generate edges in an indirect manner. The second challenge involves\naccommodating molecule generation to diffusion and accurately predicting the\nexistence of bonds. In our research, we view the iterative way of updating\nmolecule conformations in diffusion process is consistent with molecular\ndynamics and introduce a novel molecule generation method named\nGeometric-Facilitated Molecular Diffusion (GFMDiff). For the first challenge,\nwe introduce a Dual-Track Transformer Network (DTN) to fully excevate global\nspatial relationships and learn high quality representations which contribute\nto accurate predictions of features and geometries. As for the second\nchallenge, we design Geometric-Facilitated Loss (GFLoss) which intervenes the\nformation of bonds during the training period, instead of directly embedding\nedges into the latent space. Comprehensive experiments on current benchmarks\ndemonstrate the superiority of GFMDiff.\n",
    "link": "http://arxiv.org/abs/2401.02683v1"
  },
  {
    "title": "Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer\n  Level Loss",
    "authors": "Yatharth Gupta, Vishnu V. Jaddipal, Harish Prabhala, Sayak Paul, Patrick Von Platen",
    "abstract": "  Stable Diffusion XL (SDXL) has become the best open source text-to-image\nmodel (T2I) for its versatility and top-notch image quality. Efficiently\naddressing the computational demands of SDXL models is crucial for wider reach\nand applicability. In this work, we introduce two scaled-down variants, Segmind\nStable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter\nUNets, respectively, achieved through progressive removal using layer-level\nlosses focusing on reducing the model size while preserving generative quality.\nWe release these models weights at https://hf.co/Segmind. Our methodology\ninvolves the elimination of residual networks and transformer blocks from the\nU-Net structure of SDXL, resulting in significant reductions in parameters, and\nlatency. Our compact models effectively emulate the original SDXL by\ncapitalizing on transferred knowledge, achieving competitive results against\nlarger multi-billion parameter SDXL. Our work underscores the efficacy of\nknowledge distillation coupled with layer-level losses in reducing model size\nwhile preserving the high-quality generative capabilities of SDXL, thus\nfacilitating more accessible deployment in resource-constrained environments.\n",
    "link": "http://arxiv.org/abs/2401.02677v1"
  },
  {
    "title": "A unified multichannel far-field speech recognition system: combining\n  neural beamforming with attention based end-to-end model",
    "authors": "Dongdi Zhao, Jianbo Ma, Lu Lu, Jinke Li, Xuan Ji, Lei Zhu, Fuming Fang, Ming Liu, Feijun Jiang",
    "abstract": "  Far-field speech recognition is a challenging task that conventionally uses\nsignal processing beamforming to attack noise and interference problem. But the\nperformance has been found usually limited due to heavy reliance on\nenvironmental assumption. In this paper, we propose a unified multichannel\nfar-field speech recognition system that combines the neural beamforming and\ntransformer-based Listen, Spell, Attend (LAS) speech recognition system, which\nextends the end-to-end speech recognition system further to include speech\nenhancement. Such framework is then jointly trained to optimize the final\nobjective of interest. Specifically, factored complex linear projection (fCLP)\nhas been adopted to form the neural beamforming. Several pooling strategies to\ncombine look directions are then compared in order to find the optimal\napproach. Moreover, information of the source direction is also integrated in\nthe beamforming to explore the usefulness of source direction as a prior, which\nis usually available especially in multi-modality scenario. Experiments on\ndifferent microphone array geometry are conducted to evaluate the robustness\nagainst spacing variance of microphone array. Large in-house databases are used\nto evaluate the effectiveness of the proposed framework and the proposed method\nachieve 19.26\\% improvement when compared with a strong baseline.\n",
    "link": "http://arxiv.org/abs/2401.02673v1"
  },
  {
    "title": "Zero-shot Microclimate Prediction with Deep Learning",
    "authors": "Iman Deznabi, Peeyush Kumar, Madalina Fiterau",
    "abstract": "  Weather station data is a valuable resource for climate prediction, however,\nits reliability can be limited in remote locations. To compound the issue,\nmaking local predictions often relies on sensor data that may not be accessible\nfor a new, previously unmonitored location. In response to these challenges, we\npropose a novel zero-shot learning approach designed to forecast various\nclimate measurements at new and unmonitored locations. Our method surpasses\nconventional weather forecasting techniques in predicting microclimate\nvariables by leveraging knowledge extracted from other geographic locations.\n",
    "link": "http://arxiv.org/abs/2401.02665v1"
  },
  {
    "title": "A backdoor attack against link prediction tasks with graph neural\n  networks",
    "authors": "Jiazhu Dai, Haoyu Sun",
    "abstract": "  Graph Neural Networks (GNNs) are a class of deep learning models capable of\nprocessing graph-structured data, and they have demonstrated significant\nperformance in a variety of real-world applications. Recent studies have found\nthat GNN models are vulnerable to backdoor attacks. When specific patterns\n(called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input\ndata, the backdoor embedded in the GNN models is activated, which misclassifies\nthe input data into the target class label specified by the attacker, whereas\nwhen there are no backdoor triggers in the input, the backdoor embedded in the\nGNN models is not activated, and the models work normally. Backdoor attacks are\nhighly stealthy and expose GNN models to serious security risks. Currently,\nresearch on backdoor attacks against GNNs mainly focus on tasks such as graph\nclassification and node classification, and backdoor attacks against link\nprediction tasks are rarely studied. In this paper, we propose a backdoor\nattack against the link prediction tasks based on GNNs and reveal the existence\nof such security vulnerability in GNN models, which make the backdoored GNN\nmodels to incorrectly predict unlinked two nodes as having a link relationship\nwhen a trigger appear. The method uses a single node as the trigger and poison\nselected node pairs in the training graph, and then the backdoor will be\nembedded in the GNN models through the training process. In the inference\nstage, the backdoor in the GNN models can be activated by simply linking the\ntrigger node to the two end nodes of the unlinked node pairs in the input data,\ncausing the GNN models to produce incorrect link prediction results for the\ntarget node pairs.\n",
    "link": "http://arxiv.org/abs/2401.02663v1"
  },
  {
    "title": "Nurse-in-the-Loop Artificial Intelligence for Precision Management of\n  Type 2 Diabetes in a Clinical Trial Utilizing Transfer-Learned Predictive\n  Digital Twin",
    "authors": "Syed Hasib Akhter Faruqui, Adel Alaeddini, Yan Du, Shiyu Li, Kumar Sharma, Jing Wang",
    "abstract": "  Background: Type 2 diabetes (T2D) is a prevalent chronic disease with a\nsignificant risk of serious health complications and negative impacts on the\nquality of life. Given the impact of individual characteristics and lifestyle\non the treatment plan and patient outcomes, it is crucial to develop precise\nand personalized management strategies. Artificial intelligence (AI) provides\ngreat promise in combining patterns from various data sources with nurses'\nexpertise to achieve optimal care. Methods: This is a 6-month ancillary study\namong T2D patients (n = 20, age = 57 +- 10). Participants were randomly\nassigned to an intervention (AI, n=10) group to receive daily AI-generated\nindividualized feedback or a control group without receiving the daily feedback\n(non-AI, n=10) in the last three months. The study developed an online\nnurse-in-the-loop predictive control (ONLC) model that utilizes a predictive\ndigital twin (PDT). The PDT was developed using a transfer-learning-based\nArtificial Neural Network. The PDT was trained on participants self-monitoring\ndata (weight, food logs, physical activity, glucose) from the first three\nmonths, and the online control algorithm applied particle swarm optimization to\nidentify impactful behavioral changes for maintaining the patient's glucose and\nweight levels for the next three months. The ONLC provided the intervention\ngroup with individualized feedback and recommendations via text messages. The\nPDT was re-trained weekly to improve its performance. Findings: The trained\nONLC model achieved &gt;=80% prediction accuracy across all patients while the\nmodel was tuned online. Participants in the intervention group exhibited a\ntrend of improved daily steps and stable or improved total caloric and total\ncarb intake as recommended.\n",
    "link": "http://arxiv.org/abs/2401.02661v1"
  },
  {
    "title": "A Deep Q-Learning based Smart Scheduling of EVs for Demand Response in\n  Smart Grids",
    "authors": "Viorica Rozina Chifu, Tudor Cioara, Cristina Bianca Pop, Horia Rusu, Ionut Anghel",
    "abstract": "  Economic and policy factors are driving the continuous increase in the\nadoption and usage of electrical vehicles (EVs). However, despite being a\ncleaner alternative to combustion engine vehicles, EVs have negative impacts on\nthe lifespan of microgrid equipment and energy balance due to increased power\ndemand and the timing of their usage. In our view grid management should\nleverage on EVs scheduling flexibility to support local network balancing\nthrough active participation in demand response programs. In this paper, we\npropose a model-free solution, leveraging Deep Q-Learning to schedule the\ncharging and discharging activities of EVs within a microgrid to align with a\ntarget energy profile provided by the distribution system operator. We adapted\nthe Bellman Equation to assess the value of a state based on specific rewards\nfor EV scheduling actions and used a neural network to estimate Q-values for\navailable actions and the epsilon-greedy algorithm to balance exploitation and\nexploration to meet the target energy profile. The results are promising\nshowing that the proposed solution can effectively schedule the EVs charging\nand discharging actions to align with the target profile with a Person\ncoefficient of 0.99, handling effective EVs scheduling situations that involve\ndynamicity given by the e-mobility features, relying only on data with no\nknowledge of EVs and microgrid dynamics.\n",
    "link": "http://arxiv.org/abs/2401.02653v1"
  },
  {
    "title": "Adaptive Discounting of Training Time Attacks",
    "authors": "Ridhima Bector, Abhay Aradhya, Chai Quek, Zinovi Rabinovich",
    "abstract": "  Among the most insidious attacks on Reinforcement Learning (RL) solutions are\ntraining-time attacks (TTAs) that create loopholes and backdoors in the learned\nbehaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are\nnow available, where the attacker forces a specific, target behaviour upon a\ntraining RL agent (victim). However, even state-of-the-art C-TTAs focus on\ntarget behaviours that could be naturally adopted by the victim if not for a\nparticular feature of the environment dynamics, which C-TTAs exploit. In this\nwork, we show that a C-TTA is possible even when the target behaviour is\nun-adoptable due to both environment dynamics as well as non-optimality with\nrespect to the victim objective(s). To find efficient attacks in this context,\nwe develop a specialised flavour of the DDPG algorithm, which we term\ngammaDDPG, that learns this stronger version of C-TTA. gammaDDPG dynamically\nalters the attack policy planning horizon based on the victim's current\nbehaviour. This improves effort distribution throughout the attack timeline and\nreduces the effect of uncertainty the attacker has about the victim. To\ndemonstrate the features of our method and better relate the results to prior\nresearch, we borrow a 3D grid domain from a state-of-the-art C-TTA for our\nexperiments. Code is available at \"bit.ly/github-rb-gDDPG\".\n",
    "link": "http://arxiv.org/abs/2401.02652v1"
  },
  {
    "title": "Simple Hierarchical Planning with Diffusion",
    "authors": "Chang Chen, Fei Deng, Kenji Kawaguchi, Caglar Gulcehre, Sungjin Ahn",
    "abstract": "  Diffusion-based generative methods have proven effective in modeling\ntrajectories with offline datasets. However, they often face computational\nchallenges and can falter in generalization, especially in capturing temporal\nabstractions for long-horizon tasks. To overcome this, we introduce the\nHierarchical Diffuser, a simple, fast, yet surprisingly effective planning\nmethod combining the advantages of hierarchical and diffusion-based planning.\nOur model adopts a \"jumpy\" planning strategy at the higher level, which allows\nit to have a larger receptive field but at a lower computational cost -- a\ncrucial factor for diffusion-based planning methods, as we have empirically\nverified. Additionally, the jumpy sub-goals guide our low-level planner,\nfacilitating a fine-tuning stage and further improving our approach's\neffectiveness. We conducted empirical evaluations on standard offline\nreinforcement learning benchmarks, demonstrating our method's superior\nperformance and efficiency in terms of training and planning speed compared to\nthe non-hierarchical Diffuser as well as other hierarchical planning methods.\nMoreover, we explore our model's generalization capability, particularly on how\nour method improves generalization capabilities on compositional\nout-of-distribution tasks.\n",
    "link": "http://arxiv.org/abs/2401.02644v1"
  },
  {
    "title": "Training and Serving System of Foundation Models: A Comprehensive Survey",
    "authors": "Jiahang Zhou, Yanyu Chen, Zicong Hong, Wuhui Chen, Yue Yu, Tao Zhang, Hui Wang, Chuanfu Zhang, Zibin Zheng",
    "abstract": "  Foundation models (e.g., ChatGPT, DALL-E, PengCheng Mind, PanGu-$\\Sigma$)\nhave demonstrated extraordinary performance in key technological areas, such as\nnatural language processing and visual recognition, and have become the\nmainstream trend of artificial general intelligence. This has led more and more\nmajor technology giants to dedicate significant human and financial resources\nto actively develop their foundation model systems, which drives continuous\ngrowth of these models' parameters. As a result, the training and serving of\nthese models have posed significant challenges, including substantial computing\npower, memory consumption, bandwidth demands, etc. Therefore, employing\nefficient training and serving strategies becomes particularly crucial. Many\nresearchers have actively explored and proposed effective methods. So, a\ncomprehensive survey of them is essential for system developers and\nresearchers. This paper extensively explores the methods employed in training\nand serving foundation models from various perspectives. It provides a detailed\ncategorization of these state-of-the-art methods, including finer aspects such\nas network, computing, and storage. Additionally, the paper summarizes the\nchallenges and presents a perspective on the future development direction of\nfoundation model systems. Through comprehensive discussion and analysis, it\nhopes to provide a solid theoretical basis and practical guidance for future\nresearch and applications, promoting continuous innovation and development in\nfoundation model systems.\n",
    "link": "http://arxiv.org/abs/2401.02643v1"
  },
  {
    "title": "Characteristics and prevalence of fake social media profiles with\n  AI-generated faces",
    "authors": "Kai-Cheng Yang, Danishjeet Singh, Filippo Menczer",
    "abstract": "  Recent advancements in generative artificial intelligence (AI) have raised\nconcerns about their potential to create convincing fake social media accounts,\nbut empirical evidence is lacking. In this paper, we present a systematic\nanalysis of Twitter(X) accounts using human faces generated by Generative\nAdversarial Networks (GANs) for their profile pictures. We present a dataset of\n1,353 such accounts and show that they are used to spread scams, spam, and\namplify coordinated messages, among other inauthentic activities. Leveraging a\nfeature of GAN-generated faces -- consistent eye placement -- and supplementing\nit with human annotation, we devise an effective method for identifying\nGAN-generated profiles in the wild. Applying this method to a random sample of\nactive Twitter users, we estimate a lower bound for the prevalence of profiles\nusing GAN-generated faces between 0.021% and 0.044% -- around 10K daily active\naccounts. These findings underscore the emerging threats posed by multimodal\ngenerative AI. We release the source code of our detection method and the data\nwe collect to facilitate further investigation. Additionally, we provide\npractical heuristics to assist social media users in recognizing such accounts.\n",
    "link": "http://arxiv.org/abs/2401.02627v1"
  },
  {
    "title": "Progress and Prospects in 3D Generative AI: A Technical Overview\n  including 3D human",
    "authors": "Song Bai, Jie Li",
    "abstract": "  While AI-generated text and 2D images continue to expand its territory, 3D\ngeneration has gradually emerged as a trend that cannot be ignored. Since the\nyear 2023 an abundant amount of research papers has emerged in the domain of 3D\ngeneration. This growth encompasses not just the creation of 3D objects, but\nalso the rapid development of 3D character and motion generation. Several key\nfactors contribute to this progress. The enhanced fidelity in stable diffusion,\ncoupled with control methods that ensure multi-view consistency, and realistic\nhuman models like SMPL-X, contribute synergistically to the production of 3D\nmodels with remarkable consistency and near-realistic appearances. The\nadvancements in neural network-based 3D storing and rendering models, such as\nNeural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have\naccelerated the efficiency and realism of neural rendered models. Furthermore,\nthe multimodality capabilities of large language models have enabled language\ninputs to transcend into human motion outputs. This paper aims to provide a\ncomprehensive overview and summary of the relevant papers published mostly\nduring the latter half year of 2023. It will begin by discussing the AI\ngenerated object models in 3D, followed by the generated 3D human models, and\nfinally, the generated 3D human motions, culminating in a conclusive summary\nand a vision for the future.\n",
    "link": "http://arxiv.org/abs/2401.02620v1"
  },
  {
    "title": "Neural Causal Abstractions",
    "authors": "Kevin Xia, Elias Bareinboim",
    "abstract": "  The abilities of humans to understand the world in terms of cause and effect\nrelationships, as well as to compress information into abstract concepts, are\ntwo hallmark features of human intelligence. These two topics have been studied\nin tandem in the literature under the rubric of causal abstractions theory. In\npractice, it remains an open problem how to best leverage abstraction theory in\nreal-world causal inference tasks, where the true mechanisms are unknown and\nonly limited data is available. In this paper, we develop a new family of\ncausal abstractions by clustering variables and their domains. This approach\nrefines and generalizes previous notions of abstractions to better accommodate\nindividual causal distributions that are spawned by Pearl's causal hierarchy.\nWe show that such abstractions are learnable in practical settings through\nNeural Causal Models (Xia et al., 2021), enabling the use of the deep learning\ntoolkit to solve various challenging causal inference tasks -- identification,\nestimation, sampling -- at different levels of granularity. Finally, we\nintegrate these results with representation learning to create more flexible\nabstractions, moving these results closer to practical applications. Our\nexperiments support the theory and illustrate how to scale causal inferences to\nhigh-dimensional settings involving image data.\n",
    "link": "http://arxiv.org/abs/2401.02602v1"
  },
  {
    "title": "Object-oriented backdoor attack against image captioning",
    "authors": "Meiling Li, Nan Zhong, Xinpeng Zhang, Zhenxing Qian, Sheng Li",
    "abstract": "  Backdoor attack against image classification task has been widely studied and\nproven to be successful, while there exist little research on the backdoor\nattack against vision-language models. In this paper, we explore backdoor\nattack towards image captioning models by poisoning training data. Assuming the\nattacker has total access to the training dataset, and cannot intervene in\nmodel construction or training process. Specifically, a portion of benign\ntraining samples is randomly selected to be poisoned. Afterwards, considering\nthat the captions are usually unfolded around objects in an image, we design an\nobject-oriented method to craft poisons, which aims to modify pixel values by a\nslight range with the modification number proportional to the scale of the\ncurrent detected object region. After training with the poisoned data, the\nattacked model behaves normally on benign images, but for poisoned images, the\nmodel will generate some sentences irrelevant to the given image. The attack\ncontrols the model behavior on specific test images without sacrificing the\ngeneration performance on benign test images. Our method proves the weakness of\nimage captioning models to backdoor attack and we hope this work can raise the\nawareness of defending against backdoor attack in the image captioning field.\n",
    "link": "http://arxiv.org/abs/2401.02600v1"
  },
  {
    "title": "Synthetic Information towards Maximum Posterior Ratio for deep learning\n  on Imbalanced Data",
    "authors": "Hung Nguyen, Morris Chang",
    "abstract": "  This study examines the impact of class-imbalanced data on deep learning\nmodels and proposes a technique for data balancing by generating synthetic data\nfor the minority class. Unlike random-based oversampling, our method\nprioritizes balancing the informative regions by identifying high entropy\nsamples. Generating well-placed synthetic data can enhance machine learning\nalgorithms accuracy and efficiency, whereas poorly-placed ones may lead to\nhigher misclassification rates. We introduce an algorithm that maximizes the\nprobability of generating a synthetic sample in the correct region of its class\nby optimizing the class posterior ratio. Additionally, to maintain data\ntopology, synthetic data are generated within each minority sample's\nneighborhood. Our experimental results on forty-one datasets demonstrate the\nsuperior performance of our technique in enhancing deep-learning models.\n",
    "link": "http://arxiv.org/abs/2401.02591v1"
  },
  {
    "title": "Identification of 4FGL uncertain sources at Higher Resolutions with\n  Inverse Discrete Wavelet Transform",
    "authors": "Haitao Cao, Hubing Xiao, Zhijian Luo, Xiangtao Zeng, Junhui Fan",
    "abstract": "  In the forthcoming era of big astronomical data, it is a burden to find out\ntarget sources from ground-based and space-based telescopes. Although Machine\nLearning (ML) methods have been extensively utilized to address this issue, the\nincorporation of in-depth data analysis can significantly enhance the\nefficiency of identifying target sources when dealing with massive volumes of\nastronomical data. In this work, we focused on the task of finding AGN\ncandidates and identifying BL Lac/FSRQ candidates from the 4FGL DR3 uncertain\nsources. We studied the correlations among the attributes of the 4FGL DR3\ncatalogue and proposed a novel method, named FDIDWT, to transform the original\ndata. The transformed dataset is characterized as low-dimensional and\nfeature-highlighted, with the estimation of correlation features by Fractal\nDimension (FD) theory and the multi-resolution analysis by Inverse Discrete\nWavelet Transform (IDWT). Combining the FDIDWT method with an improved\nlightweight MatchboxConv1D model, we accomplished two missions: (1) to\ndistinguish the Active Galactic Nuclei (AGNs) from others (Non-AGNs) in the\n4FGL DR3 uncertain sources with an accuracy of 96.65%, namely, Mission A; (2)\nto classify blazar candidates of uncertain type (BCUs) into BL Lacertae objects\n(BL Lacs) or Flat Spectrum Radio Quasars (FSRQs) with an accuracy of 92.03%,\nnamely, Mission B. There are 1354 AGN candidates in Mission A, 482 BL Lacs\ncandidates and 128 FSRQ candidates in Mission B were found. The results show a\nhigh consistency of greater than 98% with the results in previous works. In\naddition, our method has the advantage of finding less variable and relatively\nfaint sources than ordinary methods.\n",
    "link": "http://arxiv.org/abs/2401.02589v1"
  },
  {
    "title": "Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting",
    "authors": "Van Minh Nguyen, Emma Sandidge, Trupti Mahendrakar, Ryan T. White",
    "abstract": "  The accelerating deployment of spacecraft in orbit have generated interest in\non-orbit servicing (OOS), inspection of spacecraft, and active debris removal\n(ADR). Such missions require precise rendezvous and proximity operations in the\nvicinity of non-cooperative, possible unknown, resident space objects. Safety\nconcerns with manned missions and lag times with ground-based control\nnecessitate complete autonomy. This requires robust characterization of the\ntarget's geometry. In this article, we present an approach for mapping\ngeometries of satellites on orbit based on 3D Gaussian Splatting that can run\non computing resources available on current spaceflight hardware. We\ndemonstrate model training and 3D rendering performance on a\nhardware-in-the-loop satellite mock-up under several realistic lighting and\nmotion conditions. Our model is shown to be capable of training on-board and\nrendering higher quality novel views of an unknown satellite nearly 2 orders of\nmagnitude faster than previous NeRF-based algorithms. Such on-board\ncapabilities are critical to enable downstream machine intelligence tasks\nnecessary for autonomous guidance, navigation, and control tasks.\n",
    "link": "http://arxiv.org/abs/2401.02588v1"
  },
  {
    "title": "Federated Learning for distribution skewed data using sample weights",
    "authors": "Hung Nguyen, Peiyuan Wu, Morris Chang",
    "abstract": "  One of the most challenging issues in federated learning is that the data is\noften not independent and identically distributed (nonIID). Clients are\nexpected to contribute the same type of data and drawn from one global\ndistribution. However, data are often collected in different ways from\ndifferent resources. Thus, the data distributions among clients might be\ndifferent from the underlying global distribution. This creates a weight\ndivergence issue and reduces federated learning performance. This work focuses\non improving federated learning performance for skewed data distribution across\nclients. The main idea is to adjust the client distribution closer to the\nglobal distribution using sample weights. Thus, the machine learning model\nconverges faster with higher accuracy. We start from the fundamental concept of\nempirical risk minimization and theoretically derive a solution for adjusting\nthe distribution skewness using sample weights. To determine sample weights, we\nimplicitly exchange density information by leveraging a neural network-based\ndensity estimation model, MADE. The clients data distribution can then be\nadjusted without exposing their raw data. Our experiment results on three\nreal-world datasets show that the proposed method not only improves federated\nlearning accuracy but also significantly reduces communication costs compared\nto the other experimental methods.\n",
    "link": "http://arxiv.org/abs/2401.02586v1"
  }
]