[
  {
    "title": "Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar\n  Creation",
    "authors": "Xiyi Chen, Marko Mihajlovic, Shaofei Wang, Sergey Prokudin, Siyu Tang",
    "abstract": "  Recent advances in generative diffusion models have enabled the previously\nunfeasible capability of generating 3D assets from a single input image or a\ntext prompt. In this work, we aim to enhance the quality and functionality of\nthese models for the task of creating controllable, photorealistic human\navatars. We achieve this by integrating a 3D morphable model into the\nstate-of-the-art multiview-consistent diffusion approach. We demonstrate that\naccurate conditioning of a generative pipeline on the articulated 3D model\nenhances the baseline model performance on the task of novel view synthesis\nfrom a single image. More importantly, this integration facilitates a seamless\nand accurate incorporation of facial expression and body pose control into the\ngeneration process. To the best of our knowledge, our proposed framework is the\nfirst diffusion model to enable the creation of fully 3D-consistent,\nanimatable, and photorealistic human avatars from a single image of an unseen\nsubject; extensive quantitative and qualitative evaluations demonstrate the\nadvantages of our approach over existing state-of-the-art avatar creation\nmodels on both novel view and novel expression synthesis tasks.\n",
    "link": "http://arxiv.org/abs/2401.04728v1"
  },
  {
    "title": "RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation",
    "authors": "Mahdi Nikdan, Soroush Tabesh, Dan Alistarh",
    "abstract": "  We investigate parameter-efficient fine-tuning (PEFT) methods that can\nprovide good accuracy under limited computational and memory budgets in the\ncontext of large language models (LLMs). We present a new PEFT method called\nRobust Adaptation (RoSA) inspired by robust principal component analysis (PCA)\nthat jointly trains $\\textit{low-rank}$ and $\\textit{highly-sparse}$ components\non top of a set of fixed pretrained weights to efficiently approximate the\nperformance of a full-fine-tuning (FFT) solution. Across a series of\nchallenging generative tasks such as grade-school math and SQL query\ngeneration, which require fine-tuning for good performance, we show that RoSA\noutperforms both LoRA and pure sparse fine-tuning, at the same parameter\nbudget. We provide system support for RoSA to complement the training\nalgorithm, specifically in the form of sparse GPU kernels which enable memory-\nand computationally-efficient training. Our code will be made available at\nhttps://github.com/IST-DASLab/RoSA}{\\texttt{https://github.com/IST-DASLab/RoSA\n",
    "link": "http://arxiv.org/abs/2401.04679v1"
  },
  {
    "title": "Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA\n  Cats and Dogs Dataset",
    "authors": "Galib Muhammad Shahriar Himel, Md. Masudul Islam",
    "abstract": "  As the most basic application and implementation of deep learning, image\nclassification has grown in popularity. Various datasets are provided by\nrenowned data science communities for benchmarking machine learning algorithms\nand pre-trained models. The ASSIRA Cats &amp; Dogs dataset is one of them and is\nbeing used in this research for its overall acceptance and benchmark standards.\nA comparison of various pre-trained models is demonstrated by using different\ntypes of optimizers and loss functions. Hyper-parameters are changed to gain\nthe best result from a model. By applying this approach, we have got higher\naccuracy without major changes in the training model. To run the experiment, we\nused three different computer architectures: a laptop equipped with NVIDIA\nGeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a\ndesktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate\nsupremacy in terms of accuracy over the previously done experiments on this\ndataset. From this experiment, the highest accuracy which is 99.65% is gained\nusing the NASNet Large.\n",
    "link": "http://arxiv.org/abs/2401.04666v1"
  },
  {
    "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence\n  Lengths in Large Language Models",
    "authors": "Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong",
    "abstract": "  Linear attention is an efficient attention mechanism that has recently\nemerged as a promising alternative to conventional softmax attention. With its\nability to process tokens in linear computational complexities, linear\nattention, in theory, can handle sequences of unlimited length without\nsacrificing speed, i.e., maintaining a constant training speed for various\nsequence lengths with a fixed memory consumption. However, due to the issue\nwith cumulative summation (cumsum), current linear attention algorithms cannot\ndemonstrate their theoretical advantage in a causal setting. In this paper, we\npresent Lightning Attention-2, the first linear attention implementation that\nenables linear attention to realize its theoretical computational benefits. To\nachieve this, we leverage the thought of tiling, separately handling the\nintra-block and inter-block components in linear attention calculation.\nSpecifically, we utilize the conventional attention computation mechanism for\nthe intra-blocks and apply linear attention kernel tricks for the inter-blocks.\nA tiling technique is adopted through both forward and backward procedures to\ntake full advantage of the GPU hardware. We implement our algorithm in Triton\nto make it IO-aware and hardware-friendly. Various experiments are conducted on\ndifferent model sizes and sequence lengths. Lightning Attention-2 retains\nconsistent training and inference speed regardless of input sequence length and\nis significantly faster than other attention mechanisms. The source code is\navailable at https://github.com/OpenNLPLab/lightning-attention.\n",
    "link": "http://arxiv.org/abs/2401.04658v1"
  },
  {
    "title": "A novel framework for generalization of deep hidden physics models",
    "authors": "Vijay Kag, Birupaksha Pal",
    "abstract": "  Modelling of systems where the full system information is unknown is an oft\nencountered problem for various engineering and industrial applications, as\nit's either impossible to consider all the complex physics involved or simpler\nmodels are considered to keep within the limits of the available resources.\nRecent advances in greybox modelling like the deep hidden physics models\naddress this space by combining data and physics. However, for most real-life\napplications, model generalizability is a key issue, as retraining a model for\nevery small change in system inputs and parameters or modification in domain\nconfiguration can render the model economically unviable. In this work we\npresent a novel enhancement to the idea of hidden physics models which can\ngeneralize for changes in system inputs, parameters and domains. We also show\nthat this approach holds promise in system discovery as well and helps learn\nthe hidden physics for the changed system inputs, parameters and domain\nconfiguration.\n",
    "link": "http://arxiv.org/abs/2401.04648v1"
  },
  {
    "title": "Advancing Ante-Hoc Explainable Models through Generative Adversarial\n  Networks",
    "authors": "Tanmay Garg, Deepika Vemuri, Vineeth N Balasubramanian",
    "abstract": "  This paper presents a novel concept learning framework for enhancing model\ninterpretability and performance in visual classification tasks. Our approach\nappends an unsupervised explanation generator to the primary classifier network\nand makes use of adversarial training. During training, the explanation module\nis optimized to extract visual concepts from the classifier's latent\nrepresentations, while the GAN-based module aims to discriminate images\ngenerated from concepts, from true images. This joint training scheme enables\nthe model to implicitly align its internally learned concepts with\nhuman-interpretable visual properties. Comprehensive experiments demonstrate\nthe robustness of our approach, while producing coherent concept activations.\nWe analyse the learned concepts, showing their semantic concordance with object\nparts and visual attributes. We also study how perturbations in the adversarial\ntraining protocol impact both classification and concept acquisition. In\nsummary, this work presents a significant step towards building inherently\ninterpretable deep vision models with task-aligned concept representations - a\nkey enabler for developing trustworthy AI for real-world perception tasks.\n",
    "link": "http://arxiv.org/abs/2401.04647v1"
  },
  {
    "title": "Applying Large Language Models API to Issue Classification Problem",
    "authors": "Gabriel Aracena, Kyle Luster, Fabio Santos, Igor Steinmacher, Marco A. Gerosa",
    "abstract": "  Effective prioritization of issue reports is crucial in software engineering\nto optimize resource allocation and address critical problems promptly.\nHowever, the manual classification of issue reports for prioritization is\nlaborious and lacks scalability. Alternatively, many open source software (OSS)\nprojects employ automated processes for this task, albeit relying on\nsubstantial datasets for adequate training. This research seeks to devise an\nautomated approach that ensures reliability in issue prioritization, even when\ntrained on smaller datasets. Our proposed methodology harnesses the power of\nGenerative Pre-trained Transformers (GPT), recognizing their potential to\nefficiently handle this task. By leveraging the capabilities of such models, we\naim to develop a robust system for prioritizing issue reports accurately,\nmitigating the necessity for extensive training data while maintaining\nreliability. In our research, we have developed a reliable GPT-based approach\nto accurately label and prioritize issue reports with a reduced training\ndataset. By reducing reliance on massive data requirements and focusing on\nfew-shot fine-tuning, our methodology offers a more accessible and efficient\nsolution for issue prioritization in software engineering. Our model predicted\nissue types in individual projects up to 93.2% in precision, 95% in recall, and\n89.3% in F1-score.\n",
    "link": "http://arxiv.org/abs/2401.04637v1"
  },
  {
    "title": "Deep Reinforcement Multi-agent Learning framework for Information\n  Gathering with Local Gaussian Processes for Water Monitoring",
    "authors": "Samuel Yanes Luis, Dmitriy Shutin, Juan Marchal G\u00f3mez, Daniel Guti\u00e9rrez Reina, Sergio Toral Mar\u00edn",
    "abstract": "  The conservation of hydrological resources involves continuously monitoring\ntheir contamination. A multi-agent system composed of autonomous surface\nvehicles is proposed in this paper to efficiently monitor the water quality. To\nachieve a safe control of the fleet, the fleet policy should be able to act\nbased on measurements and to the the fleet state. It is proposed to use Local\nGaussian Processes and Deep Reinforcement Learning to jointly obtain effective\nmonitoring policies. Local Gaussian processes, unlike classical global Gaussian\nprocesses, can accurately model the information in a dissimilar spatial\ncorrelation which captures more accurately the water quality information. A\nDeep convolutional policy is proposed, that bases the decisions on the\nobservation on the mean and variance of this model, by means of an information\ngain reward. Using a Double Deep Q-Learning algorithm, agents are trained to\nminimize the estimation error in a safe manner thanks to a Consensus-based\nheuristic. Simulation results indicate an improvement of up to 24% in terms of\nthe mean absolute error with the proposed models. Also, training results with\n1-3 agents indicate that our proposed approach returns 20% and 24% smaller\naverage estimation errors for, respectively, monitoring water quality variables\nand monitoring algae blooms, as compared to state-of-the-art approaches\n",
    "link": "http://arxiv.org/abs/2401.04631v1"
  },
  {
    "title": "DebugBench: Evaluating Debugging Capability of Large Language Models",
    "authors": "Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Zhiyuan Liu, Maosong Sun",
    "abstract": "  Large Language Models (LLMs) have demonstrated exceptional coding capability.\nHowever, as another critical component of programming proficiency, the\ndebugging capability of LLMs remains relatively unexplored. Previous\nevaluations of LLMs' debugging ability are significantly limited by the risk of\ndata leakage, the scale of the dataset, and the variety of tested bugs. To\novercome these deficiencies, we introduce `DebugBench', an LLM debugging\nbenchmark consisting of 4,253 instances. It covers four major bug categories\nand 18 minor types in C++, Java, and Python. To construct DebugBench, we\ncollect code snippets from the LeetCode community, implant bugs into source\ndata with GPT-4, and assure rigorous quality checks. We evaluate two commercial\nand three open-source models in a zero-shot scenario. We find that (1) while\nclosed-source models like GPT-4 exhibit inferior debugging performance compared\nto humans, open-source models such as Code Llama fail to attain any pass rate\nscores; (2) the complexity of debugging notably fluctuates depending on the bug\ncategory; (3) incorporating runtime feedback has a clear impact on debugging\nperformance which is not always helpful. As an extension, we also compare LLM\ndebugging and code generation, revealing a strong correlation between them for\nclosed-source models. These findings will benefit the development of LLMs in\ndebugging.\n",
    "link": "http://arxiv.org/abs/2401.04621v1"
  },
  {
    "title": "Agent Alignment in Evolving Social Norms",
    "authors": "Shimin Li, Tianxiang Sun, Xipeng Qiu",
    "abstract": "  Agents based on Large Language Models (LLMs) are increasingly permeating\nvarious domains of human production and life, highlighting the importance of\naligning them with human values. The current alignment of AI systems primarily\nfocuses on passively aligning LLMs through human intervention. However, agents\npossess characteristics like receiving environmental feedback and\nself-evolution, rendering the LLM alignment methods inadequate. In response, we\npropose an evolutionary framework for agent evolution and alignment, named\nEvolutionaryAgent, which transforms agent alignment into a process of evolution\nand selection under the principle of survival of the fittest. In an environment\nwhere social norms continuously evolve, agents better adapted to the current\nsocial norms will have a higher probability of survival and proliferation,\nwhile those inadequately aligned dwindle over time. Experimental results\nassessing the agents from multiple perspectives in aligning with social norms\ndemonstrate that EvolutionaryAgent possesses the capability to align\nprogressively better with the evolving social norms while maintaining its\nproficiency in general tasks. Effectiveness tests conducted on various open and\nclosed-source LLMs as the foundation for agents also prove the applicability of\nour approach.\n",
    "link": "http://arxiv.org/abs/2401.04620v1"
  },
  {
    "title": "A Deep Network for Explainable Prediction of Non-Imaging Phenotypes\n  using Anatomical Multi-View Data",
    "authors": "Yuxiang Wei, Yuqian Chen, Tengfei Xue, Leo Zekelman, Nikos Makris, Yogesh Rathi, Weidong Cai, Fan Zhang, Lauren J. O' Donnell",
    "abstract": "  Large datasets often contain multiple distinct feature sets, or views, that\noffer complementary information that can be exploited by multi-view learning\nmethods to improve results. We investigate anatomical multi-view data, where\neach brain anatomical structure is described with multiple feature sets. In\nparticular, we focus on sets of white matter microstructure and connectivity\nfeatures from diffusion MRI, as well as sets of gray matter area and thickness\nfeatures from structural MRI. We investigate machine learning methodology that\napplies multi-view approaches to improve the prediction of non-imaging\nphenotypes, including demographics (age), motor (strength), and cognition\n(picture vocabulary). We present an explainable multi-view network (EMV-Net)\nthat can use different anatomical views to improve prediction performance. In\nthis network, each individual anatomical view is processed by a view-specific\nfeature extractor and the extracted information from each view is fused using a\nlearnable weight. This is followed by a wavelet transform-based module to\nobtain complementary information across views which is then applied to\ncalibrate the view-specific information. Additionally, the calibrator produces\nan attention-based calibration score to indicate anatomical structures'\nimportance for interpretation.\n",
    "link": "http://arxiv.org/abs/2401.04579v1"
  },
  {
    "title": "Masked Audio Generation using a Single Non-Autoregressive Transformer",
    "authors": "Alon Ziv, Itai Gat, Gael Le Lan, Tal Remez, Felix Kreuk, Alexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, Yossi Adi",
    "abstract": "  We introduce MAGNeT, a masked generative sequence modeling method that\noperates directly over several streams of audio tokens. Unlike prior work,\nMAGNeT is comprised of a single-stage, non-autoregressive transformer. During\ntraining, we predict spans of masked tokens obtained from a masking scheduler,\nwhile during inference we gradually construct the output sequence using several\ndecoding steps. To further enhance the quality of the generated audio, we\nintroduce a novel rescoring method in which, we leverage an external\npre-trained model to rescore and rank predictions from MAGNeT, which will be\nthen used for later decoding steps. Lastly, we explore a hybrid version of\nMAGNeT, in which we fuse between autoregressive and non-autoregressive models\nto generate the first few seconds in an autoregressive manner while the rest of\nthe sequence is being decoded in parallel. We demonstrate the efficiency of\nMAGNeT for the task of text-to-music and text-to-audio generation and conduct\nan extensive empirical evaluation, considering both objective metrics and human\nstudies. The proposed approach is comparable to the evaluated baselines, while\nbeing significantly faster (x7 faster than the autoregressive baseline).\nThrough ablation studies and analysis, we shed light on the importance of each\nof the components comprising MAGNeT, together with pointing to the trade-offs\nbetween autoregressive and non-autoregressive modeling, considering latency,\nthroughput, and generation quality. Samples are available on our demo page\nhttps://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.\n",
    "link": "http://arxiv.org/abs/2401.04577v1"
  },
  {
    "title": "Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual\n  Concept Understanding",
    "authors": "Yatong Bai, Utsav Garg, Apaar Shanker, Haoming Zhang, Samyak Parajuli, Erhan Bas, Isidora Filipovic, Amelia N. Chu, Eugenia D Fomitcheva, Elliot Branson, Aerin Kim, Somayeh Sojoudi, Kyunghyun Cho",
    "abstract": "  Vision and vision-language applications of neural networks, such as image\nclassification and captioning, rely on large-scale annotated datasets that\nrequire non-trivial data-collecting processes. This time-consuming endeavor\nhinders the emergence of large-scale datasets, limiting researchers and\npractitioners to a small number of choices. Therefore, we seek more efficient\nways to collect and annotate images. Previous initiatives have gathered\ncaptions from HTML alt-texts and crawled social media postings, but these data\nsources suffer from noise, sparsity, or subjectivity. For this reason, we turn\nto commercial shopping websites whose data meet three criteria: cleanliness,\ninformativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset,\na large-scale public dataset with 15 million image-caption pairs from publicly\navailable e-commerce websites. When compared with existing general-domain\ndatasets, the LGS images focus on the foreground object and have less complex\nbackgrounds. Our experiments on LGS show that the classifiers trained on\nexisting benchmark datasets do not readily generalize to e-commerce data, while\nspecific self-supervised visual feature extractors can better generalize.\nFurthermore, LGS's high-quality e-commerce-focused images and bimodal nature\nmake it advantageous for vision-language bi-modal tasks: LGS enables\nimage-captioning models to generate richer captions and helps text-to-image\ngeneration models achieve e-commerce style transfer.\n",
    "link": "http://arxiv.org/abs/2401.04575v1"
  },
  {
    "title": "Evaluating Language Model Agency through Negotiations",
    "authors": "Tim R. Davidson, Veniamin Veselovsky, Martin Josifoski, Maxime Peyrard, Antoine Bosselut, Michal Kosinski, Robert West",
    "abstract": "  Companies, organizations, and governments increasingly exploit Language\nModels' (LM) remarkable capability to display agent-like behavior. As LMs are\nadopted to perform tasks with growing autonomy, there exists an urgent need for\nreliable and scalable evaluation benchmarks. Current, predominantly static LM\nbenchmarks are ill-suited to evaluate such dynamic applications. Thus, we\npropose jointly evaluating LM performance and alignment through the lenses of\nnegotiation games. We argue that this common task better reflects real-world\ndeployment conditions while offering insights into LMs' decision-making\nprocesses. Crucially, negotiation games allow us to study multi-turn, and\ncross-model interactions, modulate complexity, and side-step accidental data\nleakage in evaluation. We report results for six publicly accessible LMs from\nseveral major providers on a variety of negotiation games, evaluating both\nself-play and cross-play performance. Noteworthy findings include: (i)\nopen-source models are currently unable to complete these tasks; (ii)\ncooperative bargaining games prove challenging; and (iii) the most powerful\nmodels do not always \"win\".\n",
    "link": "http://arxiv.org/abs/2401.04536v1"
  },
  {
    "title": "MERA: A Comprehensive LLM Evaluation in Russian",
    "authors": "Alena Fenogenova, Artem Chervyakov, Nikita Martynov, Anastasia Kozlova, Maria Tikhonova, Albina Akhmetgareeva, Anton Emelyanov, Denis Shevelev, Pavel Lebedev, Leonid Sinev, Ulyana Isaeva, Katerina Kolomeytseva, Daniil Moskovskiy, Elizaveta Goncharova, Nikita Savushkin, Polina Mikhailova, Denis Dimitrov, Alexander Panchenko, Sergei Markov",
    "abstract": "  Over the past few years, one of the most notable advancements in AI research\nhas been in foundation models (FMs), headlined by the rise of language models\n(LMs). As the models' size increases, LMs demonstrate enhancements in\nmeasurable aspects and the development of new qualitative features. However,\ndespite researchers' attention and the rapid growth in LM application, the\ncapabilities, limitations, and associated risks still need to be better\nunderstood. To address these issues, we introduce an open Multimodal Evaluation\nof Russian-language Architectures (MERA), a new instruction benchmark for\nevaluating foundation models oriented towards the Russian language. The\nbenchmark encompasses 21 evaluation tasks for generative models in 11 skill\ndomains and is designed as a black-box test to ensure the exclusion of data\nleakage. The paper introduces a methodology to evaluate FMs and LMs in zero-\nand few-shot fixed instruction settings that can be extended to other\nmodalities. We propose an evaluation methodology, an open-source code base for\nthe MERA assessment, and a leaderboard with a submission system. We evaluate\nopen LMs as baselines and find that they are still far behind the human level.\nWe publicly release MERA to guide forthcoming research, anticipate\ngroundbreaking model features, standardize the evaluation procedure, and\naddress potential societal drawbacks.\n",
    "link": "http://arxiv.org/abs/2401.04531v1"
  },
  {
    "title": "The Critique of Critique",
    "authors": "Shichao Sun, Junlong Li, Weizhe Yuan, Ruifeng Yuan, Wenjie Li, Pengfei Liu",
    "abstract": "  Critique, as a natural language description for assessing the quality of\nmodel-generated content, has been proven to play an essential role in the\ntraining, evaluation, and refinement of Large Language Models (LLMs). However,\nthere is a lack of principled understanding in evaluating the quality of the\ncritique itself. In this paper, we pioneer the critique of critique, termed\nMetaCritique, which is a framework to evaluate the critique from two aspects,\ni.e., factuality as precision score and comprehensiveness as recall score. We\ncalculate the harmonic mean of precision and recall as the overall rating\ncalled F1 score. To obtain a reliable evaluation outcome, we propose Atomic\nInformation Units (AIUs), which describe the critique in a more fine-grained\nmanner. MetaCritique takes each AIU into account and aggregates each AIU's\njudgment for the overall score. Moreover, given the evaluation process involves\nintricate reasoning, our MetaCritique provides a natural language rationale to\nsupport each judgment. We construct a meta-evaluation dataset containing 300\ncritiques (2653 AIUs) across four tasks (question answering, reasoning,\nentailment, and summarization), and we conduct a comparative study to\ndemonstrate the feasibility and effectiveness. Experiments also show superior\ncritique judged by MetaCritique leads to better refinement, indicating\ngenerative artificial intelligence indeed has the potential to be significantly\nadvanced with our MetaCritique. We will release relevant code and\nmeta-evaluation datasets at https://github.com/GAIR-NLP/MetaCritique.\n",
    "link": "http://arxiv.org/abs/2401.04518v1"
  },
  {
    "title": "Exploring Prompt-Based Methods for Zero-Shot Hypernym Prediction with\n  Large Language Models",
    "authors": "Mikhail Tikhomirov, Natalia Loukachevitch",
    "abstract": "  This article investigates a zero-shot approach to hypernymy prediction using\nlarge language models (LLMs). The study employs a method based on text\nprobability calculation, applying it to various generated prompts. The\nexperiments demonstrate a strong correlation between the effectiveness of\nlanguage model prompts and classic patterns, indicating that preliminary prompt\nselection can be carried out using smaller models before moving to larger ones.\nWe also explore prompts for predicting co-hyponyms and improving hypernymy\npredictions by augmenting prompts with additional information through\nautomatically identified co-hyponyms. An iterative approach is developed for\npredicting higher-level concepts, which further improves the quality on the\nBLESS dataset (MAP = 0.8).\n",
    "link": "http://arxiv.org/abs/2401.04515v1"
  },
  {
    "title": "TechGPT-2.0: A large language model project to solve the task of\n  knowledge graph construction",
    "authors": "Jiaqi Wang, Yuying Chang, Zhong Li, Ning An, Qi Ma, Lei Hei, Haibo Luo, Yifei Lu, Feiliang Ren",
    "abstract": "  Large language models have exhibited robust performance across diverse\nnatural language processing tasks. This report introduces TechGPT-2.0, a\nproject designed to enhance the capabilities of large language models\nspecifically in knowledge graph construction tasks, including named entity\nrecognition (NER) and relationship triple extraction (RTE) tasks in NLP\napplications. Additionally, it serves as a LLM accessible for research within\nthe Chinese open-source model community. We offer two 7B large language model\nweights and a QLoRA weight specialized for processing lengthy texts.Notably,\nTechGPT-2.0 is trained on Huawei's Ascend server. Inheriting all\nfunctionalities from TechGPT-1.0, it exhibits robust text processing\ncapabilities, particularly in the domains of medicine and law. Furthermore, we\nintroduce new capabilities to the model, enabling it to process texts in\nvarious domains such as geographical areas, transportation, organizations,\nliterary works, biology, natural sciences, astronomical objects, and\narchitecture. These enhancements also fortified the model's adeptness in\nhandling hallucinations, unanswerable queries, and lengthy texts. This report\nprovides a comprehensive and detailed introduction to the full fine-tuning\nprocess on Huawei's Ascend servers, encompassing experiences in Ascend server\ndebugging, instruction fine-tuning data processing, and model training. Our\ncode is available at https://github.com/neukg/TechGPT-2.0\n",
    "link": "http://arxiv.org/abs/2401.04507v1"
  },
  {
    "title": "Optimal Survival Trees: A Dynamic Programming Approach",
    "authors": "Tim Huisman, Jacobus G. M. van der Linden, Emir Demirovi\u0107",
    "abstract": "  Survival analysis studies and predicts the time of death, or other singular\nunrepeated events, based on historical data, while the true time of death for\nsome instances is unknown. Survival trees enable the discovery of complex\nnonlinear relations in a compact human comprehensible model, by recursively\nsplitting the population and predicting a distinct survival distribution in\neach leaf node. We use dynamic programming to provide the first survival tree\nmethod with optimality guarantees, enabling the assessment of the optimality\ngap of heuristics. We improve the scalability of our method through a special\nalgorithm for computing trees up to depth two. The experiments show that our\nmethod's run time even outperforms some heuristics for realistic cases while\nobtaining similar out-of-sample performance with the state-of-the-art.\n",
    "link": "http://arxiv.org/abs/2401.04489v1"
  },
  {
    "title": "Fighting Fire with Fire: Adversarial Prompting to Generate a\n  Misinformation Detection Dataset",
    "authors": "Shrey Satapara, Parth Mehta, Debasis Ganguly, Sandip Modha",
    "abstract": "  The recent success in language generation capabilities of large language\nmodels (LLMs), such as GPT, Bard, Llama etc., can potentially lead to concerns\nabout their possible misuse in inducing mass agitation and communal hatred via\ngenerating fake news and spreading misinformation. Traditional means of\ndeveloping a misinformation ground-truth dataset does not scale well because of\nthe extensive manual effort required to annotate the data. In this paper, we\npropose an LLM-based approach of creating silver-standard ground-truth datasets\nfor identifying misinformation. Specifically speaking, given a trusted news\narticle, our proposed approach involves prompting LLMs to automatically\ngenerate a summarised version of the original article. The prompts in our\nproposed approach act as a controlling mechanism to generate specific types of\nfactual incorrectness in the generated summaries, e.g., incorrect quantities,\nfalse attributions etc. To investigate the usefulness of this dataset, we\nconduct a set of experiments where we train a range of supervised models for\nthe task of misinformation detection.\n",
    "link": "http://arxiv.org/abs/2401.04481v1"
  },
  {
    "title": "TwinBooster: Synergising Large Language Models with Barlow Twins and\n  Gradient Boosting for Enhanced Molecular Property Prediction",
    "authors": "Maximilian G. Schuh, Davide Boldini, Stephan A. Sieber",
    "abstract": "  The success of drug discovery and development relies on the precise\nprediction of molecular activities and properties. While in silico molecular\nproperty prediction has shown remarkable potential, its use has been limited so\nfar to assays for which large amounts of data are available. In this study, we\nuse a fine-tuned large language model to integrate biological assays based on\ntheir textual information, coupled with Barlow Twins, a Siamese neural network\nusing a novel self-supervised learning approach. This architecture uses both\nassay information and molecular fingerprints to extract the true molecular\ninformation. TwinBooster enables the prediction of properties of unseen\nbioassays and molecules by providing state-of-the-art zero-shot learning tasks.\nRemarkably, our artificial intelligence pipeline shows excellent performance on\nthe FS-Mol benchmark. This breakthrough demonstrates the application of deep\nlearning to critical property prediction tasks where data is typically scarce.\nBy accelerating the early identification of active molecules in drug discovery\nand development, this method has the potential to help streamline the\nidentification of novel therapeutics.\n",
    "link": "http://arxiv.org/abs/2401.04478v1"
  },
  {
    "title": "Combining Embedding-Based and Semantic-Based Models for Post-hoc\n  Explanations in Recommender Systems",
    "authors": "Ngoc Luyen Le, Marie-H\u00e9l\u00e8ne Abel, Philippe Gouspillou",
    "abstract": "  In today's data-rich environment, recommender systems play a crucial role in\ndecision support systems. They provide to users personalized recommendations\nand explanations about these recommendations. Embedding-based models, despite\ntheir widespread use, often suffer from a lack of interpretability, which can\nundermine trust and user engagement. This paper presents an approach that\ncombines embedding-based and semantic-based models to generate post-hoc\nexplanations in recommender systems, leveraging ontology-based knowledge graphs\nto improve interpretability and explainability. By organizing data within a\nstructured framework, ontologies enable the modeling of intricate relationships\nbetween entities, which is essential for generating explanations. By combining\nembedding-based and semantic based models for post-hoc explanations in\nrecommender systems, the framework we defined aims at producing meaningful and\neasy-to-understand explanations, enhancing user trust and satisfaction, and\npotentially promoting the adoption of recommender systems across the e-commerce\nsector.\n",
    "link": "http://arxiv.org/abs/2401.04474v1"
  },
  {
    "title": "A Survey on Efficient Federated Learning Methods for Foundation Model\n  Training",
    "authors": "Herbert Woisetschl\u00e4ger, Alexander Isenko, Shiqiang Wang, Ruben Mayer, Hans-Arno Jacobsen",
    "abstract": "  Federated Learning (FL) has become an established technique to facilitate\nprivacy-preserving collaborative training. However, new approaches to FL often\ndiscuss their contributions involving small deep-learning models only. With the\ntremendous success of transformer models, the following question arises: What\nis necessary to operationalize foundation models in an FL application? Knowing\nthat computation and communication often take up similar amounts of time in FL,\nwe introduce a novel taxonomy focused on computational and communication\nefficiency methods in FL applications. This said, these methods aim to optimize\nthe training time and reduce communication between clients and the server. We\nalso look at the current state of widely used FL frameworks and discuss future\nresearch potentials based on existing approaches in FL research and beyond.\n",
    "link": "http://arxiv.org/abs/2401.04472v1"
  },
  {
    "title": "MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation",
    "authors": "Weimin Wang, Jiawei Liu, Zhijie Lin, Jiangqiao Yan, Shuo Chen, Chetwin Low, Tuyen Hoang, Jie Wu, Jun Hao Liew, Hanshu Yan, Daquan Zhou, Jiashi Feng",
    "abstract": "  The growing demand for high-fidelity video generation from textual\ndescriptions has catalyzed significant research in this field. In this work, we\nintroduce MagicVideo-V2 that integrates the text-to-image model, video motion\ngenerator, reference image embedding module and frame interpolation module into\nan end-to-end video generation pipeline. Benefiting from these architecture\ndesigns, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution\nvideo with remarkable fidelity and smoothness. It demonstrates superior\nperformance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph,\nMoon Valley and Stable Video Diffusion model via user evaluation at large\nscale.\n",
    "link": "http://arxiv.org/abs/2401.04468v1"
  },
  {
    "title": "Image classification network enhancement methods based on knowledge\n  injection",
    "authors": "Yishuang Tian, Ning Wang, Liang Zhang",
    "abstract": "  The current deep neural network algorithm still stays in the end-to-end\ntraining supervision method like Image-Label pairs, which makes traditional\nalgorithm is difficult to explain the reason for the results, and the\nprediction logic is difficult to understand and analyze. The current algorithm\ndoes not use the existing human knowledge information, which makes the model\nnot in line with the human cognition model and makes the model not suitable for\nhuman use. In order to solve the above problems, the present invention provides\na deep neural network training method based on the human knowledge, which uses\nthe human cognition model to construct the deep neural network training model,\nand uses the existing human knowledge information to construct the deep neural\nnetwork training model. This paper proposes a multi-level hierarchical deep\nlearning algorithm, which is composed of multi-level hierarchical deep neural\nnetwork architecture and multi-level hierarchical deep learning framework. The\nexperimental results show that the proposed algorithm can effectively explain\nthe hidden information of the neural network. The goal of our study is to\nimprove the interpretability of deep neural networks (DNNs) by providing an\nanalysis of the impact of knowledge injection on the classification task. We\nconstructed a knowledge injection dataset with matching knowledge data and\nimage classification data. The knowledge injection dataset is the benchmark\ndataset for the experiments in the paper. Our model expresses the improvement\nin interpretability and classification task performance of hidden layers at\ndifferent scales.\n",
    "link": "http://arxiv.org/abs/2401.04441v1"
  },
  {
    "title": "Empirical Analysis of Anomaly Detection on Hyperspectral Imaging Using\n  Dimension Reduction Methods",
    "authors": "Dongeon Kim, YeongHyeon Park",
    "abstract": "  Recent studies try to use hyperspectral imaging (HSI) to detect foreign\nmatters in products because it enables to visualize the invisible wavelengths\nincluding ultraviolet and infrared. Considering the enormous image channels of\nthe HSI, several dimension reduction methods-e.g., PCA or UMAP-can be\nconsidered to reduce but those cannot ease the fundamental limitations, as\nfollows: (1) latency of HSI capturing. (2) less explanation ability of the\nimportant channels. In this paper, to circumvent the aforementioned methods,\none of the ways to channel reduction, on anomaly detection proposed HSI.\nDifferent from feature extraction methods (i.e., PCA or UMAP), feature\nselection can sort the feature by impact and show better explainability so we\nmight redesign the task-optimized and cost-effective spectroscopic camera. Via\nthe extensive experiment results with synthesized MVTec AD dataset, we confirm\nthat the feature selection method shows 6.90x faster at the inference phase\ncompared with feature extraction-based approaches while preserving anomaly\ndetection performance. Ultimately, we conclude the advantage of feature\nselection which is effective yet fast.\n",
    "link": "http://arxiv.org/abs/2401.04437v1"
  },
  {
    "title": "i-Rebalance: Personalized Vehicle Repositioning for Supply Demand\n  Balance",
    "authors": "Haoyang Chen, Peiyan Sun, Qiyuan Song, Wanyuan Wang, Weiwei Wu, Wencan Zhang, Guanyu Gao, Yan Lyu",
    "abstract": "  Ride-hailing platforms have been facing the challenge of balancing demand and\nsupply. Existing vehicle reposition techniques often treat drivers as\nhomogeneous agents and relocate them deterministically, assuming compliance\nwith the reposition. In this paper, we consider a more realistic and\ndriver-centric scenario where drivers have unique cruising preferences and can\ndecide whether to take the recommendation or not on their own. We propose\ni-Rebalance, a personalized vehicle reposition technique with deep\nreinforcement learning (DRL). i-Rebalance estimates drivers' decisions on\naccepting reposition recommendations through an on-field user study involving\n99 real drivers. To optimize supply-demand balance and enhance preference\nsatisfaction simultaneously, i-Rebalance has a sequential reposition strategy\nwith dual DRL agents: Grid Agent to determine the reposition order of idle\nvehicles, and Vehicle Agent to provide personalized recommendations to each\nvehicle in the pre-defined order. This sequential learning strategy facilitates\nmore effective policy training within a smaller action space compared to\ntraditional joint-action methods. Evaluation of real-world trajectory data\nshows that i-Rebalance improves driver acceptance rate by 38.07% and total\ndriver income by 9.97%.\n",
    "link": "http://arxiv.org/abs/2401.04429v1"
  },
  {
    "title": "Estimating Text Similarity based on Semantic Concept Embeddings",
    "authors": "Tim vor der Br\u00fcck, Marc Pouly",
    "abstract": "  Due to their ease of use and high accuracy, Word2Vec (W2V) word embeddings\nenjoy great success in the semantic representation of words, sentences, and\nwhole documents as well as for semantic similarity estimation. However, they\nhave the shortcoming that they are directly extracted from a surface\nrepresentation, which does not adequately represent human thought processes and\nalso performs poorly for highly ambiguous words. Therefore, we propose Semantic\nConcept Embeddings (CE) based on the MultiNet Semantic Network (SN) formalism,\nwhich addresses both shortcomings. The evaluation on a marketing target group\ndistribution task showed that the accuracy of predicted target groups can be\nincreased by combining traditional word embeddings with semantic CEs.\n",
    "link": "http://arxiv.org/abs/2401.04422v1"
  },
  {
    "title": "Optimal Transcoding Resolution Prediction for Efficient Per-Title\n  Bitrate Ladder Estimation",
    "authors": "Jinhai Yang, Mengxi Guo, Shijie Zhao, Junlin Li, Li Zhang",
    "abstract": "  Adaptive video streaming requires efficient bitrate ladder construction to\nmeet heterogeneous network conditions and end-user demands. Per-title optimized\nencoding typically traverses numerous encoding parameters to search the\nPareto-optimal operating points for each video. Recently, researchers have\nattempted to predict the content-optimized bitrate ladder for pre-encoding\noverhead reduction. However, existing methods commonly estimate the encoding\nparameters on the Pareto front and still require subsequent pre-encodings. In\nthis paper, we propose to directly predict the optimal transcoding resolution\nat each preset bitrate for efficient bitrate ladder construction. We adopt a\nTemporal Attentive Gated Recurrent Network to capture spatial-temporal features\nand predict transcoding resolutions as a multi-task classification problem. We\ndemonstrate that content-optimized bitrate ladders can thus be efficiently\ndetermined without any pre-encoding. Our method well approximates the\nground-truth bitrate-resolution pairs with a slight Bj{\\o}ntegaard Delta rate\nloss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.\n",
    "link": "http://arxiv.org/abs/2401.04405v1"
  },
  {
    "title": "IGNITE: Individualized GeNeration of Imputations in Time-series\n  Electronic health records",
    "authors": "Ghadeer O. Ghosheh, Jin Li, Tingting Zhu",
    "abstract": "  Electronic Health Records present a valuable modality for driving\npersonalized medicine, where treatment is tailored to fit individual-level\ndifferences. For this purpose, many data-driven machine learning and\nstatistical models rely on the wealth of longitudinal EHRs to study patients'\nphysiological and treatment effects. However, longitudinal EHRs tend to be\nsparse and highly missing, where missingness could also be informative and\nreflect the underlying patient's health status. Therefore, the success of\ndata-driven models for personalized medicine highly depends on how the EHR data\nis represented from physiological data, treatments, and the missing values in\nthe data. To this end, we propose a novel deep-learning model that learns the\nunderlying patient dynamics over time across multivariate data to generate\npersonalized realistic values conditioning on an individual's demographic\ncharacteristics and treatments. Our proposed model, IGNITE (Individualized\nGeNeration of Imputations in Time-series Electronic health records), utilises a\nconditional dual-variational autoencoder augmented with dual-stage attention to\ngenerate missing values for an individual. In IGNITE, we further propose a\nnovel individualized missingness mask (IMM), which helps our model generate\nvalues based on the individual's observed data and missingness patterns. We\nfurther extend the use of IGNITE from imputing missingness to a personalized\ndata synthesizer, where it generates missing EHRs that were never observed\nprior or even generates new patients for various applications. We validate our\nmodel on three large publicly available datasets and show that IGNITE\noutperforms state-of-the-art approaches in missing data reconstruction and task\nprediction.\n",
    "link": "http://arxiv.org/abs/2401.04402v1"
  },
  {
    "title": "Machine unlearning through fine-grained model parameters perturbation",
    "authors": "Zhiwei Zuo, Zhuo Tang, Kenli Li, Anwitaman Datta",
    "abstract": "  Machine unlearning techniques, which involve retracting data records and\nreducing influence of said data on trained models, help with the user privacy\nprotection objective but incur significant computational costs. Weight\nperturbation-based unlearning is a general approach, but it typically involves\nglobally modifying the parameters. We propose fine-grained Top-K and Random-k\nparameters perturbed inexact machine unlearning strategies that address the\nprivacy needs while keeping the computational costs tractable.\n  In order to demonstrate the efficacy of our strategies we also tackle the\nchallenge of evaluating the effectiveness of machine unlearning by considering\nthe model's generalization performance across both unlearning and remaining\ndata. To better assess the unlearning effect and model generalization, we\npropose novel metrics, namely, the forgetting rate and memory retention rate.\nHowever, for inexact machine unlearning, current metrics are inadequate in\nquantifying the degree of forgetting that occurs after unlearning strategies\nare applied. To address this, we introduce SPD-GAN, which subtly perturbs the\ndistribution of data targeted for unlearning. Then, we evaluate the degree of\nunlearning by measuring the performance difference of the models on the\nperturbed unlearning data before and after the unlearning process. By\nimplementing these innovative techniques and metrics, we achieve\ncomputationally efficacious privacy protection in machine learning applications\nwithout significant sacrifice of model performance. Furthermore, this approach\nprovides a novel method for evaluating the degree of unlearning.\n",
    "link": "http://arxiv.org/abs/2401.04385v1"
  },
  {
    "title": "Towards Explainable Artificial Intelligence (XAI): A Data Mining\n  Perspective",
    "authors": "Haoyi Xiong, Xuhong L, Xiaofei Zhang, Jiamin Chen, Xinhao Sun, Yuchen Li, Zeyi Sun, Mengnan Du",
    "abstract": "  Given the complexity and lack of transparency in deep neural networks (DNNs),\nextensive efforts have been made to make these systems more interpretable or\nexplain their behaviors in accessible terms. Unlike most reviews, which focus\non algorithmic and model-centric perspectives, this work takes a \"data-centric\"\nview, examining how data collection, processing, and analysis contribute to\nexplainable AI (XAI). We categorize existing work into three categories subject\nto their purposes: interpretations of deep models, referring to feature\nattributions and reasoning processes that correlate data points with model\noutputs; influences of training data, examining the impact of training data\nnuances, such as data valuation and sample anomalies, on decision-making\nprocesses; and insights of domain knowledge, discovering latent patterns and\nfostering new knowledge from data and models to advance social values and\nscientific discovery. Specifically, we distill XAI methodologies into data\nmining operations on training and testing data across modalities, such as\nimages, text, and tabular data, as well as on training logs, checkpoints,\nmodels and other DNN behavior descriptors. In this way, our study offers a\ncomprehensive, data-centric examination of XAI from a lens of data mining\nmethods and applications.\n",
    "link": "http://arxiv.org/abs/2401.04374v1"
  },
  {
    "title": "Representative Feature Extraction During Diffusion Process for Sketch\n  Extraction with One Example",
    "authors": "Kwan Yun, Youngseo Kim, Kwanggyoon Seo, Chang Wook Seo, Junyong Noh",
    "abstract": "  We introduce DiffSketch, a method for generating a variety of stylized\nsketches from images. Our approach focuses on selecting representative features\nfrom the rich semantics of deep features within a pretrained diffusion model.\nThis novel sketch generation method can be trained with one manual drawing.\nFurthermore, efficient sketch extraction is ensured by distilling a trained\ngenerator into a streamlined extractor. We select denoising diffusion features\nthrough analysis and integrate these selected features with VAE features to\nproduce sketches. Additionally, we propose a sampling scheme for training\nmodels using a conditional generative approach. Through a series of\ncomparisons, we verify that distilled DiffSketch not only outperforms existing\nstate-of-the-art sketch extraction methods but also surpasses diffusion-based\nstylization methods in the task of extracting sketches.\n",
    "link": "http://arxiv.org/abs/2401.04362v1"
  },
  {
    "title": "Improving the Robustness of Knowledge-Grounded Dialogue via Contrastive\n  Learning",
    "authors": "Jiaan Wang, Jianfeng Qu, Kexin Wang, Zhixu Li, Wen Hua, Ximing Li, An Liu",
    "abstract": "  Knowledge-grounded dialogue (KGD) learns to generate an informative response\nbased on a given dialogue context and external knowledge (\\emph{e.g.},\nknowledge graphs; KGs). Recently, the emergence of large language models (LLMs)\nand pre-training techniques has brought great success to knowledge-grounded\ndialogue. However, when building KGD systems in real applications, there are\nvarious real-world noises that are inevitable to face. For example, the\ndialogue context might involve perturbations such as misspellings and\nabbreviations. In addition, KGs typically suffer from incompletion and also\nmight contain erroneous and outdated facts. Such real-world noises pose a\nchallenge to the robustness of KGD systems and hinder their applications in the\nreal world. In this paper, we propose an entity-based contrastive learning\nframework for improving the robustness of KGD. Specifically, we make use of the\nentity information in a KGD sample to create both its positive and negative\nsamples which involve semantic-irrelevant and semantic-relevant perturbations,\nrespectively. The contrastive learning framework ensures the KGD model is aware\nof these two types of perturbations, thus generating informative responses with\nthe potentially noisy inputs in real applications. Experimental results on\nthree benchmark datasets show that our method achieves new state-of-the-art\nperformance in terms of automatic evaluation scores, verifying its\neffectiveness and potentiality. Furthermore, we show that our method can\ngenerate better responses than comparison models in both the noisy and the\nfew-shot settings.\n",
    "link": "http://arxiv.org/abs/2401.04361v1"
  },
  {
    "title": "Iterative Feedback Network for Unsupervised Point Cloud Registration",
    "authors": "Yifan Xie, Boyu Wang, Shiqi Li, Jihua Zhu",
    "abstract": "  As a fundamental problem in computer vision, point cloud registration aims to\nseek the optimal transformation for aligning a pair of point clouds. In most\nexisting methods, the information flows are usually forward transferring, thus\nlacking the guidance from high-level information to low-level information.\nBesides, excessive high-level information may be overly redundant, and directly\nusing it may conflict with the original low-level information. In this paper,\nwe propose a novel Iterative Feedback Network (IFNet) for unsupervised point\ncloud registration, in which the representation of low-level features is\nefficiently enriched by rerouting subsequent high-level features. Specifically,\nour IFNet is built upon a series of Feedback Registration Block (FRB) modules,\nwith each module responsible for generating the feedforward rigid\ntransformation and feedback high-level features. These FRB modules are cascaded\nand recurrently unfolded over time. Further, the Feedback Transformer is\ndesigned to efficiently select relevant information from feedback high-level\nfeatures, which is utilized to refine the low-level features. What's more, we\nincorporate a geometry-awareness descriptor to empower the network for making\nfull use of most geometric information, which leads to more precise\nregistration results. Extensive experiments on various benchmark datasets\ndemonstrate the superior registration performance of our IFNet.\n",
    "link": "http://arxiv.org/abs/2401.04357v1"
  },
  {
    "title": "A Change Point Detection Integrated Remaining Useful Life Estimation\n  Model under Variable Operating Conditions",
    "authors": "Anushiya Arunan, Yan Qin, Xiaoli Li, Chau Yuen",
    "abstract": "  By informing the onset of the degradation process, health status evaluation\nserves as a significant preliminary step for reliable remaining useful life\n(RUL) estimation of complex equipment. This paper proposes a novel temporal\ndynamics learning-based model for detecting change points of individual\ndevices, even under variable operating conditions, and utilises the learnt\nchange points to improve the RUL estimation accuracy. During offline model\ndevelopment, the multivariate sensor data are decomposed to learn fused\ntemporal correlation features that are generalisable and representative of\nnormal operation dynamics across multiple operating conditions. Monitoring\nstatistics and control limit thresholds for normal behaviour are dynamically\nconstructed from these learnt temporal features for the unsupervised detection\nof device-level change points. The detected change points then inform the\ndegradation data labelling for training a long short-term memory (LSTM)-based\nRUL estimation model. During online monitoring, the temporal correlation\ndynamics of a query device is monitored for breach of the control limit derived\nin offline training. If a change point is detected, the device's RUL is\nestimated with the well-trained offline model for early preventive action.\nUsing C-MAPSS turbofan engines as the case study, the proposed method improved\nthe accuracy by 5.6\\% and 7.5\\% for two scenarios with six operating\nconditions, when compared to existing LSTM-based RUL estimation models that do\nnot consider heterogeneous change points.\n",
    "link": "http://arxiv.org/abs/2401.04351v1"
  },
  {
    "title": "Memory-Efficient Personalization using Quantized Diffusion Model",
    "authors": "Hyogon Ryu, Seohyun Lim, Hyunjung Shim",
    "abstract": "  The rise of billion-parameter diffusion models like Stable Diffusion XL,\nImagen, and Dall-E3 markedly advances the field of generative AI. However,\ntheir large-scale nature poses challenges in fine-tuning and deployment due to\nhigh resource demands and slow inference speed. This paper ventures into the\nrelatively unexplored yet promising realm of fine-tuning quantized diffusion\nmodels. We establish a strong baseline by customizing three models: PEQA for\nfine-tuning quantization parameters, Q-Diffusion for post-training\nquantization, and DreamBooth for personalization. Our analysis reveals a\nnotable trade-off between subject and prompt fidelity within the baseline\nmodel. To address these issues, we introduce two strategies, inspired by the\ndistinct roles of different timesteps in diffusion models: S1 optimizing a\nsingle set of fine-tuning parameters exclusively at selected intervals, and S2\ncreating multiple fine-tuning parameter sets, each specialized for different\ntimestep intervals. Our approach not only enhances personalization but also\nupholds prompt fidelity and image quality, significantly outperforming the\nbaseline qualitatively and quantitatively. The code will be made publicly\navailable.\n",
    "link": "http://arxiv.org/abs/2401.04339v1"
  },
  {
    "title": "Deep Efficient Private Neighbor Generation for Subgraph Federated\n  Learning",
    "authors": "Ke Zhang, Lichao Sun, Bolin Ding, Siu Ming Yiu, Carl Yang",
    "abstract": "  Behemoth graphs are often fragmented and separately stored by multiple data\nowners as distributed subgraphs in many realistic applications. Without harming\ndata privacy, it is natural to consider the subgraph federated learning\n(subgraph FL) scenario, where each local client holds a subgraph of the entire\nglobal graph, to obtain globally generalized graph mining models. To overcome\nthe unique challenge of incomplete information propagation on local subgraphs\ndue to missing cross-subgraph neighbors, previous works resort to the\naugmentation of local neighborhoods through the joint FL of missing neighbor\ngenerators and GNNs. Yet their technical designs have profound limitations\nregarding the utility, efficiency, and privacy goals of FL. In this work, we\npropose FedDEP to comprehensively tackle these challenges in subgraph FL.\nFedDEP consists of a series of novel technical designs: (1) Deep neighbor\ngeneration through leveraging the GNN embeddings of potential missing\nneighbors; (2) Efficient pseudo-FL for neighbor generation through embedding\nprototyping; and (3) Privacy protection through noise-less\nedge-local-differential-privacy.\n  We analyze the correctness and efficiency of FedDEP, and provide theoretical\nguarantees on its privacy.\n  Empirical results on four real-world datasets justify the clear benefits of\nproposed techniques.\n",
    "link": "http://arxiv.org/abs/2401.04336v1"
  },
  {
    "title": "Large Language Models for Robotics: Opportunities, Challenges, and\n  Perspectives",
    "authors": "Jiaqi Wang, Zihao Wu, Yiwei Li, Hanqi Jiang, Peng Shu, Enze Shi, Huawen Hu, Chong Ma, Yiheng Liu, Xuhui Wang, Yincheng Yao, Xuan Liu, Huaqin Zhao, Zhengliang Liu, Haixing Dai, Lin Zhao, Bao Ge, Xiang Li, Tianming Liu, Shu Zhang",
    "abstract": "  Large language models (LLMs) have undergone significant expansion and have\nbeen increasingly integrated across various domains. Notably, in the realm of\nrobot task planning, LLMs harness their advanced reasoning and language\ncomprehension capabilities to formulate precise and efficient action plans\nbased on natural language instructions. However, for embodied tasks, where\nrobots interact with complex environments, text-only LLMs often face challenges\ndue to a lack of compatibility with robotic visual perception. This study\nprovides a comprehensive overview of the emerging integration of LLMs and\nmultimodal LLMs into various robotic tasks. Additionally, we propose a\nframework that utilizes multimodal GPT-4V to enhance embodied task planning\nthrough the combination of natural language instructions and robot visual\nperceptions. Our results, based on diverse datasets, indicate that GPT-4V\neffectively enhances robot performance in embodied tasks. This extensive survey\nand evaluation of LLMs and multimodal LLMs across a variety of robotic tasks\nenriches the understanding of LLM-centric embodied intelligence and provides\nforward-looking insights toward bridging the gap in Human-Robot-Environment\ninteraction.\n",
    "link": "http://arxiv.org/abs/2401.04334v1"
  },
  {
    "title": "Coupling Graph Neural Networks with Fractional Order Continuous\n  Dynamics: A Robustness Study",
    "authors": "Qiyu Kang, Kai Zhao, Yang Song, Yihang Xie, Yanan Zhao, Sijie Wang, Rui She, Wee Peng Tay",
    "abstract": "  In this work, we rigorously investigate the robustness of graph neural\nfractional-order differential equation (FDE) models. This framework extends\nbeyond traditional graph neural (integer-order) ordinary differential equation\n(ODE) models by implementing the time-fractional Caputo derivative. Utilizing\nfractional calculus allows our model to consider long-term memory during the\nfeature updating process, diverging from the memoryless Markovian updates seen\nin traditional graph neural ODE models. The superiority of graph neural FDE\nmodels over graph neural ODE models has been established in environments free\nfrom attacks or perturbations. While traditional graph neural ODE models have\nbeen verified to possess a degree of stability and resilience in the presence\nof adversarial attacks in existing literature, the robustness of graph neural\nFDE models, especially under adversarial conditions, remains largely\nunexplored. This paper undertakes a detailed assessment of the robustness of\ngraph neural FDE models. We establish a theoretical foundation outlining the\nrobustness characteristics of graph neural FDE models, highlighting that they\nmaintain more stringent output perturbation bounds in the face of input and\ngraph topology disturbances, compared to their integer-order counterparts. Our\nempirical evaluations further confirm the enhanced robustness of graph neural\nFDE models, highlighting their potential in adversarially robust applications.\n",
    "link": "http://arxiv.org/abs/2401.04331v1"
  },
  {
    "title": "BD-MSA: Body decouple VHR Remote Sensing Image Change Detection method\n  guided by multi-scale feature information aggregation",
    "authors": "Yonghui Tan, Xiaolong Li, Yishu Chen, Jinquan Ai",
    "abstract": "  The purpose of remote sensing image change detection (RSCD) is to detect\ndifferences between bi-temporal images taken at the same place. Deep learning\nhas been extensively used to RSCD tasks, yielding significant results in terms\nof result recognition. However, due to the shooting angle of the satellite, the\nimpacts of thin clouds, and certain lighting conditions, the problem of fuzzy\nedges in the change region in some remote sensing photographs cannot be\nproperly handled using current RSCD algorithms. To solve this issue, we\nproposed a Body Decouple Multi-Scale by fearure Aggregation change detection\n(BD-MSA), a novel model that collects both global and local feature map\ninformation in the channel and space dimensions of the feature map during the\ntraining and prediction phases. This approach allows us to successfully extract\nthe change region's boundary information while also divorcing the change\nregion's main body from its boundary. Numerous studies have shown that the\nassessment metrics and evaluation effects of the model described in this paper\non the publicly available datasets DSIFN-CD and S2Looking are the best when\ncompared to other models.\n",
    "link": "http://arxiv.org/abs/2401.04330v1"
  },
  {
    "title": "Know Your Needs Better: Towards Structured Understanding of Marketer\n  Demands with Analogical Reasoning Augmented LLMs",
    "authors": "Junjie Wang, Dan Yang, Binbin Hu, Yue Shen, Ziqi Liu, Wen Zhang, Jinjie Gu, Zhiqiang Zhang",
    "abstract": "  In this paper, we explore a new way for user targeting, where non-expert\nmarketers could select their target users solely given demands in natural\nlanguage form. The key to this issue is how to transform natural languages into\npractical structured logical languages, i.e., the structured understanding of\nmarketer demands. Considering the impressive natural language processing\nability of large language models (LLMs), we try to leverage LLMs to solve this\nissue. Past research indicates that the reasoning ability of LLMs can be\neffectively enhanced through chain-of-thought (CoT) prompting. But existing\nmethods still have some limitations: (1) Previous methods either use simple\n\"Let's think step by step\" spells or provide fixed examples in demonstrations\nwithout considering compatibility between prompts and questions, making LLMs\nineffective in some complex reasoning tasks such as structured language\ntransformation. (2) Previous methods are often implemented in closed-source\nmodels or excessively large models, which is not suitable in industrial\npractical scenarios. Based on these, we propose ARALLM (i.e., Analogical\nReasoning Augmented Large Language Models) consisting of two modules:\nAnalogical Reasoning based Prompting and Reasoning-Augmented Multi-Task Model\nDistillation.\n",
    "link": "http://arxiv.org/abs/2401.04319v1"
  },
  {
    "title": "StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For\n  Multi-Agent Environments",
    "authors": "Sean Kulinski, Nicholas R. Waytowich, James Z. Hare, David I. Inouye",
    "abstract": "  Spatial reasoning tasks in multi-agent environments such as event prediction,\nagent type identification, or missing data imputation are important for\nmultiple applications (e.g., autonomous surveillance over sensor networks and\nsubtasks for reinforcement learning (RL)). StarCraft II game replays encode\nintelligent (and adversarial) multi-agent behavior and could provide a testbed\nfor these tasks; however, extracting simple and standardized representations\nfor prototyping these tasks is laborious and hinders reproducibility. In\ncontrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled\nrapid prototyping and reproducibility of ML methods. Following the simplicity\nof these datasets, we construct a benchmark spatial reasoning dataset based on\nStarCraft II replays that exhibit complex multi-agent behaviors, while still\nbeing as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize\na window of 255 consecutive game states to create 3.6 million summary images\nfrom 60,000 replays, including all relevant metadata such as game outcome and\nplayer races. We develop three formats of decreasing complexity: Hyperspectral\nimages that include one channel for every unit type (similar to multispectral\ngeospatial images), RGB images that mimic CIFAR10, and grayscale images that\nmimic MNIST. We show how this dataset can be used for prototyping spatial\nreasoning methods. All datasets, code for extraction, and code for dataset\nloading can be found at https://starcraftdata.davidinouye.com\n",
    "link": "http://arxiv.org/abs/2401.04290v1"
  }
]