[
  {
    "title": "The Definitive Guide to Policy Gradients in Deep Reinforcement Learning:\n  Theory, Algorithms and Implementations",
    "authors": "Matthias Lehmann",
    "abstract": "  In recent years, various powerful policy gradient algorithms have been\nproposed in deep reinforcement learning. While all these algorithms build on\nthe Policy Gradient Theorem, the specific design choices differ significantly\nacross algorithms. We provide a holistic overview of on-policy policy gradient\nalgorithms to facilitate the understanding of both their theoretical\nfoundations and their practical implementations. In this overview, we include a\ndetailed proof of the continuous version of the Policy Gradient Theorem,\nconvergence results and a comprehensive discussion of practical algorithms. We\ncompare the most prominent algorithms on continuous control environments and\nprovide insights on the benefits of regularization. All code is available at\nhttps://github.com/Matt00n/PolicyGradientsJax.\n",
    "link": "http://arxiv.org/abs/2401.13662v1"
  },
  {
    "title": "Inadequacy of common stochastic neural networks for reliable clinical\n  decision support",
    "authors": "Adrian Lindenmeyer, Malte Blattmann, Stefan Franke, Thomas Neumuth, Daniel Schneider",
    "abstract": "  Widespread adoption of AI for medical decision making is still hindered due\nto ethical and safety-related concerns. For AI-based decision support systems\nin healthcare settings it is paramount to be reliable and trustworthy. Common\ndeep learning approaches, however, have the tendency towards overconfidence\nunder data shift. Such inappropriate extrapolation beyond evidence-based\nscenarios may have dire consequences. This highlights the importance of\nreliable estimation of local uncertainty and its communication to the end user.\nWhile stochastic neural networks have been heralded as a potential solution to\nthese issues, this study investigates their actual reliability in clinical\napplications. We centered our analysis on the exemplary use case of mortality\nprediction for ICU hospitalizations using EHR from MIMIC3 study. For\npredictions on the EHR time series, Encoder-Only Transformer models were\nemployed. Stochasticity of model functions was achieved by incorporating common\nmethods such as Bayesian neural network layers and model ensembles. Our models\nachieve state of the art performance in terms of discrimination performance\n(AUC ROC: 0.868+-0.011, AUC PR: 0.554+-0.034) and calibration on the mortality\nprediction benchmark. However, epistemic uncertainty is critically\nunderestimated by the selected stochastic deep learning methods. A heuristic\nproof for the responsible collapse of the posterior distribution is provided.\nOur findings reveal the inadequacy of commonly used stochastic deep learning\napproaches to reliably recognize OoD samples. In both methods, unsubstantiated\nmodel confidence is not prevented due to strongly biased functional posteriors,\nrendering them inappropriate for reliable clinical decision support. This\nhighlights the need for approaches with more strictly enforced or inherent\ndistance-awareness to known data points, e.g., using kernel-based techniques.\n",
    "link": "http://arxiv.org/abs/2401.13657v1"
  },
  {
    "title": "Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity\n  Detectors",
    "authors": "Francesco Della Santa, Sandra Pieraccini",
    "abstract": "  In this paper, we present a novel approach for detecting the discontinuity\ninterfaces of a discontinuous function. This approach leverages Graph-Informed\nNeural Networks (GINNs) and sparse grids to address discontinuity detection\nalso in domains of dimension larger than 3. GINNs, trained to identify troubled\npoints on sparse grids, exploit graph structures built on the grids to achieve\nefficient and accurate discontinuity detection performances. We also introduce\na recursive algorithm for general sparse grid-based detectors, characterized by\nconvergence properties and easy applicability. Numerical experiments on\nfunctions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust\ngeneralization of GINNs in detecting discontinuity interfaces. Notably, the\ntrained GINNs offer portability and versatility, allowing integration into\nvarious algorithms and sharing among users.\n",
    "link": "http://arxiv.org/abs/2401.13652v1"
  },
  {
    "title": "How Good is ChatGPT at Face Biometrics? A First Look into Recognition,\n  Soft Biometrics, and Explainability",
    "authors": "Ivan DeAndres-Tame, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez, Javier Ortega-Garcia",
    "abstract": "  Large Language Models (LLMs) such as GPT developed by OpenAI, have already\nshown astonishing results, introducing quick changes in our society. This has\nbeen intensified by the release of ChatGPT which allows anyone to interact in a\nsimple conversational way with LLMs, without any experience in the field\nneeded. As a result, ChatGPT has been rapidly applied to many different tasks\nsuch as code- and song-writer, education, virtual assistants, etc., showing\nimpressive results for tasks for which it was not trained (zero-shot learning).\n  The present study aims to explore the ability of ChatGPT, based on the recent\nGPT-4 multimodal LLM, for the task of face biometrics. In particular, we\nanalyze the ability of ChatGPT to perform tasks such as face verification,\nsoft-biometrics estimation, and explainability of the results. ChatGPT could be\nvery valuable to further increase the explainability and transparency of the\nautomatic decisions in human scenarios. Experiments are carried out in order to\nevaluate the performance and robustness of ChatGPT, using popular public\nbenchmarks and comparing the results with state-of-the-art methods in the\nfield. The results achieved in this study show the potential of LLMs such as\nChatGPT for face biometrics, especially to enhance explainability. For\nreproducibility reasons, we release all the code in GitHub.\n",
    "link": "http://arxiv.org/abs/2401.13641v1"
  },
  {
    "title": "Enhancing Image Retrieval : A Comprehensive Study on Photo Search using\n  the CLIP Mode",
    "authors": "Naresh Kumar Lahajal, Harini S",
    "abstract": "  Photo search, the task of retrieving images based on textual queries, has\nwitnessed significant advancements with the introduction of CLIP (Contrastive\nLanguage-Image Pretraining) model. CLIP leverages a vision-language pre\ntraining approach, wherein it learns a shared representation space for images\nand text, enabling cross-modal understanding. This model demonstrates the\ncapability to understand the semantic relationships between diverse image and\ntext pairs, allowing for efficient and accurate retrieval of images based on\nnatural language queries. By training on a large-scale dataset containing\nimages and their associated textual descriptions, CLIP achieves remarkable\ngeneralization, providing a powerful tool for tasks such as zero-shot learning\nand few-shot classification. This abstract summarizes the foundational\nprinciples of CLIP and highlights its potential impact on advancing the field\nof photo search, fostering a seamless integration of natural language\nunderstanding and computer vision for improved information retrieval in\nmultimedia applications\n",
    "link": "http://arxiv.org/abs/2401.13613v1"
  },
  {
    "title": "Non-Intrusive Speech Intelligibility Prediction for Hearing-Impaired\n  Users using Intermediate ASR Features and Human Memory Models",
    "authors": "Rhiannon Mogridge, George Close, Robert Sutherland, Thomas Hain, Jon Barker, Stefan Goetze, Anton Ragni",
    "abstract": "  Neural networks have been successfully used for non-intrusive speech\nintelligibility prediction. Recently, the use of feature representations\nsourced from intermediate layers of pre-trained self-supervised and\nweakly-supervised models has been found to be particularly useful for this\ntask. This work combines the use of Whisper ASR decoder layer representations\nas neural network input features with an exemplar-based, psychologically\nmotivated model of human memory to predict human intelligibility ratings for\nhearing-aid users. Substantial performance improvement over an established\nintrusive HASPI baseline system is found, including on enhancement systems and\nlisteners unseen in the training data, with a root mean squared error of 25.3\ncompared with the baseline of 28.7.\n",
    "link": "http://arxiv.org/abs/2401.13611v1"
  },
  {
    "title": "Stream-based perception for cognitive agents in mobile ecosystems",
    "authors": "Jeremias D\u00f6tterl, Ralf Bruns, J\u00fcrgen Dunkel, Sascha Ossowski",
    "abstract": "  Cognitive agent abstractions can help to engineer intelligent systems across\nmobile devices. On smartphones, the data obtained from onboard sensors can give\nvaluable insights into the user's current situation. Unfortunately, today's\ncognitive agent frameworks cannot cope well with the challenging\ncharacteristics of sensor data. Sensor data is located on a low abstraction\nlevel and the individual data elements are not meaningful when observed in\nisolation. In contrast, cognitive agents operate on high-level percepts and\nlack the means to effectively detect complex spatio-temporal patterns in\nsequences of multiple percepts. In this paper, we present a stream-based\nperception approach that enables the agents to perceive meaningful situations\nin low-level sensor data streams. We present a crowdshipping case study where\nautonomous, self-interested agents collaborate to deliver parcels to their\ndestinations. We show how situations derived from smartphone sensor data can\ntrigger and guide auctions, which the agents use to reach agreements.\nExperiments with real smartphone data demonstrate the benefits of stream-based\nagent perception.\n",
    "link": "http://arxiv.org/abs/2401.13604v1"
  },
  {
    "title": "Graph Guided Question Answer Generation for Procedural\n  Question-Answering",
    "authors": "Hai X. Pham, Isma Hadji, Xinnuo Xu, Ziedune Degutyte, Jay Rainey, Evangelos Kazakos, Afsaneh Fazly, Georgios Tzimiropoulos, Brais Martinez",
    "abstract": "  In this paper, we focus on task-specific question answering (QA). To this\nend, we introduce a method for generating exhaustive and high-quality training\ndata, which allows us to train compact (e.g., run on a mobile device),\ntask-specific QA models that are competitive against GPT variants. The key\ntechnological enabler is a novel mechanism for automatic question-answer\ngeneration from procedural text which can ingest large amounts of textual\ninstructions and produce exhaustive in-domain QA training data. While current\nQA data generation methods can produce well-formed and varied data, their\nnon-exhaustive nature is sub-optimal for training a QA model. In contrast, we\nleverage the highly structured aspect of procedural text and represent each\nstep and the overall flow of the procedure as graphs. We then condition on\ngraph nodes to automatically generate QA pairs in an exhaustive and\ncontrollable manner. Comprehensive evaluations of our method show that: 1)\nsmall models trained with our data achieve excellent performance on the target\nQA task, even exceeding that of GPT3 and ChatGPT despite being several orders\nof magnitude smaller. 2) semantic coverage is the key indicator for downstream\nQA performance. Crucially, while large language models excel at syntactic\ndiversity, this does not necessarily result in improvements on the end QA\nmodel. In contrast, the higher semantic coverage provided by our method is\ncritical for QA performance.\n",
    "link": "http://arxiv.org/abs/2401.13594v1"
  },
  {
    "title": "Evaluation of General Large Language Models in Contextually Assessing\n  Semantic Concepts Extracted from Adult Critical Care Electronic Health Record\n  Notes",
    "authors": "Darren Liu, Cheng Ding, Delgersuren Bold, Monique Bouvier, Jiaying Lu, Benjamin Shickel, Craig S. Jabaley, Wenhui Zhang, Soojin Park, Michael J. Young, Mark S. Wainwright, Gilles Clermont, Parisa Rashidi, Eric S. Rosenthal, Laurie Dimisko, Ran Xiao, Joo Heung Yoon, Carl Yang, Xiao Hu",
    "abstract": "  The field of healthcare has increasingly turned its focus towards Large\nLanguage Models (LLMs) due to their remarkable performance. However, their\nperformance in actual clinical applications has been underexplored. Traditional\nevaluations based on question-answering tasks don't fully capture the nuanced\ncontexts. This gap highlights the need for more in-depth and practical\nassessments of LLMs in real-world healthcare settings. Objective: We sought to\nevaluate the performance of LLMs in the complex clinical context of adult\ncritical care medicine using systematic and comprehensible analytic methods,\nincluding clinician annotation and adjudication. Methods: We investigated the\nperformance of three general LLMs in understanding and processing real-world\nclinical notes. Concepts from 150 clinical notes were identified by MetaMap and\nthen labeled by 9 clinicians. Each LLM's proficiency was evaluated by\nidentifying the temporality and negation of these concepts using different\nprompts for an in-depth analysis. Results: GPT-4 showed overall superior\nperformance compared to other LLMs. In contrast, both GPT-3.5 and\ntext-davinci-003 exhibit enhanced performance when the appropriate prompting\nstrategies are employed. The GPT family models have demonstrated considerable\nefficiency, evidenced by their cost-effectiveness and time-saving capabilities.\nConclusion: A comprehensive qualitative performance evaluation framework for\nLLMs is developed and operationalized. This framework goes beyond singular\nperformance aspects. With expert annotations, this methodology not only\nvalidates LLMs' capabilities in processing complex medical data but also\nestablishes a benchmark for future LLM evaluations across specialized domains.\n",
    "link": "http://arxiv.org/abs/2401.13588v1"
  },
  {
    "title": "Prompt Weight Experiments for LLM Instruction Fine-Tuning",
    "authors": "Mathew Huerta-Enochian",
    "abstract": "  We present a small study analyzing how prompt token classification loss\nweighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on\ninstruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1\nand LLaMA 2 using multiple instruction datasets. We found that models\nfine-tuned on our short-completion dataset have a negative quadratic\nrelationship with PLW while models fine-tuned on long-completion datasets were\nunaffected by PLW.\n",
    "link": "http://arxiv.org/abs/2401.13586v1"
  },
  {
    "title": "Benchmarking the Fairness of Image Upsampling Methods",
    "authors": "Mike Laszkiewicz, Imant Daunhawer, Julia E. Vogt, Asja Fischer, Johannes Lederer",
    "abstract": "  Recent years have witnessed a rapid development of deep generative models for\ncreating synthetic media, such as images and videos. While the practical\napplications of these models in everyday tasks are enticing, it is crucial to\nassess the inherent risks regarding their fairness. In this work, we introduce\na comprehensive framework for benchmarking the performance and fairness of\nconditional generative models. We develop a set of\nmetrics$\\unicode{x2013}$inspired by their supervised fairness\ncounterparts$\\unicode{x2013}$to evaluate the models on their fairness and\ndiversity. Focusing on the specific application of image upsampling, we create\na benchmark covering a wide variety of modern upsampling methods. As part of\nthe benchmark, we introduce UnfairFace, a subset of FairFace that replicates\nthe racial distribution of common large-scale face datasets. Our empirical\nstudy highlights the importance of using an unbiased training set and reveals\nvariations in how the algorithms respond to dataset imbalances. Alarmingly, we\nfind that none of the considered methods produces statistically fair and\ndiverse results.\n",
    "link": "http://arxiv.org/abs/2401.13555v1"
  },
  {
    "title": "Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific\n  Input Representation and Diffusion Outpainting",
    "authors": "Hounsu Kim, Soonbeom Choi, Juhan Nam",
    "abstract": "  Synthesizing performing guitar sound is a highly challenging task due to the\npolyphony and high variability in expression. Recently, deep generative models\nhave shown promising results in synthesizing expressive polyphonic instrument\nsounds from music scores, often using a generic MIDI input. In this work, we\npropose an expressive acoustic guitar sound synthesis model with a customized\ninput representation to the instrument, which we call guitarroll. We implement\nthe proposed approach using diffusion-based outpainting which can generate\naudio with long-term consistency. To overcome the lack of MIDI/audio-paired\ndatasets, we used not only an existing guitar dataset but also collected data\nfrom a high quality sample-based guitar synthesizer. Through quantitative and\nqualitative evaluations, we show that our proposed model has higher audio\nquality than the baseline model and generates more realistic timbre sounds than\nthe previous leading work.\n",
    "link": "http://arxiv.org/abs/2401.13498v1"
  },
  {
    "title": "Separable Physics-Informed Neural Networks for the solution of\n  elasticity problems",
    "authors": "Vasiliy A. Es'kin, Danil V. Davydov, Julia V. Gur'eva, Alexey O. Malkhanov, Mikhail E. Smorkalov",
    "abstract": "  A method for solving elasticity problems based on separable physics-informed\nneural networks (SPINN) in conjunction with the deep energy method (DEM) is\npresented. Numerical experiments have been carried out for a number of problems\nshowing that this method has a significantly higher convergence rate and\naccuracy than the vanilla physics-informed neural networks (PINN) and even\nSPINN based on a system of partial differential equations (PDEs). In addition,\nusing the SPINN in the framework of DEM approach it is possible to solve\nproblems of the linear theory of elasticity on complex geometries, which is\nunachievable with the help of PINNs in frames of partial differential\nequations. Considered problems are very close to the industrial problems in\nterms of geometry, loading, and material parameters.\n",
    "link": "http://arxiv.org/abs/2401.13486v1"
  },
  {
    "title": "How AI Ideas Affect the Creativity, Diversity, and Evolution of Human\n  Ideas: Evidence From a Large, Dynamic Experiment",
    "authors": "Joshua Ashkinaze, Julia Mendelsohn, Li Qiwei, Ceren Budak, Eric Gilbert",
    "abstract": "  Exposure to large language model output is rapidly increasing. How will\nseeing AI-generated ideas affect human ideas? We conducted an experiment (800+\nparticipants, 40+ countries) where participants viewed creative ideas that were\nfrom ChatGPT or prior experimental participants and then brainstormed their own\nidea. We varied the number of AI-generated examples (none, low, or high\nexposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic\nexperiment design -- ideas from prior participants in an experimental condition\nare used as stimuli for future participants in the same experimental condition\n-- mimics the interdependent process of cultural creation: creative ideas are\nbuilt upon prior ideas. Hence, we capture the compounding effects of having\nLLMs 'in the culture loop'. We find that high AI exposure (but not low AI\nexposure) did not affect the creativity of individual ideas but did increase\nthe average amount and rate of change of collective idea diversity. AI made\nideas different, not better. There were no main effects of disclosure. We also\nfound that self-reported creative people were less influenced by knowing an\nidea was from AI, and that participants were more likely to knowingly adopt AI\nideas when the task was difficult. Our findings suggest that introducing AI\nideas into society may increase collective diversity but not individual\ncreativity.\n",
    "link": "http://arxiv.org/abs/2401.13481v1"
  },
  {
    "title": "Growing from Exploration: A self-exploring framework for robots based on\n  foundation models",
    "authors": "Shoujie Li, Ran Yu, Tong Wu, JunWen Zhong, Xiao-Ping Zhang, Wenbo Ding",
    "abstract": "  Intelligent robot is the ultimate goal in the robotics field. Existing works\nleverage learning-based or optimization-based methods to accomplish\nhuman-defined tasks. However, the challenge of enabling robots to explore\nvarious environments autonomously remains unresolved. In this work, we propose\na framework named GExp, which enables robots to explore and learn autonomously\nwithout human intervention. To achieve this goal, we devise modules including\nself-exploration, knowledge-base-building, and close-loop feedback based on\nfoundation models. Inspired by the way that infants interact with the world,\nGExp encourages robots to understand and explore the environment with a series\nof self-generated tasks. During the process of exploration, the robot will\nacquire skills from beneficial experiences that are useful in the future. GExp\nprovides robots with the ability to solve complex tasks through\nself-exploration. GExp work is independent of prior interactive knowledge and\nhuman intervention, allowing it to adapt directly to different scenarios,\nunlike previous studies that provided in-context examples as few-shot learning.\nIn addition, we propose a workflow of deploying the real-world robot system\nwith self-learned skills as an embodied assistant.\n",
    "link": "http://arxiv.org/abs/2401.13462v1"
  },
  {
    "title": "Multi-Agent Diagnostics for Robustness via Illuminated Diversity",
    "authors": "Mikayel Samvelyan, Davide Paglieri, Minqi Jiang, Jack Parker-Holder, Tim Rockt\u00e4schel",
    "abstract": "  In the rapidly advancing field of multi-agent systems, ensuring robustness in\nunfamiliar and adversarial settings is crucial. Notwithstanding their\noutstanding performance in familiar environments, these systems often falter in\nnew situations due to overfitting during the training phase. This is especially\npronounced in settings where both cooperative and competitive behaviours are\npresent, encapsulating a dual nature of overfitting and generalisation\nchallenges. To address this issue, we present Multi-Agent Diagnostics for\nRobustness via Illuminated Diversity (MADRID), a novel approach for generating\ndiverse adversarial scenarios that expose strategic vulnerabilities in\npre-trained multi-agent policies. Leveraging the concepts from open-ended\nlearning, MADRID navigates the vast space of adversarial settings, employing a\ntarget policy's regret to gauge the vulnerabilities of these settings. We\nevaluate the effectiveness of MADRID on the 11vs11 version of Google Research\nFootball, one of the most complex environments for multi-agent reinforcement\nlearning. Specifically, we employ MADRID for generating a diverse array of\nadversarial settings for TiZero, the state-of-the-art approach which \"masters\"\nthe game through 45 days of training on a large-scale distributed\ninfrastructure. We expose key shortcomings in TiZero's tactical\ndecision-making, underlining the crucial importance of rigorous evaluation in\nmulti-agent systems.\n",
    "link": "http://arxiv.org/abs/2401.13460v1"
  },
  {
    "title": "Clue-Guided Path Exploration: An Efficient Knowledge Base\n  Question-Answering Framework with Low Computational Resource Consumption",
    "authors": "Dehao Tao, Feng Huang, Yongfeng Huang, Minghu Jiang",
    "abstract": "  In recent times, large language models (LLMs) have showcased remarkable\ncapabilities. However, updating their knowledge poses challenges, potentially\nleading to inaccuracies when confronted with unfamiliar queries. While\nintegrating knowledge graphs with LLMs has been explored, existing approaches\ntreat LLMs as primary decision-makers, imposing high demands on their\ncapabilities. This is particularly unsuitable for LLMs with lower computational\ncosts and relatively poorer performance. In this paper, we introduce a\nClue-Guided Path Exploration framework (CGPE) that efficiently merges a\nknowledge base with an LLM, placing less stringent requirements on the model's\ncapabilities. Inspired by the method humans use to manually retrieve knowledge,\nCGPE employs information from the question as clues to systematically explore\nthe required knowledge path within the knowledge base. Experiments on\nopen-source datasets reveal that CGPE outperforms previous methods and is\nhighly applicable to LLMs with fewer parameters. In some instances, even\nChatGLM3, with its 6 billion parameters, can rival the performance of GPT-4.\nFurthermore, the results indicate a minimal invocation frequency of CGPE on\nLLMs, suggesting reduced computational overhead. For organizations and\nindividuals facing constraints in computational resources, our research offers\nsignificant practical value.\n",
    "link": "http://arxiv.org/abs/2401.13444v1"
  },
  {
    "title": "Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction\n  and Beyond",
    "authors": "Lang Nie, Chunyu Lin, Kang Liao, Shuaicheng Liu, Yao Zhao",
    "abstract": "  Thin-plate spline (TPS) is a principal warp that allows for representing\nelastic, nonlinear transformation with control point motions. With the increase\nof control points, the warp becomes increasingly flexible but usually\nencounters a bottleneck caused by undesired issues, e.g., content distortion.\nIn this paper, we explore generic applications of TPS in single-image-based\nwarping tasks, such as rotation correction, rectangling, and portrait\ncorrection. To break this bottleneck, we propose the coupled thin-plate spline\nmodel (CoupledTPS), which iteratively couples multiple TPS with limited control\npoints into a more flexible and powerful transformation. Concretely, we first\ndesign an iterative search to predict new control points according to the\ncurrent latent condition. Then, we present the warping flow as a bridge for the\ncoupling of different TPS transformations, effectively eliminating\ninterpolation errors caused by multiple warps. Besides, in light of the\nlaborious annotation cost, we develop a semi-supervised learning scheme to\nimprove warping quality by exploiting unlabeled data. It is formulated through\ndual transformation between the searched control points of unlabeled data and\nits graphic augmentation, yielding an implicit correction consistency\nconstraint. Finally, we collect massive unlabeled data to exhibit the benefit\nof our semi-supervised scheme in rotation correction. Extensive experiments\ndemonstrate the superiority and universality of CoupledTPS over the existing\nstate-of-the-art (SoTA) solutions for rotation correction and beyond. The code\nand data will be available at https://github.com/nie-lang/CoupledTPS.\n",
    "link": "http://arxiv.org/abs/2401.13432v1"
  },
  {
    "title": "Causal Perception",
    "authors": "Jose M. Alvarez, Salvatore Ruggieri",
    "abstract": "  Perception occurs when two individuals interpret the same information\ndifferently. Despite being a known phenomenon with implications for bias in\ndecision-making, as individuals' experience determines interpretation,\nperception remains largely overlooked in automated decision-making (ADM)\nsystems. In particular, it can have considerable effects on the fairness or\nfair usage of an ADM system, as fairness itself is context-specific and its\ninterpretation dependent on who is judging. In this work, we formalize\nperception under causal reasoning to capture the act of interpretation by an\nindividual. We also formalize individual experience as additional causal\nknowledge that comes with and is used by an individual. Further, we define and\ndiscuss loaded attributes, which are attributes prone to evoke perception.\nSensitive attributes, such as gender and race, are clear examples of loaded\nattributes. We define two kinds of causal perception, unfaithful and\ninconsistent, based on the causal properties of faithfulness and consistency.\nWe illustrate our framework through a series of decision-making examples and\ndiscuss relevant fairness applications. The goal of this work is to position\nperception as a parameter of interest, useful for extending the standard,\nsingle interpretation ADM problem formulation.\n",
    "link": "http://arxiv.org/abs/2401.13408v1"
  },
  {
    "title": "Past, Present, Future: A Comprehensive Exploration of AI Use Cases in\n  the UMBRELLA IoT Testbed",
    "authors": "Peizheng Li, Ioannis Mavromatis, Aftab Khan",
    "abstract": "  UMBRELLA is a large-scale, open-access Internet of Things (IoT) ecosystem\nincorporating over 200 multi-sensor multi-wireless nodes, 20 collaborative\nrobots, and edge-intelligence-enabled devices. This paper provides a guide to\nthe implemented and prospective artificial intelligence (AI) capabilities of\nUMBRELLA in real-world IoT systems. Four existing UMBRELLA applications are\npresented in detail: 1) An automated streetlight monitoring for detecting\nissues and triggering maintenance alerts; 2) A Digital twin of building\nenvironments providing enhanced air quality sensing with reduced cost; 3) A\nlarge-scale Federated Learning framework for reducing communication overhead;\nand 4) An intrusion detection for containerised applications identifying\nmalicious activities. Additionally, the potential of UMBRELLA is outlined for\nfuture smart city and multi-robot crowdsensing applications enhanced by\nsemantic communications and multi-agent planning. Finally, to realise the above\nuse-cases we discuss the need for a tailored MLOps platform to automate\nUMBRELLA model pipelines and establish trust.\n",
    "link": "http://arxiv.org/abs/2401.13346v1"
  },
  {
    "title": "Full Bayesian Significance Testing for Neural Networks",
    "authors": "Zehua Liu, Zimeng Li, Jingyuan Wang, Yue He",
    "abstract": "  Significance testing aims to determine whether a proposition about the\npopulation distribution is the truth or not given observations. However,\ntraditional significance testing often needs to derive the distribution of the\ntesting statistic, failing to deal with complex nonlinear relationships. In\nthis paper, we propose to conduct Full Bayesian Significance Testing for neural\nnetworks, called \\textit{n}FBST, to overcome the limitation in relationship\ncharacterization of traditional approaches. A Bayesian neural network is\nutilized to fit the nonlinear and multi-dimensional relationships with small\nerrors and avoid hard theoretical derivation by computing the evidence value.\nBesides, \\textit{n}FBST can test not only global significance but also local\nand instance-wise significance, which previous testing methods don't focus on.\nMoreover, \\textit{n}FBST is a general framework that can be extended based on\nthe measures selected, such as Grad-\\textit{n}FBST, LRP-\\textit{n}FBST,\nDeepLIFT-\\textit{n}FBST, LIME-\\textit{n}FBST. A range of experiments on both\nsimulated and real data are conducted to show the advantages of our method.\n",
    "link": "http://arxiv.org/abs/2401.13335v1"
  },
  {
    "title": "Explainable Bayesian Optimization",
    "authors": "Tanmay Chakraborty, Christin Seifert, Christian Wirth",
    "abstract": "  In industry, Bayesian optimization (BO) is widely applied in the human-AI\ncollaborative parameter tuning of cyber-physical systems. However, BO's\nsolutions may deviate from human experts' actual goal due to approximation\nerrors and simplified objectives, requiring subsequent tuning. The black-box\nnature of BO limits the collaborative tuning process because the expert does\nnot trust the BO recommendations. Current explainable AI (XAI) methods are not\ntailored for optimization and thus fall short of addressing this gap. To bridge\nthis gap, we propose TNTRules (TUNE-NOTUNE Rules), a post-hoc, rule-based\nexplainability method that produces high quality explanations through\nmultiobjective optimization. Our evaluation of benchmark optimization problems\nand real-world hyperparameter optimization tasks demonstrates TNTRules'\nsuperiority over state-of-the-art XAI methods in generating high quality\nexplanations. This work contributes to the intersection of BO and XAI,\nproviding interpretable optimization techniques for real-world applications.\n",
    "link": "http://arxiv.org/abs/2401.13334v1"
  },
  {
    "title": "Information That Matters: Exploring Information Needs of People Affected\n  by Algorithmic Decisions",
    "authors": "Timoth\u00e9e Schmude, Laura Koesten, Torsten M\u00f6ller, Sebastian Tschiatschek",
    "abstract": "  Explanations of AI systems rarely address the information needs of people\naffected by algorithmic decision-making (ADM). This gap between conveyed\ninformation and information that matters to affected stakeholders can impede\nunderstanding and adherence to regulatory frameworks such as the AI Act. To\naddress this gap, we present the \"XAI Novice Question Bank\": A catalog of\naffected stakeholders' information needs in two ADM use cases (employment\nprediction and health monitoring), covering the categories data, system\ncontext, system usage, and system specifications. Information needs were\ngathered in an interview study where participants received explanations in\nresponse to their inquiries. Participants further reported their understanding\nand decision confidence, showing that while confidence tended to increase after\nreceiving explanations, participants also met understanding challenges, such as\nbeing unable to tell why their understanding felt incomplete. Explanations\nfurther influenced participants' perceptions of the systems' risks and\nbenefits, which they confirmed or changed depending on the use case. When risks\nwere perceived as high, participants expressed particular interest in\nexplanations about intention, such as why and to what end a system was put in\nplace. With this work, we aim to support the inclusion of affected stakeholders\ninto explainability by contributing an overview of information and challenges\nrelevant to them when deciding on the adoption of ADM systems. We close by\nsummarizing our findings in a list of six key implications that inform the\ndesign of future explanations for affected stakeholder audiences.\n",
    "link": "http://arxiv.org/abs/2401.13324v1"
  },
  {
    "title": "Deep Learning for Improved Polyp Detection from Synthetic Narrow-Band\n  Imaging",
    "authors": "Mathias Ramm Haugland, Hemin Ali Qadir, Ilangko Balasingham",
    "abstract": "  To cope with the growing prevalence of colorectal cancer (CRC), screening\nprograms for polyp detection and removal have proven their usefulness.\nColonoscopy is considered the best-performing procedure for CRC screening. To\nease the examination, deep learning based methods for automatic polyp detection\nhave been developed for conventional white-light imaging (WLI). Compared with\nWLI, narrow-band imaging (NBI) can improve polyp classification during\ncolonoscopy but requires special equipment. We propose a CycleGAN-based\nframework to convert images captured with regular WLI to synthetic NBI (SNBI)\nas a pre-processing method for improving object detection on WLI when NBI is\nunavailable. This paper first shows that better results for polyp detection can\nbe achieved on NBI compared to a relatively similar dataset of WLI. Secondly,\nexperimental results demonstrate that our proposed modality translation can\nachieve improved polyp detection on SNBI images generated from WLI compared to\nthe original WLI. This is because our WLI-to-SNBI translation model can enhance\nthe observation of polyp surface patterns in the generated SNBI images.\n",
    "link": "http://arxiv.org/abs/2401.13315v1"
  },
  {
    "title": "ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in\n  Large Multimodal Models",
    "authors": "Rohan Wadhawan, Hritik Bansal, Kai-Wei Chang, Nanyun Peng",
    "abstract": "  Recent advancements in AI have led to the development of large multimodal\nmodels (LMMs) capable of processing complex tasks involving joint reasoning\nover text and visual content in the image (e.g., navigating maps in public\nplaces). This paper introduces ConTextual, a novel benchmark comprising\ninstructions designed explicitly to evaluate LMMs' ability to perform\ncontext-sensitive text-rich visual reasoning. ConTextual emphasizes diverse\nreal-world scenarios (e.g., time-reading, navigation, shopping and more)\ndemanding a deeper understanding of the interactions between textual and visual\nelements. Our findings reveal a significant performance gap of 30.8% between\nthe best-performing LMM, GPT-4V(ision), and human capabilities using human\nevaluation indicating substantial room for improvement in context-sensitive\ntext-rich visual reasoning. Notably, while GPT-4V excelled in abstract\ncategories like meme and quote interpretation, its overall performance still\nlagged behind humans. In addition to human evaluations, we also employed\nautomatic evaluation metrics using GPT-4, uncovering similar trends in\nperformance disparities. We also perform a fine-grained evaluation across\ndiverse visual contexts and provide qualitative analysis which provides a\nrobust framework for future advancements in the LMM design.\nhttps://con-textual.github.io/\n",
    "link": "http://arxiv.org/abs/2401.13311v1"
  },
  {
    "title": "Towards Explainable Harmful Meme Detection through Multimodal Debate\n  between Large Language Models",
    "authors": "Hongzhan Lin, Ziyang Luo, Wei Gao, Jing Ma, Bo Wang, Ruichao Yang",
    "abstract": "  The age of social media is flooded with Internet memes, necessitating a clear\ngrasp and effective identification of harmful ones. This task presents a\nsignificant challenge due to the implicit meaning embedded in memes, which is\nnot explicitly conveyed through the surface text and image. However, existing\nharmful meme detection methods do not present readable explanations that unveil\nsuch implicit meaning to support their detection decisions. In this paper, we\npropose an explainable approach to detect harmful memes, achieved through\nreasoning over conflicting rationales from both harmless and harmful positions.\nSpecifically, inspired by the powerful capacity of Large Language Models (LLMs)\non text generation and reasoning, we first elicit multimodal debate between\nLLMs to generate the explanations derived from the contradictory arguments.\nThen we propose to fine-tune a small language model as the debate judge for\nharmfulness inference, to facilitate multimodal fusion between the harmfulness\nrationales and the intrinsic multimodal information within memes. In this way,\nour model is empowered to perform dialectical reasoning over intricate and\nimplicit harm-indicative patterns, utilizing multimodal explanations\noriginating from both harmless and harmful arguments. Extensive experiments on\nthree public meme datasets demonstrate that our harmful meme detection approach\nachieves much better performance than state-of-the-art methods and exhibits a\nsuperior capacity for explaining the meme harmfulness of the model predictions.\n",
    "link": "http://arxiv.org/abs/2401.13298v1"
  },
  {
    "title": "RefreshNet: Learning Multiscale Dynamics through Hierarchical Refreshing",
    "authors": "Junaid Farooq, Danish Rafiq, Pantelis R. Vlachas, Mohammad Abid Bazaz",
    "abstract": "  Forecasting complex system dynamics, particularly for long-term predictions,\nis persistently hindered by error accumulation and computational burdens. This\nstudy presents RefreshNet, a multiscale framework developed to overcome these\nchallenges, delivering an unprecedented balance between computational\nefficiency and predictive accuracy. RefreshNet incorporates convolutional\nautoencoders to identify a reduced order latent space capturing essential\nfeatures of the dynamics, and strategically employs multiple recurrent neural\nnetwork (RNN) blocks operating at varying temporal resolutions within the\nlatent space, thus allowing the capture of latent dynamics at multiple temporal\nscales. The unique \"refreshing\" mechanism in RefreshNet allows coarser blocks\nto reset inputs of finer blocks, effectively controlling and alleviating error\naccumulation. This design demonstrates superiority over existing techniques\nregarding computational efficiency and predictive accuracy, especially in\nlong-term forecasting. The framework is validated using three benchmark\napplications: the FitzHugh-Nagumo system, the Reaction-Diffusion equation, and\nKuramoto-Sivashinsky dynamics. RefreshNet significantly outperforms\nstate-of-the-art methods in long-term forecasting accuracy and speed, marking a\nsignificant advancement in modeling complex systems and opening new avenues in\nunderstanding and predicting their behavior.\n",
    "link": "http://arxiv.org/abs/2401.13282v1"
  },
  {
    "title": "Can AI Assistants Know What They Don't Know?",
    "authors": "Qinyuan Cheng, Tianxiang Sun, Xiangyang Liu, Wenwei Zhang, Zhangyue Yin, Shimin Li, Linyang Li, Kai Chen, Xipeng Qiu",
    "abstract": "  Recently, AI assistants based on large language models (LLMs) show surprising\nperformance in many tasks, such as dialogue, solving math problems, writing\ncode, and using tools. Although LLMs possess intensive world knowledge, they\nstill make factual errors when facing some knowledge intensive tasks, like\nopen-domain question answering. These untruthful responses from the AI\nassistant may cause significant risks in practical applications. We believe\nthat an AI assistant's refusal to answer questions it does not know is a\ncrucial method for reducing hallucinations and making the assistant truthful.\nTherefore, in this paper, we ask the question \"Can AI assistants know what they\ndon't know and express them through natural language?\" To answer this question,\nwe construct a model-specific \"I don't know\" (Idk) dataset for an assistant,\nwhich contains its known and unknown questions, based on existing open-domain\nquestion answering datasets. Then we align the assistant with its corresponding\nIdk dataset and observe whether it can refuse to answer its unknown questions\nafter alignment. Experimental results show that after alignment with Idk\ndatasets, the assistant can refuse to answer most its unknown questions. For\nquestions they attempt to answer, the accuracy is significantly higher than\nbefore the alignment.\n",
    "link": "http://arxiv.org/abs/2401.13275v1"
  },
  {
    "title": "Audio-Infused Automatic Image Colorization by Exploiting Audio Scene\n  Semantics",
    "authors": "Pengcheng Zhao, Yanxiang Chen, Yang Zhao, Wei Jia, Zhao Zhang, Ronggang Wang, Richang Hong",
    "abstract": "  Automatic image colorization is inherently an ill-posed problem with\nuncertainty, which requires an accurate semantic understanding of scenes to\nestimate reasonable colors for grayscale images. Although recent\ninteraction-based methods have achieved impressive performance, it is still a\nvery difficult task to infer realistic and accurate colors for automatic\ncolorization. To reduce the difficulty of semantic understanding of grayscale\nscenes, this paper tries to utilize corresponding audio, which naturally\ncontains extra semantic information about the same scene. Specifically, a novel\naudio-infused automatic image colorization (AIAIC) network is proposed, which\nconsists of three stages. First, we take color image semantics as a bridge and\npretrain a colorization network guided by color image semantics. Second, the\nnatural co-occurrence of audio and video is utilized to learn the color\nsemantic correlations between audio and visual scenes. Third, the implicit\naudio semantic representation is fed into the pretrained network to finally\nrealize the audio-guided colorization. The whole process is trained in a\nself-supervised manner without human annotation. In addition, an audiovisual\ncolorization dataset is established for training and testing. Experiments\ndemonstrate that audio guidance can effectively improve the performance of\nautomatic colorization, especially for some scenes that are difficult to\nunderstand only from visual modality.\n",
    "link": "http://arxiv.org/abs/2401.13270v1"
  },
  {
    "title": "Designing Redistribution Mechanisms for Reducing Transaction Fees in\n  Blockchains",
    "authors": "Sankarshan Damle, Manisha Padala, Sujit Gujar",
    "abstract": "  Blockchains deploy Transaction Fee Mechanisms (TFMs) to determine which user\ntransactions to include in blocks and determine their payments (i.e.,\ntransaction fees). Increasing demand and scarce block resources have led to\nhigh user transaction fees. As these blockchains are a public resource, it may\nbe preferable to reduce these transaction fees. To this end, we introduce\nTransaction Fee Redistribution Mechanisms (TFRMs) -- redistributing VCG\npayments collected from such TFM as rebates to minimize transaction fees.\nClassic redistribution mechanisms (RMs) achieve this while ensuring Allocative\nEfficiency (AE) and User Incentive Compatibility (UIC). Our first result shows\nthe non-triviality of applying RM in TFMs. More concretely, we prove that it is\nimpossible to reduce transaction fees when (i) transactions that are not\nconfirmed do not receive rebates and (ii) the miner can strategically\nmanipulate the mechanism. Driven by this, we propose \\emph{Robust} TFRM\n(\\textsf{R-TFRM}): a mechanism that compromises on an honest miner's individual\nrationality to guarantee strictly positive rebates to the users. We then\nintroduce \\emph{robust} and \\emph{rational} TFRM (\\textsf{R}$^2$\\textsf{-TFRM})\nthat uses trusted on-chain randomness that additionally guarantees miner's\nindividual rationality (in expectation) and strictly positive rebates. Our\nresults show that TFRMs provide a promising new direction for reducing\ntransaction fees in public blockchains.\n",
    "link": "http://arxiv.org/abs/2401.13262v1"
  },
  {
    "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for\n  Personalized Dialogue Systems",
    "authors": "Hongru Wang, Wenyu Huang, Yang Deng, Rui Wang, Zezhong Wang, Yufei Wang, Fei Mi, Jeff Z. Pan, Kam-Fai Wong",
    "abstract": "  Large Language Models (LLMs) has shown exceptional capabilities in many\nnatual language understanding and generation tasks. However, the\npersonalization issue still remains a much-coveted property, especially when it\ncomes to the multiple sources involved in the dialogue system. To better plan\nand incorporate the use of multiple sources in generating personalized\nresponse, we firstly decompose it into three sub-tasks: Knowledge Source\nSelection, Knowledge Retrieval, and Response Generation. We then propose a\nnovel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)\nSpecifically, we unify these three sub-tasks with different formulations into\nthe same sequence-to-sequence paradigm during the training, to adaptively\nretrieve evidences and evaluate the relevance on-demand using special tokens,\ncalled acting tokens and evaluation tokens. Enabling language models to\ngenerate acting tokens facilitates interaction with various knowledge sources,\nallowing them to adapt their behavior to diverse task requirements. Meanwhile,\nevaluation tokens gauge the relevance score between the dialogue context and\nthe retrieved evidence. In addition, we carefully design a self-refinement\nmechanism to iteratively refine the generated response considering 1) the\nconsistency scores between the generated response and retrieved evidence; and\n2) the relevance scores. Experiments on two personalized datasets (DuLeMon and\nKBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge\nsource selection and response generation task with itself as a retriever in a\nunified manner. Extensive analyses and discussions are provided for shedding\nsome new perspectives for personalized dialogue systems.\n",
    "link": "http://arxiv.org/abs/2401.13256v1"
  },
  {
    "title": "From Random to Informed Data Selection: A Diversity-Based Approach to\n  Optimize Human Annotation and Few-Shot Learning",
    "authors": "Alexandre Alcoforado, Thomas Palmeira Ferraz, Lucas Hideki Okamura, Israel Campos Fama, Arnold Moya Lavado, B\u00e1rbara Dias Bueno, Bruno Veloso, Anna Helena Reali Costa",
    "abstract": "  A major challenge in Natural Language Processing is obtaining annotated data\nfor supervised learning. An option is the use of crowdsourcing platforms for\ndata annotation. However, crowdsourcing introduces issues related to the\nannotator's experience, consistency, and biases. An alternative is to use\nzero-shot methods, which in turn have limitations compared to their few-shot or\nfully supervised counterparts. Recent advancements driven by large language\nmodels show potential, but struggle to adapt to specialized domains with\nseverely limited data. The most common approaches therefore involve the human\nitself randomly annotating a set of datapoints to build initial datasets. But\nrandomly sampling data to be annotated is often inefficient as it ignores the\ncharacteristics of the data and the specific needs of the model. The situation\nworsens when working with imbalanced datasets, as random sampling tends to\nheavily bias towards the majority classes, leading to excessive annotated data.\nTo address these issues, this paper contributes an automatic and informed data\nselection architecture to build a small dataset for few-shot learning. Our\nproposal minimizes the quantity and maximizes diversity of data selected for\nhuman annotation, while improving model performance.\n",
    "link": "http://arxiv.org/abs/2401.13229v1"
  },
  {
    "title": "Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large\n  Language Models",
    "authors": "Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Xueqi Chen",
    "abstract": "  Exploring the application of large-scale language models to graph learning is\na novel endeavor. However, the vast amount of information inherent in large\ngraphs poses significant challenges to this process. This paper focuses on the\nlink prediction task and introduces LPNL (Link Prediction via Natural\nLanguage), a framework based on a large language model designed for scalable\nlink prediction on large-scale heterogeneous graphs.We design novel prompts for\nlink prediction that articulate graph details in natural language. We propose a\ntwo-stage sampling pipeline to extract crucial information from large-scale\nheterogeneous graphs, and a divide-and-conquer strategy to control the input\ntoken count within predefined limits, addressing the challenge of overwhelming\ninformation. We fine-tune a T5 model based on our self-supervised learning\ndesigned for for link prediction. Extensive experiments on a large public\nheterogeneous graphs demonstrate that LPNL outperforms various advanced\nbaselines, highlighting its remarkable performance in link prediction tasks on\nlarge-scale graphs.\n",
    "link": "http://arxiv.org/abs/2401.13227v1"
  },
  {
    "title": "TAT-LLM: A Specialized Language Model for Discrete Reasoning over\n  Tabular and Textual Data",
    "authors": "Fengbin Zhu, Ziyang Liu, Fuli Feng, Chao Wang, Moxin Li, Tat-Seng Chua",
    "abstract": "  In this work, we address question answering (QA) over a hybrid of tabular and\ntextual data that are very common content on the Web (e.g. SEC filings), where\ndiscrete reasoning capabilities are often required. Recently, large language\nmodels (LLMs) like GPT-4 have demonstrated strong multi-step reasoning\ncapabilities. We then consider harnessing the amazing power of LLMs to solve\nour task. We abstract a Step-wise Pipeline for tabular and textual QA, which\nconsists of three key steps, including Extractor, Reasoner and Executor, and\ninitially design an instruction to instantiate the pipeline and validate that\nGPT-4 outperforms all existing methods. However, utilizing an online LLM like\nGPT-4 holds various challenges in terms of cost, latency, and data security\nrisk, which motivates us to specialize smaller LLMs in this task. We develop a\nTAT-LLM language model by fine-tuning LLaMA 2 with the training data generated\nautomatically from existing expert-annotated datasets following the Step-wise\nPipeline. The experimental results have verified that our TAT-LLM model can\noutperform all baseline models, including the previous best fine-tuned models\nand very large-scale LLMs like GPT-4 on FinQA, TAT-QA and TAT-DQA benchmarks.\nWe hope our work can serve as a pioneering example of specializing smaller\nlanguage models for specific tasks.\n",
    "link": "http://arxiv.org/abs/2401.13223v1"
  },
  {
    "title": "TEPI: Taxonomy-aware Embedding and Pseudo-Imaging for Scarcely-labeled\n  Zero-shot Genome Classification",
    "authors": "Sathyanarayanan Aakur, Vishalini R. Laguduva, Priyadharsini Ramamurthy, Akhilesh Ramachandran",
    "abstract": "  A species' genetic code or genome encodes valuable evolutionary, biological,\nand phylogenetic information that aids in species recognition, taxonomic\nclassification, and understanding genetic predispositions like drug resistance\nand virulence. However, the vast number of potential species poses significant\nchallenges in developing a general-purpose whole genome classification tool.\nTraditional bioinformatics tools have made notable progress but lack\nscalability and are computationally expensive. Machine learning-based\nframeworks show promise but must address the issue of large classification\nvocabularies with long-tail distributions. In this study, we propose addressing\nthis problem through zero-shot learning using TEPI, Taxonomy-aware Embedding\nand Pseudo-Imaging. We represent each genome as pseudo-images and map them to a\ntaxonomy-aware embedding space for reasoning and classification. This embedding\nspace captures compositional and phylogenetic relationships of species,\nenabling predictions in extensive search spaces. We evaluate TEPI using two\nrigorous zero-shot settings and demonstrate its generalization capabilities\nqualitatively on curated, large-scale, publicly sourced data.\n",
    "link": "http://arxiv.org/abs/2401.13219v1"
  },
  {
    "title": "AMANet: Advancing SAR Ship Detection with Adaptive Multi-Hierarchical\n  Attention Network",
    "authors": "Xiaolin Ma, Junkai Cheng, Aihua Li, Yuhua Zhang, Zhilong Lin",
    "abstract": "  Recently, methods based on deep learning have been successfully applied to\nship detection for synthetic aperture radar (SAR) images. Despite the\ndevelopment of numerous ship detection methodologies, detecting small and\ncoastal ships remains a significant challenge due to the limited features and\nclutter in coastal environments. For that, a novel adaptive multi-hierarchical\nattention module (AMAM) is proposed to learn multi-scale features and\nadaptively aggregate salient features from various feature layers, even in\ncomplex environments. Specifically, we first fuse information from adjacent\nfeature layers to enhance the detection of smaller targets, thereby achieving\nmulti-scale feature enhancement. Then, to filter out the adverse effects of\ncomplex backgrounds, we dissect the previously fused multi-level features on\nthe channel, individually excavate the salient regions, and adaptively\namalgamate features originating from different channels. Thirdly, we present a\nnovel adaptive multi-hierarchical attention network (AMANet) by embedding the\nAMAM between the backbone network and the feature pyramid network (FPN).\nBesides, the AMAM can be readily inserted between different frameworks to\nimprove object detection. Lastly, extensive experiments on two large-scale SAR\nship detection datasets demonstrate that our AMANet method is superior to\nstate-of-the-art methods.\n",
    "link": "http://arxiv.org/abs/2401.13214v1"
  },
  {
    "title": "AdCorDA: Classifier Refinement via Adversarial Correction and Domain\n  Adaptation",
    "authors": "Lulan Shen, Ali Edalati, Brett Meyer, Warren Gross, James J. Clark",
    "abstract": "  This paper describes a simple yet effective technique for refining a\npretrained classifier network. The proposed AdCorDA method is based on\nmodification of the training set and making use of the duality between network\nweights and layer inputs. We call this input space training. The method\nconsists of two stages - adversarial correction followed by domain adaptation.\nAdversarial correction uses adversarial attacks to correct incorrect\ntraining-set classifications. The incorrectly classified samples of the\ntraining set are removed and replaced with the adversarially corrected samples\nto form a new training set, and then, in the second stage, domain adaptation is\nperformed back to the original training set. Extensive experimental validations\nshow significant accuracy boosts of over 5% on the CIFAR-100 dataset. The\ntechnique can be straightforwardly applied to refinement of weight-quantized\nneural networks, where experiments show substantial enhancement in performance\nover the baseline. The adversarial correction technique also results in\nenhanced robustness to adversarial attacks.\n",
    "link": "http://arxiv.org/abs/2401.13212v1"
  },
  {
    "title": "Boosting the Transferability of Adversarial Examples via Local Mixup and\n  Adaptive Step Size",
    "authors": "Junlin Liu, Xinchen Lyu",
    "abstract": "  Adversarial examples are one critical security threat to various visual\napplications, where injected human-imperceptible perturbations can confuse the\noutput.Generating transferable adversarial examples in the black-box setting is\ncrucial but challenging in practice. Existing input-diversity-based methods\nadopt different image transformations, but may be inefficient due to\ninsufficient input diversity and an identical perturbation step size. Motivated\nby the fact that different image regions have distinctive weights in\nclassification, this paper proposes a black-box adversarial generative\nframework by jointly designing enhanced input diversity and adaptive step\nsizes. We design local mixup to randomly mix a group of transformed adversarial\nimages, strengthening the input diversity. For precise adversarial generation,\nwe project the perturbation into the $tanh$ space to relax the boundary\nconstraint. Moreover, the step sizes of different regions can be dynamically\nadjusted by integrating a second-order momentum.Extensive experiments on\nImageNet validate that our framework can achieve superior transferability\ncompared to state-of-the-art baselines.\n",
    "link": "http://arxiv.org/abs/2401.13205v1"
  },
  {
    "title": "MLLMReID: Multimodal Large Language Model-based Person Re-identification",
    "authors": "Shan Yang, Yongfei Zhang",
    "abstract": "  Multimodal large language models (MLLM) have achieved satisfactory results in\nmany tasks. However, their performance in the task of person re-identification\n(ReID) has not been explored to date. This paper will investigate how to adapt\nthem for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID\nimage-text datasets, and then use their visual encoder as a backbone for ReID.\nHowever, there still exist two apparent issues: (1) Designing instructions for\nReID, MLLMs may overfit specific instructions, and designing a variety of\ninstructions will lead to higher costs. (2) Latent image feature vectors from\nLLMs are not involved in loss computation. Instructional learning, aligning\nimage-text features, results in indirect optimization and a learning objective\nthat inadequately utilizes features, limiting effectiveness in person feature\nlearning. To address these problems, this paper proposes MLLMReID: Multimodal\nLarge Language Model-based ReID. Firstly, we proposed Common Instruction, a\nsimple approach that leverages the essence ability of LLMs to continue writing,\navoiding complex and diverse instruction design. Secondly, we proposed\nDirectReID, which effectively employs the latent image feature vectors of\nimages outputted by LLMs in ReID tasks. The experimental results demonstrate\nthe superiority of our method. We will open-source the code on GitHub.\n",
    "link": "http://arxiv.org/abs/2401.13201v1"
  },
  {
    "title": "Catch-Up Mix: Catch-Up Class for Struggling Filters in CNN",
    "authors": "Minsoo Kang, Minkoo Kang, Suhyun Kim",
    "abstract": "  Deep learning has made significant advances in computer vision, particularly\nin image classification tasks. Despite their high accuracy on training data,\ndeep learning models often face challenges related to complexity and\noverfitting. One notable concern is that the model often relies heavily on a\nlimited subset of filters for making predictions. This dependency can result in\ncompromised generalization and an increased vulnerability to minor variations.\nWhile regularization techniques like weight decay, dropout, and data\naugmentation are commonly used to address this issue, they may not directly\ntackle the reliance on specific filters. Our observations reveal that the heavy\nreliance problem gets severe when slow-learning filters are deprived of\nlearning opportunities due to fast-learning filters. Drawing inspiration from\nimage augmentation research that combats over-reliance on specific image\nregions by removing and replacing parts of images, our idea is to mitigate the\nproblem of over-reliance on strong filters by substituting highly activated\nfeatures. To this end, we present a novel method called Catch-up Mix, which\nprovides learning opportunities to a wide range of filters during training,\nfocusing on filters that may lag behind. By mixing activation maps with\nrelatively lower norms, Catch-up Mix promotes the development of more diverse\nrepresentations and reduces reliance on a small subset of filters. Experimental\nresults demonstrate the superiority of our method in various vision\nclassification datasets, providing enhanced robustness.\n",
    "link": "http://arxiv.org/abs/2401.13193v1"
  },
  {
    "title": "Generative Design of Crystal Structures by Point Cloud Representations\n  and Diffusion Model",
    "authors": "Zhelin Li, Rami Mrad, Runxian Jiao, Guan Huang, Jun Shan, Shibing Chu, Yuanping Chen",
    "abstract": "  Efficiently generating energetically stable crystal structures has long been\na challenge in material design, primarily due to the immense arrangement of\natoms in a crystal lattice. To facilitate the discovery of stable material, we\npresent a framework for the generation of synthesizable materials, leveraging a\npoint cloud representation to encode intricate structural information. At the\nheart of this framework lies the introduction of a diffusion model as its\nfoundational pillar. To gauge the efficacy of our approach, we employ it to\nreconstruct input structures from our training datasets, rigorously validating\nits high reconstruction performance. Furthermore, we demonstrate the profound\npotential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely\nnew materials, emphasizing their synthesizability. Our research stands as a\nnoteworthy contribution to the advancement of materials design and synthesis\nthrough the cutting-edge avenue of generative design instead of the\nconventional substitution or experience-based discovery.\n",
    "link": "http://arxiv.org/abs/2401.13192v1"
  },
  {
    "title": "AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents",
    "authors": "Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, Junxian He",
    "abstract": "  Evaluating large language models (LLMs) as general-purpose agents is\nessential for understanding their capabilities and facilitating their\nintegration into practical applications. However, the evaluation process\npresents substantial challenges. A primary obstacle is the benchmarking of\nagent performance across diverse scenarios within a unified framework,\nespecially in maintaining partially-observable environments and ensuring\nmulti-round interactions. Moreover, current evaluation frameworks mostly focus\non the final success rate, revealing few insights during the process and\nfailing to provide a deep understanding of the model abilities. To address\nthese challenges, we introduce AgentBoard, a pioneering comprehensive benchmark\nand accompanied open-source evaluation framework tailored to analytical\nevaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric\nthat captures incremental advancements as well as a comprehensive evaluation\ntoolkit that features easy assessment of agents for multi-faceted analysis\nthrough interactive visualization. This not only sheds light on the\ncapabilities and limitations of LLM agents but also propels the\ninterpretability of their performance to the forefront. Ultimately, AgentBoard\nserves as a significant step towards demystifying agent behaviors and\naccelerating the development of stronger LLM agents.\n",
    "link": "http://arxiv.org/abs/2401.13178v1"
  },
  {
    "title": "Compositional Generative Inverse Design",
    "authors": "Tailin Wu, Takashi Maruyama, Long Wei, Tao Zhang, Yilun Du, Gianluca Iaccarino, Jure Leskovec",
    "abstract": "  Inverse design, where we seek to design input variables in order to optimize\nan underlying objective function, is an important problem that arises across\nfields such as mechanical engineering to aerospace engineering. Inverse design\nis typically formulated as an optimization problem, with recent works\nleveraging optimization across learned dynamics models. However, as models are\noptimized they tend to fall into adversarial modes, preventing effective\nsampling. We illustrate that by instead optimizing over the learned energy\nfunction captured by the diffusion model, we can avoid such adversarial\nexamples and significantly improve design performance. We further illustrate\nhow such a design system is compositional, enabling us to combine multiple\ndifferent diffusion models representing subcomponents of our desired system to\ndesign systems with every specified component. In an N-body interaction task\nand a challenging 2D multi-airfoil design task, we demonstrate that by\ncomposing the learned diffusion model at test time, our method allows us to\ndesign initial states and boundary shapes that are more complex than those in\nthe training data. Our method outperforms state-of-the-art neural inverse\ndesign method by an average of 41.5% in prediction MAE and 14.3% in design\nobjective for the N-body dataset and discovers formation flying to minimize\ndrag in the multi-airfoil design task. Project website and code can be found at\nhttps://github.com/AI4Science-WestlakeU/cindm.\n",
    "link": "http://arxiv.org/abs/2401.13171v1"
  },
  {
    "title": "Time-Aware Knowledge Representations of Dynamic Objects with\n  Multidimensional Persistence",
    "authors": "Baris Coskunuzer, Ignacio Segovia-Dominguez, Yuzhou Chen, Yulia R. Gel",
    "abstract": "  Learning time-evolving objects such as multivariate time series and dynamic\nnetworks requires the development of novel knowledge representation mechanisms\nand neural network architectures, which allow for capturing implicit\ntime-dependent information contained in the data. Such information is typically\nnot directly observed but plays a key role in the learning task performance. In\nturn, lack of time dimension in knowledge encoding mechanisms for\ntime-dependent data leads to frequent model updates, poor learning performance,\nand, as a result, subpar decision-making. Here we propose a new approach to a\ntime-aware knowledge representation mechanism that notably focuses on implicit\ntime-dependent topological information along multiple geometric dimensions. In\nparticular, we propose a new approach, named \\textit{Temporal MultiPersistence}\n(TMP), which produces multidimensional topological fingerprints of the data by\nusing the existing single parameter topological summaries. The main idea behind\nTMP is to merge the two newest directions in topological representation\nlearning, that is, multi-persistence which simultaneously describes data shape\nevolution along multiple key parameters, and zigzag persistence to enable us to\nextract the most salient data shape information over time. We derive\ntheoretical guarantees of TMP vectorizations and show its utility, in\napplication to forecasting on benchmark traffic flow, Ethereum blockchain, and\nelectrocardiogram datasets, demonstrating the competitive performance,\nespecially, in scenarios of limited data records. In addition, our TMP method\nimproves the computational efficiency of the state-of-the-art multipersistence\nsummaries up to 59.5 times.\n",
    "link": "http://arxiv.org/abs/2401.13157v1"
  }
]